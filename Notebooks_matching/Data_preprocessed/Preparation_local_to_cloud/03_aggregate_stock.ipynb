{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Donnée de l'INPI\n",
    "\n",
    "La class `inpi` a des erreurs. Ne pas tenir compte de ce notebook\n",
    "\n",
    "Dans les données de stock, il y a un stock inital a date du 5 mai 2017 et un ensemble de stock initial délivré à différent intervals.\n",
    "\n",
    "Il y a aussi un ensemble de données provenant de flux. -> données de flux\n",
    "\n",
    "Le format de donnée est stocké dans le json suivant:\n",
    "\n",
    "- https://s3.console.aws.amazon.com/s3/object/calfdata/INPI/TC_1/Stock/dtypes_stock.json\n",
    "\n",
    "## Documentation\n",
    "\n",
    "- [Doc INSEE](https://scm.saas.cagip.group.gca/PERNETTH/database/blob/master/Documentation/IMR/Doc_Tech_IMR_Mai_2019_v1.5.1.pdf)\n",
    "- [etalab](https://github.com/etalab/rncs_worker_api_entreprise)\n",
    "    - [Module data cleaning](https://github.com/etalab/rncs_worker_api_entreprise/blob/master/clean_csv.bash)\n",
    "- Path local: `C:\\Users\\PERNETTH\\Documents\\Projects\\GitLab\\CreationDataBase\\INPI\\TC_1\\Stock\\Stock_initial`\n",
    "- Path Reseau: `G:\\Projets\\PP2019\\2019_PROGRAMME BATICA\\DATA\\INPI\\sources\\IMR_Donnees_Saisies\\tc\\stock\\2017`\n",
    "\n",
    "### Variables communes\n",
    "\n",
    "![image](https://drive.google.com/uc?export=view&id=1laCGFEtxM4bbckNZQEkw1icBjF4pd20-)\n",
    "\n",
    "### Stock\n",
    "\n",
    "Le contenu de chaque fichier est disponible [ici](https://scm.saas.cagip.group.gca/PERNETTH/database/tree/master/INPI/TC_1)\n",
    "\n",
    "Les données brutes sont répertoriées dans des fichiers zip composées de 7 fichiers:\n",
    "\n",
    "- Actes\n",
    "- Comptes Annuels\n",
    "- ETS\n",
    "- OBS\n",
    "- PM\n",
    "- PP\n",
    "- REP\n",
    "\n",
    "### Donnees de flux \n",
    "\n",
    "- Path local: `C:\\Users\\PERNETTH\\Documents\\Projects\\GitLab\\CreationDataBase\\INPI\\TC_1\\Flux`\n",
    "- Path Reseau: `G:\\Projets\\PP2019\\2019_PROGRAMME BATICA\\DATA\\INPI\\sources\\IMR_Donnees_Saisies\\tc\\Flux`\n",
    "\n",
    "Les PP en local sont disponibles ici: `C:\\Users\\PERNETTH\\Documents\\Projects\\GitLab\\CreationDataBase\\INPI\\TC_1\\Flux\\PP`\n",
    "\n",
    "name csv : `0101_1_20170512_112544_4_PP_EVT`\n",
    "    \n",
    "    - code greffe: 0101\n",
    "    - numéro transmission: 1\n",
    "    - Date: 2017-05-12\n",
    "    - heures: 11h25m44s\n",
    "    - Type: PP\n",
    "    - Evenement: Oui\n",
    "\n",
    "**FICHIERS TRANSMIS EN CAS DE MISE A JOUR D’UN DOSSIER (EVENEMENT)**\n",
    "\n",
    "En cas de mise à jour d’un dossier suite à un événement (modification, radiation), les fichiers transmis ont une structure identique aux fichiers créés à l’immatriculation avec la présence de 2 champs spécifiques : la date de l’événement (Date_Greffe) et le libellé de l’événement (Libelle_Evt).\n",
    "\n",
    "Dans ces cas, 6 types de fichiers supplémentaires, numérotés, sont transmis correspondant à :\n",
    "* Evénements modifiant ou complétant les dossiers d’immatriculation des personnes morales (2) ou physiques (4)\n",
    "* Evénements modifiant ou complétant les informations relatives aux représentants (6) ou aux établissements (9)\n",
    "* Evénements supprimant des représentants (7 – Représentant partant) ou des établissements (10 – Etablissement supprimé)\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1FVEGNqogl1NxB84BtdF4TQztCXjmtpyo)\n",
    "\n",
    "Attention, il peut arriver que le même dossier fasse l’objet de plusieurs événements (création et modification) dans la même transmission. Il est impératif d’intégrer les événements dans l’ordre d’apparition.\n",
    "\n",
    "Les csv sont classés de la manière suivante:\n",
    "\n",
    "- Module (ETS, PP, PM, etc)\n",
    "    - NEW: New event\n",
    "    - EVT: Update event\n",
    "    \n",
    "## REP\n",
    "\n",
    "Doc REP: https://scm.saas.cagip.group.gca/PERNETTH/database/blob/master/Documentation/IMR/Doc_Tech_IMR_Mai_2019_v1.5.1.pdf\n",
    "- page 30\n",
    "\n",
    "Attention, certains champs sont remplis selon le type de la société. Ex SIREN -> Si type =PM\n",
    "\n",
    "Pourquoi deux SIREN?\n",
    "\n",
    "Le deuxième siren correspond a celui de la PM qui detient l'entreprise.\n",
    "\n",
    "Ex: Peugeot a plusieurs détenteur. Le détenteur possède un siren, il sera affiché dans siren.1 uniquement si détenteur est une PM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import boto3, json\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import inpi as inpi\n",
    "#from tqdm.notebook import tqdm\n",
    "import tqdm\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#from sagemaker import get_execution_role\n",
    "#role = get_execution_role() \n",
    "instance_aws = 'https://calfdata.s3.eu-west-3.amazonaws.com'\n",
    "bucket = 'calfdata'\n",
    "agg_datasock = inpi.inpiStockFlux(instance_aws, bucket)\n",
    "agg_datasock.uploadFileBucket(pathfile = 'INPI/TC_1/dtypes_stock.json')\n",
    "with open('dtypes_stock.json', 'r' ,encoding='utf8') as f:\n",
    "    datastore = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Batch Stock aggregation csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "for key, value in tqdm.tqdm(datastore['option'].items()): \n",
    "    if key not in ['PP', 'PM', 'ETS']:\n",
    "        print(key)\n",
    "        dtype = value[\"dtype\"]\n",
    "        parse_dates = value['parse_dates']\n",
    "        \n",
    "        for p in [True, False]:\n",
    "            agg_datasock.appendInpiStock(\n",
    "                key,\n",
    "                dtype,\n",
    "                parse_dates,\n",
    "                partiel = p,\n",
    "                return_frame = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Batch flux aggregation csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### test\n",
    "\n",
    "La class `inpi` a des erreurs. Ne pas tenir compte de ce notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def appendInpiFlux(\n",
    "                       option_extract,\n",
    "                       dtype,\n",
    "                       parse_date,\n",
    "                       new = True,\n",
    "                       year = 2017,\n",
    "                       return_frame = True,\n",
    "                       to_s3 = True):\n",
    "\n",
    "        \"\"\"\n",
    "        Append all csv files in a folder to a Pandas DataFrame and save the\n",
    "        output in S3: INPI/TC_1/Stock_processed from S3 bucket:\n",
    "        /INPI/TC_1/Flux/\n",
    "\n",
    "\n",
    "        format output-> gz\n",
    "        - Option + filename +stock +gz:\n",
    "            - 2017_NEW_PP.gz\n",
    "\n",
    "        Args:\n",
    "        option_extract: String, from list ['ACTES', 'COMPTES_ANNUELS','ETS',\n",
    "                      'OBS', 'PM', 'PP','REP']\n",
    "        dtype: variables type, use 'str', for the data and pd.Int64Dtype()\n",
    "        for integer. If possible\n",
    "        parse_date: A list with the variables to convert into dates\n",
    "        new: Boolean, if true, then go NEW folder, else EVT\n",
    "        year: integer. Locate year folder, 2017,2018,2019,2020\n",
    "\n",
    "        Return:\n",
    "            Pandas DataFrame\n",
    "        \"\"\"\n",
    "\n",
    "        # Test if in\n",
    "        list_option = [\"ACTES\", \"COMPTES_ANNUELS\", \"ETS\",\n",
    "                      \"OBS\", \"PM\", \"PP\", \"REP\"]\n",
    "\n",
    "        if option_extract not in list_option:\n",
    "            return print(\n",
    "            \"Veuillez utiliser l'un des mots clés suivants {} \\n \\\n",
    "        pour l'argument origin\".format(list_option)\n",
    "        )\n",
    "\n",
    "        #my_bucket = self.s3.Bucket(self.bucket)\n",
    "\n",
    "        list_mois =  [#\"01\", \"02\", \"03\", \"04\",\n",
    "                      \"05\",\n",
    "            #\"06\",\"07\",\"08\", \"09\", \"10\", \"11\", \"12\"\n",
    "        ]\n",
    "        path_flux = r\"C:\\Users\\PERNETTH\\Documents\\Projects\\GitLab\" \\\n",
    "        r\"\\CreationDataBase\\INPI\\TC_1\\Flux\"\n",
    "        list_errors = []\n",
    "        df_append = pd.DataFrame()\n",
    "        for month in list_mois:\n",
    "            if new:\n",
    "                subfilter = r\"{0}\\{1}\\{2}\\NEW\".format(str(year),\n",
    "                                                      month,\n",
    "                                                      option_extract)\n",
    "                new_option = 'NEW'\n",
    "            else:\n",
    "                subfilter = r\"{0}\\{1}\\{2}\\EVT\".format(str(year),\n",
    "                                                      month,\n",
    "                                                      option_extract)\n",
    "                new_option = 'EVT'\n",
    "            #print(r\"{}\\{}\".format(path_flux, subfilter))\n",
    "            \n",
    "            import os\n",
    "            for filename in os.listdir(r\"{}\\{}\".format(path_flux, subfilter)):\n",
    "                f = r\"{}\\{}\\{}\".format(path_flux, subfilter, filename)\n",
    "                try:\n",
    "                    df_ = pd.read_csv(f,\n",
    "                            sep=\";\", \n",
    "                            dtype=datastore['option']['ETS']['dtype'],\n",
    "                            parse_dates=\n",
    "                            datastore['option']['ETS']['parse_dates'],\n",
    "                                      error_bad_lines = False\n",
    "                                     )\n",
    "                    df_append = df_append.append(df_, sort = False)\n",
    "                except:\n",
    "                    list_errors.append(f)\n",
    "            \n",
    "                #df_ = pd.concat(\n",
    "        #[\n",
    "         #   pd.read_csv(file, sep=\";\", \n",
    "         #               dtype=dtype, \n",
    "         #               parse_dates=parse_date)\n",
    "        #],\n",
    "        #\"ignore_index=True,\n",
    "    #)\n",
    "                \n",
    "        return df_append, list_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "temp = appendInpiFlux(option_extract=\"ETS\",\n",
    "                            new=False,\n",
    "                            year=2017,\n",
    "                            dtype=datastore['option']['ETS']['dtype'],\n",
    "                            parse_date=datastore['option']['ETS']['parse_dates'],\n",
    "                            return_frame=True,\n",
    "                            to_s3 = False)\n",
    "#C:\\Users\\PERNETTH\\Documents\\Projects\\GitLab\\CreationDataBase\\INPI\\TC_1\\Flux\\2017\\05\\ETS\\EVT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "list_failed = []\n",
    "for key, value in tqdm.tqdm(datastore['option'].items()): \n",
    "    if key not in ['OBS']:\n",
    "        print(key)\n",
    "        try:\n",
    "            dtype = value[\"dtype\"]\n",
    "            parse_dates = value['parse_dates']\n",
    "            agg_datasock.appendInpiFlux(\n",
    "            option_extract =key,\n",
    "            new = True,\n",
    "            year = 2017,\n",
    "            dtype = dtype,\n",
    "            parse_date = parse_dates,\n",
    "            return_frame = True)\n",
    "        except:\n",
    "            list_failed.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "list_failed = []\n",
    "for key, value in tqdm.tqdm(datastore['option'].items()): \n",
    "    if key not in ['PM', 'PP']:\n",
    "        print(key)\n",
    "        try:\n",
    "            dtype = value[\"dtype\"]\n",
    "            parse_dates = value['parse_dates']\n",
    "            agg_datasock.appendInpiFlux(\n",
    "            option_extract =key,\n",
    "            new = False,\n",
    "            year = 2017,\n",
    "            dtype = dtype,\n",
    "            parse_date = parse_dates,\n",
    "            return_frame = True)\n",
    "        except:\n",
    "            list_failed.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.remove('dtypes_stock.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#agg_datasock.appendInpiStock('PP',\n",
    "#                             dtype,\n",
    "#                             parse_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#agg_datasock.appendInpiStock('PP',\n",
    "#                                    dtype,\n",
    "#                                    parse_dates,\n",
    "#                                    partiel = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
