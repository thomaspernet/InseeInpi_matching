{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import zipfile \n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import shutil\n",
    "import csv\n",
    "import gzip\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# TC \n",
    "\n",
    "- Stock\n",
    "\n",
    "path : `G:\\Projets\\PP2019\\2019_PROGRAMME BATICA\\DATA\\INPI\\sources\\IMR_Donnees_Saisies\\tc\\stock`\n",
    "\n",
    "On classe les csv selon leur balise:\n",
    "\n",
    "- PM\n",
    "- PP\n",
    "- REP\n",
    "- ETS\n",
    "- OBS\n",
    "- ACTES\n",
    "- CA\n",
    "\n",
    "Faire un log pour etre sur que chaque csv a les mêmes colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def moveCSVStock(pathcsv,pathsave):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    log_columns = []\n",
    "    for file in glob.glob(\"{}\\*.csv\".format(pathcsv)):\n",
    "        for i in [\"PM\", \"PP\", \"rep\", \"ets\", \"actes\", \"comptes_annuels\", \"obs\"]:\n",
    "            #\n",
    "            matches = re.search(i, file)\n",
    "            if matches:\n",
    "                dic_files_col = {}\n",
    "                filename = os.path.basename(file)\n",
    "                new_des = \"{}\\{}\\{}\".format(pathsave, i.upper(), filename)\n",
    "                with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "                    reader = csv.reader(f)\n",
    "\n",
    "                    col = next(reader)\n",
    "                    dic_files_col[\"filename\"] = filename\n",
    "                    dic_files_col[\"origin\"] = i\n",
    "                    dic_files_col[\"columns\"] = col\n",
    "                    row_count = sum(1 for line in f)\n",
    "                    dic_files_col[\"rows_count\"] = row_count\n",
    "                log_columns.append(dic_files_col)\n",
    "                ### save as gzip\n",
    "                shutil.move(file, new_des)\n",
    "\n",
    "    return log_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Stock\n",
    "\n",
    "Il y a deux types de stocks: \n",
    "\n",
    "- Le stock initial constitué à la date du 4 mai 2017\n",
    "- Le stock partiel\n",
    "    * Des stocks partiels constitués des dossiers complets relivrés à la demande de l’INPI après détection d’anomalies ?\n",
    "    *  Les fichiers des données contenues dans les nouvelles inscriptions (immatriculations, modifications et radiations) du Registre national du commerce et des sociétés ainsi que les informations relatives aux dépôts des actes et comptes annuels, telles que transmises par les greffes à compter du 5 mai 2017 (données de flux)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Stock initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "path = r\"G:\\Projets\\PP2019\\2019_PROGRAMME BATICA\\DATA\\INPI\\sources\\IMR_Donnees_Saisies\\tc\\stock\\2017\\05\\04\"\n",
    "list_zip = []\n",
    "for root, dirs, files in os.walk(path):\n",
    "    # Test-> on veut pas avant mai 2018\n",
    "    for name in files:\n",
    "        if name.endswith((\".zip\")):\n",
    "            path_xml = \"{}\\{}\".format(\n",
    "                    root,\n",
    "                    name)\n",
    "            list_zip.append(path_xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "len(list_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "pathsave = r\"C:\\Users\\PERNETTH\\Documents\\Projects\\GitLab\\CreationDataBase\\INPI\\TC_1\\Stock\\Stock_initial\"\n",
    "pathcsv = os.getcwd()\n",
    "list_columns_csv = []\n",
    "for file in tqdm(list_zip):\n",
    "    zipfilePath = (file)\n",
    "    zip = zipfile.ZipFile(zipfilePath)\n",
    "    zip.extractall(\".\")\n",
    "    zip.close()\n",
    "    col = moveCSVStock(pathcsv, pathsave)\n",
    "    list_columns_csv.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "len(list_columns_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "with open('conformiter_stock_initial.json', 'w') as outfile:\n",
    "    json.dump(list_columns_csv, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Stock partiel\n",
    "\n",
    "Les données de 2019 ne sont pas disponibles entièrement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "for year in [2018, 2019]:\n",
    "    path = r\"G:\\Projets\\PP2019\\2019_PROGRAMME BATICA\\DATA\\INPI\\sources\\IMR_Donnees_Saisies\\tc\\stock\\{}\".format(year)\n",
    "    list_zip = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        # Test-> on veut pas avant mai 2018\n",
    "        for name in files:\n",
    "            if name.endswith((\".zip\")):\n",
    "                path_xml = \"{}\\{}\".format(\n",
    "                    root,\n",
    "                    name)\n",
    "                list_zip.append(path_xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "len(list_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "pathsave = r\"C:\\Users\\PERNETTH\\Documents\\Projects\\GitLab\\CreationDataBase\\INPI\\TC_1\\Stock\\Stock_partiel\"\n",
    "pathcsv = os.getcwd()\n",
    "list_columns_csv = []\n",
    "for file in tqdm(list_zip):\n",
    "    zipfilePath = (file)\n",
    "    zip = zipfile.ZipFile(zipfilePath)\n",
    "    zip.extractall(\".\")\n",
    "    zip.close()\n",
    "    col = moveCSVStock(pathcsv, pathsave)\n",
    "    list_columns_csv.append(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Flux\n",
    "\n",
    "Données de 2018 et 2019 incomplètent.\n",
    "\n",
    "- Les données de 2018 ne proviennent pas de l'INPI mais de http://data.cquest.org/inpi_rncs/imr/\n",
    "- Données 2019 manquantes après mai\n",
    "\n",
    "Pour la conformité, ouvrir `Conformite.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#from shutil import copyfile\n",
    "def moveCSVFlux(file,pathsave):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    log_columns = []\n",
    "    #for file in glob.glob(\"{}\\*.csv\".format(pathcsv)):\n",
    "    for i in [\"PM\", \"PP\", \"rep\", \"ets\", \"actes\", \"comptes_annuels\", \"obs\"]:\n",
    "        filename = os.path.basename(file)\n",
    "        matches = re.search(i, filename)\n",
    "        if matches:\n",
    "            ### Check if EVT, if EVT, move to EVT folder\n",
    "            matches_evt = re.search(\"EVT\", filename)\n",
    "            if matches_evt:\n",
    "                new_des = \"{}\\{}\\EVT\\{}\".format(pathsave, i.upper(), filename)\n",
    "            else:\n",
    "                new_des = \"{}\\{}\\{}\\{}\".format(pathsave, i.upper(),'NEW', filename)\n",
    "                \n",
    "            dic_files_col = {}\n",
    "            \n",
    "            #print(new_des)\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                reader = csv.reader(f)\n",
    "\n",
    "                col = next(reader)\n",
    "                dic_files_col[\"filename\"] = filename\n",
    "                dic_files_col[\"origin\"] = i\n",
    "                dic_files_col[\"columns\"] = col\n",
    "                row_count = sum(1 for line in f)\n",
    "                dic_files_col[\"rows_count\"] = row_count\n",
    "            log_columns.append(dic_files_col)\n",
    "                ### save as gzip\n",
    "            shutil.copyfile(file, new_des)\n",
    "\n",
    "    return log_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Problème avec les deux fichiers suivants:\n",
    "\n",
    "- ['G:\\\\Projets\\\\PP2019\\\\2019_PROGRAMME BATICA\\\\DATA\\\\INPI\\\\sources\\\\IMR_Donnees_Saisies\\\\tc\\\\Flux\\\\2017\\\\05\\\\24\\\\5601\\\\5\\\\5601_5_20170512_213441_11_obs.csv',\n",
    "- 'G:\\\\Projets\\\\PP2019\\\\2019_PROGRAMME BATICA\\\\DATA\\\\INPI\\\\sources\\\\IMR_Donnees_Saisies\\\\tc\\\\Flux\\\\2017\\\\05\\\\24\\\\8401\\\\5\\\\8401_5_20170512_212823_11_obs.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Batch\n",
    "\n",
    "On doit faire de 05 à 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for mois in tqdm([\"01\", \"02\", \"03\", \"04\", \"05\"\n",
    "    #\"05\",\"06\", \"07\", \n",
    "                  #\"08\", \"09\", \"10\", \"11\", \"12\"]\n",
    "                 ]):\n",
    "    list_failed = []\n",
    "    list_zip = []\n",
    "    path = r\"G:\\Projets\\PP2019\\2019_PROGRAMME BATICA\\DATA\\INPI\\sources\\IMR_Donnees_Saisies\\tc\\Flux\\2019\\{}\".format(mois)\n",
    "    pathsave = r\"C:\\Users\\PERNETTH\\Documents\\Projects\\GitLab\\CreationDataBase\\INPI\\TC_1\\Flux\\2019\\{}\".format(mois)\n",
    "    for root, dirs, files in os.walk(path):\n",
    "    # Test-> on veut pas avant mai 2018\n",
    "        for name in files:\n",
    "            if name.endswith((\".csv\")):\n",
    "                path_xml = \"{}\\{}\".format(\n",
    "                    root,\n",
    "                    name)\n",
    "                list_zip.append(path_xml)\n",
    "                \n",
    "    pathcsv = os.getcwd()\n",
    "    list_columns_csv = []\n",
    "    \n",
    "    for file in tqdm(list_zip):\n",
    "        try:\n",
    "            col = moveCSVFlux(file, pathsave)\n",
    "        except:\n",
    "            list_failed.append(file)\n",
    "        list_columns_csv.append(col)\n",
    "    with open(\"2019_list_failed_{}.txt\".format(mois), \"w\") as file:\n",
    "        file.write(str(list_failed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for mois in tqdm([\"05\",\"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]):\n",
    "    #list_zip = []\n",
    "    path = r\"G:\\Projets\\PP2019\\2019_PROGRAMME BATICA\\DATA\\INPI\\sources\\IMR_Donnees_Saisies\\tc\\Flux\\2017\\{}\".format(mois)\n",
    "    pathsave = r\"C:\\Users\\PERNETTH\\Documents\\Projects\\GitLab\\CreationDataBase\\INPI\\TC_1\\Flux\\2017\\{}\".format(mois)\n",
    "    print(path)\n",
    "    print(pathsave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "with open('conformiteflux.json', 'w') as outfile:\n",
    "    json.dump(list_columns_csv, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Remove all CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "pathsave = r\"C:\\Users\\PERNETTH\\Documents\\Projects\\GitLab\\CreationDataBase\\INPI\\TC_1\\Flux\\2017\\08\"\n",
    "for root, dirs, files in os.walk(pathsave):\n",
    "    # Test-> on veut pas avant mai 2018\n",
    "    for name in files:\n",
    "        if name.endswith((\".csv\")):\n",
    "            toremove = \"{}\\{}\".format(root, name)\n",
    "            os.remove(toremove)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
