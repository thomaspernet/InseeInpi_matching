{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nettoyage variable enseigne inpi \n",
    "\n",
    "Copy paste from Coda to fill the information\n",
    "\n",
    "## Objective(s)\n",
    "\n",
    "*   Nous avons réalisé des tests de similarité entre l’enseigne de l’INPI et l’enseigne de l’adresse et il est avéré que l’INPI ne normalise pas suffisamment la variable pour être comparé en l’état.\n",
    "* Dans cette US, nous allons préparer la variable enseigne en supprimant les caractères spéciaux puis en mettant le texte en majuscule \n",
    "\n",
    "## Metadata \n",
    "\n",
    "* Metadata parameters are available here: \n",
    "* US Title: Nettoyage variable enseigne inpi \n",
    "* Epic: Epic 5\n",
    "* US: US 7\n",
    "* Date Begin: 8/31/2020\n",
    "* Duration Task: 1\n",
    "* Status:  \n",
    "* Source URL:[US 07 Preparation siretisation](https://coda.io/d/_dCtnoqIftTn/US-07-Preparation-siretisation_suFb9)\n",
    "* Task type:\n",
    "  * Jupyter Notebook\n",
    "* Users: :\n",
    "  * Thomas Pernet\n",
    "* Watchers:\n",
    "  * Thomas Pernet\n",
    "* Estimated Log points:\n",
    "  * One being a simple task, 15 a very difficult one\n",
    "  *  3\n",
    "* Task tag\n",
    "  *  #sql-query,#preparation-inpi\n",
    "* Toggl Tag\n",
    "  * #data-preparationtance [AWS]\n",
    "  *   \n",
    "  \n",
    "## Input Cloud Storage [AWS]\n",
    "\n",
    "If link from the internet, save it to the cloud first\n",
    "\n",
    "### Tables [AWS]\n",
    "\n",
    "1. Batch 1:\n",
    "  * Select Provider: Athena\n",
    "  * Select table(s): ets_final_sql \n",
    "    * Select only tables created from the same notebook, else copy/paste selection to add new input tables\n",
    "    * If table(s) does not exist, add them: Add New Table\n",
    "    * Information:\n",
    "      * Region: \n",
    "        * NameEurope (Paris)\n",
    "        * Code: eu-west-3\n",
    "      * Database: inpi\n",
    "      * Notebook construction file: \n",
    "        * https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/03_ETS_add_variables.md\n",
    "    \n",
    "## Destination Output/Delivery\n",
    "\n",
    "  * AWS\n",
    "    * Athena: \n",
    "      * Region: Europe (Paris)\n",
    "      * Database: siretisation\n",
    "      * Tables (Add name new table): ets_inpi_sql\n",
    "      * List new tables\n",
    "           * ets_inpi_sql\n",
    "\n",
    "## Things to know (Steps, Attention points or new flow of information)\n",
    "\n",
    "### Sources of information  (meeting notes, Documentation, Query, URL)\n",
    "\n",
    "1. Jupyter Notebook (Github Link)\n",
    "  1. md :https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/02_siretisation/07_pourcentage_siretisation_v3.md#connexion-serveur\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connexion serveur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsPy.aws_authorization import aws_connector\n",
    "from awsPy.aws_athena import service_athena\n",
    "from awsPy.aws_s3 import service_s3\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os, shutil\n",
    "\n",
    "path = os.getcwd()\n",
    "parent_path = str(Path(path).parent)\n",
    "path_cred = r\"{}/credential_AWS.json\".format(parent_path)\n",
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                       region = 'eu-west-3')\n",
    "\n",
    "region = 'eu-west-3'\n",
    "bucket = 'calfdata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                       region = region)\n",
    "client= con.client_boto()\n",
    "s3 = service_s3.connect_S3(client = client,\n",
    "                      bucket = bucket, verbose = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_setting = True\n",
    "if pandas_setting:\n",
    "    cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation tables\n",
    "\n",
    "## Steps\n",
    "\n",
    "- Prendre la table  `ets_final_sql` de l'inpi\n",
    "- Nettoyer la variable `enseigne`:\n",
    "    - enlever les accents\n",
    "    - Mise en majuscule\n",
    "\n",
    "```\n",
    "REGEXP_REPLACE(\n",
    "      NORMALIZE(enseigne, NFD), \n",
    "      '\\pM', \n",
    "      ''\n",
    "    ) AS enseigne\n",
    "```\n",
    "\n",
    "Mise a jour tableau des règles de nettoyage:\n",
    "\n",
    "\n",
    "| Table | Variables | Article | Digit | Debut/fin espace | Espace | Accent | Upper |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| INPI | adresse_regex_inpi | X | X | X | X | X | X |\n",
    "| INPI | adresse_distance_inpi | X | X | X | X | X | X |\n",
    "| INPI | adresse_reconstituee_inpi |  |  | X | X | X | X |\n",
    "|INPI| enseigne |  |  |  |  | X |X |\n",
    "| INSEE | adresse_reconstituee_insee |  |  | X | X | X | X |\n",
    "| INSEE | adresse_distance_insee | X | X | X | X | X | X |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output = 'INPI/sql_output'\n",
    "database = 'inpi'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple INPUT / output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH data_ AS (\n",
    "SELECT index_id, enseigne\n",
    "FROM inpi.ets_final_sql \n",
    "WHERE enseigne != ''\n",
    "\n",
    ")\n",
    "SELECT data_.index_id, data_.enseigne as input, output\n",
    "FROM data_\n",
    "INNER JOIN (\n",
    "  SELECT index_id, \n",
    "  REGEXP_REPLACE(\n",
    "      NORMALIZE(enseigne, NFD), \n",
    "      '\\pM', \n",
    "      ''\n",
    "    ) AS output\n",
    "            FROM \n",
    "  data_\n",
    "            ) as tb_enseigne\n",
    "            ON data_.index_id = tb_enseigne.index_id\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database=database,\n",
    "            s3_output=s3_output,\n",
    "  filename = 'exemple_enseigne', ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "CREATE DATABASE IF NOT EXISTS siretisation\n",
    "  COMMENT 'DB avec tb pour la siretisation'\n",
    "  LOCATION 's3://calfdata/inpi/SIRETISATION/'\n",
    "\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query =\"\"\"\n",
    "DROP TABLE `siretisation.ets_inpi_sql`;\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database=database,\n",
    "            s3_output=s3_output,\n",
    "  filename = None, ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "CREATE TABLE siretisation.ets_inpi_sql\n",
    "WITH (\n",
    "  format='PARQUET'\n",
    ") AS\n",
    "SELECT index_id, sequence_id, siren, code_greffe, nom_greffe, numero_gestion, id_etablissement, status, origin, date_greffe, file_timestamp, libelle_evt, last_libele_evt, status_admin, type, status_ets, \"siège_pm\", rcs_registre, adresse_ligne1, adresse_ligne2, adresse_ligne3, adresse_reconstituee_inpi, adresse_regex_inpi, adresse_distance_inpi, list_numero_voie_matching_inpi, numero_voie_matching, voie_clean, type_voie_matching, code_postal, code_postal_matching, ville, ville_matching, code_commune, pays, domiciliataire_nom, domiciliataire_siren, domiciliataire_greffe, \"domiciliataire_complément\", \"siege_domicile_représentant\", nom_commercial, REGEXP_REPLACE(\n",
    "      NORMALIZE(enseigne, NFD), \n",
    "      '\\pM', \n",
    "      ''\n",
    "    ) AS enseigne, \"activité_ambulante\" , \"activité_saisonnière\", \"activité_non_sédentaire\", \"date_début_activité\", \"activité\", origine_fonds, origine_fonds_info, type_exploitation, csv_source\n",
    "FROM inpi.ets_final_sql \n",
    "\"\"\"\n",
    "\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database=database,\n",
    "            s3_output=s3_output,\n",
    "  filename = None, ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "\n",
    "- Imprimer 10 lignes avec des enseignes différentes de null, avec deux colonnes, une colonne input et une colonne output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH data_ AS (\n",
    "SELECT index_id, enseigne\n",
    "FROM siretisation.ets_inpi_sql\n",
    "WHERE enseigne != ''\n",
    "\n",
    ")\n",
    "SELECT data_.index_id, data_.enseigne as input, output\n",
    "FROM data_\n",
    "INNER JOIN (\n",
    "  SELECT index_id, \n",
    "  REGEXP_REPLACE(\n",
    "      NORMALIZE(enseigne, NFD), \n",
    "      '\\pM', \n",
    "      ''\n",
    "    ) AS output\n",
    "            FROM \n",
    "  data_\n",
    "            ) as tb_enseigne\n",
    "            ON data_.index_id = tb_enseigne.index_id\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database=database,\n",
    "            s3_output=s3_output,\n",
    "  filename = 'exemple_enseigne', ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, shutil, urllib, ipykernel, json\n",
    "from pathlib import Path\n",
    "from notebook import notebookapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_report(extension = \"html\", keep_code = False):\n",
    "    \"\"\"\n",
    "    Create a report from the current notebook and save it in the \n",
    "    Report folder (Parent-> child directory)\n",
    "    \n",
    "    1. Exctract the current notbook name\n",
    "    2. Convert the Notebook \n",
    "    3. Move the newly created report\n",
    "    \n",
    "    Args:\n",
    "    extension: string. Can be \"html\", \"pdf\", \"md\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ### Get notebook name\n",
    "    connection_file = os.path.basename(ipykernel.get_connection_file())\n",
    "    kernel_id = connection_file.split('-', 1)[0].split('.')[0]\n",
    "\n",
    "    for srv in notebookapp.list_running_servers():\n",
    "        try:\n",
    "            if srv['token']=='' and not srv['password']:  \n",
    "                req = urllib.request.urlopen(srv['url']+'api/sessions')\n",
    "            else:\n",
    "                req = urllib.request.urlopen(srv['url']+ \\\n",
    "                                             'api/sessions?token=' + \\\n",
    "                                             srv['token'])\n",
    "            sessions = json.load(req)\n",
    "            notebookname = sessions[0]['name']\n",
    "        except:\n",
    "            pass  \n",
    "    \n",
    "    sep = '.'\n",
    "    path = os.getcwd()\n",
    "    #parent_path = str(Path(path).parent)\n",
    "    \n",
    "    ### Path report\n",
    "    #path_report = \"{}/Reports\".format(parent_path)\n",
    "    #path_report = \"{}/Reports\".format(path)\n",
    "    \n",
    "    ### Path destination\n",
    "    name_no_extension = notebookname.split(sep, 1)[0]\n",
    "    source_to_move = name_no_extension +'.{}'.format(extension)\n",
    "    dest = os.path.join(path,'Reports', source_to_move)\n",
    "    \n",
    "    ### Generate notebook\n",
    "    if keep_code:\n",
    "        os.system('jupyter nbconvert --to {} {}'.format(\n",
    "    extension,notebookname))\n",
    "    else:\n",
    "        os.system('jupyter nbconvert --no-input --to {} {}'.format(\n",
    "    extension,notebookname))\n",
    "    \n",
    "    ### Move notebook to report folder\n",
    "    #time.sleep(5)\n",
    "    shutil.move(source_to_move, dest)\n",
    "    print(\"Report Available at this adress:\\n {}\".format(dest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_report(extension = \"html\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nteract": {
   "version": "0.24.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
