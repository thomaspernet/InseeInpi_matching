{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation INPI-ETS\n",
    "\n",
    "Dans ce notebook, on prepare la donnée ETS afin d'être concatenée, puis envoyée dans le S3.\n",
    "- https://docs.aws.amazon.com/athena/latest/ug/csv.html\n",
    "\n",
    "L'idée est de récupérer les stocks initiaux, partiels,créations et les événements pour créer un csv avec tout ce qui c'est passé au cours d'une année donnée tout en remplissant les éléments manquants.\n",
    "\n",
    "Il y a 3 dossiers source:\n",
    "\n",
    "- [Stock initial](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/TC_1/01_donnee_source/Stock/Stock_Initial/)\n",
    "- [Stock partiel](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/TC_1/01_donnee_source/Stock/Stock_Partiel/)\n",
    "- [Flux](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/TC_1/01_donnee_source/Flux/)\n",
    "\n",
    "\n",
    "On détaille ici le process pour les Etablissements, mais il faut noter qu'il y a un notebook qui reprend exactement la même structure pour chaque catégorie:\n",
    "- ACTES\n",
    "- COMPTES_ANNUELS\n",
    "- ETS\n",
    "- OBS\n",
    "- PM\n",
    "- PP\n",
    "- REP\n",
    "\n",
    "\n",
    "Dans chacun des dossiers, on va récupérer la catégorie qui nous intéresse, selon l'année:\n",
    "\n",
    "```\n",
    "01_donnees_source\n",
    "    ├───Flux\n",
    "    │   ├───2017\n",
    "    │   │   ├───ACTES\n",
    "    │   │   │   └───NEW\n",
    "    │   │   ├───COMPTES_ANNUELS\n",
    "    │   │   │   └───NEW\n",
    "    │   │   ├───ETS\n",
    "    │   │   │   ├───EVT\n",
    "    │   │   │   └───NEW\n",
    "    │   │   ├───OBS\n",
    "    │   │   │   └───NEW\n",
    "    │   │   ├───PM\n",
    "    │   │   │   ├───EVT\n",
    "    │   │   │   └───NEW\n",
    "    │   │   ├───PP\n",
    "    │   │   │   ├───EVT\n",
    "    │   │   │   └───NEW\n",
    "    │   │   └───REP\n",
    "    │   │       ├───EVT\n",
    "    │   │       └───NEW\n",
    "    │   ├───2018\n",
    "    │   │   ├───ACTES\n",
    "    │   │   │   └───NEW\n",
    "    │   │   ├───COMPTES_ANNUELS\n",
    "    │   │   │   └───NEW\n",
    "    │   │   ├───ETS\n",
    "    │   │   │   ├───EVT\n",
    "    │   │   │   └───NEW\n",
    "    │   │   ├───OBS\n",
    "    │   │   │   └───NEW\n",
    "    │   │   ├───PM\n",
    "    │   │   │   ├───EVT\n",
    "    │   │   │   └───NEW\n",
    "    │   │   ├───PP\n",
    "    │   │   │   ├───EVT\n",
    "    │   │   │   └───NEW\n",
    "    │   │   └───REP\n",
    "    │   │       ├───EVT\n",
    "    │   │       └───NEW\n",
    "    │   └───2019\n",
    "    │   │   ├───ACTES\n",
    "    │   │   │   └───NEW\n",
    "    │   │   ├───COMPTES_ANNUELS\n",
    "    │   │   │   └───NEW\n",
    "    │   │   ├───ETS\n",
    "    │   │   │   ├───EVT\n",
    "    │   │   │   └───NEW\n",
    "    │   │   ├───OBS\n",
    "    │   │   │   └───NEW\n",
    "    │   │   ├───PM\n",
    "    │   │   │   ├───EVT\n",
    "    │   │   │   └───NEW\n",
    "    │   │   ├───PP\n",
    "    │   │   │   ├───EVT\n",
    "    │   │   │   └───NEW\n",
    "    │   │   └───REP\n",
    "    │   │       ├───EVT\n",
    "    │   │       └───NEW\n",
    "    └───Stock\n",
    "        ├───Stock_initial\n",
    "            ├───2017\n",
    "            │   ├───ACTES\n",
    "            │   ├───COMPTES_ANNUELS\n",
    "            │   ├───ETS\n",
    "            │   ├───OBS\n",
    "            │   ├───PM\n",
    "            │   ├───PP\n",
    "            │   └───REP\n",
    "        └───Stock_partiel\n",
    "            ├───2018\n",
    "            │   ├───ACTES\n",
    "            │   ├───COMPTES_ANNUELS\n",
    "            │   ├───ETS\n",
    "            │   ├───OBS\n",
    "            │   ├───PM\n",
    "            │   ├───PP\n",
    "            │   └───REP\n",
    "            ├───2019\n",
    "            │   ├───ACTES\n",
    "            │   ├───COMPTES_ANNUELS\n",
    "            │   ├───ETS\n",
    "            │   ├───OBS\n",
    "            │   ├───PM\n",
    "            │   ├───PP\n",
    "            │   └───REP\n",
    "            └───2020\n",
    "                ├───ACTES\n",
    "                ├───COMPTES_ANNUELS\n",
    "                ├───ETS\n",
    "                ├───OBS\n",
    "                ├───PM\n",
    "                ├───PP\n",
    "                └───REP\n",
    "```\n",
    "\n",
    "Exemple pour les Etablissements\n",
    "\n",
    "- Stock:\n",
    "    - [Stock initial](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/TC_1/01_donnee_source/Stock/Stock_initial/ETS/)\n",
    "    - [Stock partiel 2018](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/TC_1/01_donnee_source/Stock/Stock_partiel/2018/ETS)\n",
    "- Flux\n",
    "    - [NEW 2017](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/TC_1/01_donnee_source/Flux/2017/ETS/NEW/)\n",
    "    - [EVT 2017](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/TC_1/01_donnee_source/Flux/2017/ETS/EVT/)\n",
    "\n",
    "\n",
    "## Steps: Benchmark ETS\n",
    "\n",
    "- Step 1: Parametres et queries\n",
    "\t- [Préparation parametres json](https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/01_Athena_concatenate_ETS.md#pr%C3%A9paration-json-parameters)\n",
    "\t- [Query préparation table](https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/01_Athena_concatenate_ETS.md#query-pr%C3%A9paration-table)\n",
    "    - [Query preparation evt](https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/01_Athena_concatenate_ETS.md#query-pr%C3%A9paration-%C3%A9v%C3%A9nement)\n",
    "\t- [Query preparation partiel](https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/01_Athena_concatenate_ETS.md#query-preparation-partiel)\n",
    "\t- [Query remplissage EVT](https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/01_Athena_concatenate_ETS.md#query-remplissage-evt)\n",
    "\n",
    "- [Step 2](https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/01_Athena_concatenate_ETS.md#step-2-concatenation-data): Concatenation et preparation data\n",
    "\t- Stock\n",
    "\t\t- [Initial](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/TC_1/01_donnee_source/Stock/Stock_Initial/)\n",
    "\t\t- [Partiel](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/TC_1/01_donnee_source/Stock/Stock_Partiel/)\n",
    "\t- Flux\n",
    "\t\t- [NEW](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/TC_1/01_donnee_source/Flux/2017/ETS/NEW/)\n",
    "\t\t- [EVT](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/TC_1/01_donnee_source/Flux/2017/ETS/EVT/)\n",
    "\t\t\t- Remplissage valeurs manquantes\n",
    "\n",
    "- Step 3: Creation table Initial/Partiel/EVT/NEW\n",
    "\n",
    "- Step 4: Creation statut partiel\n",
    "\t- Création colonne status qui indique si les lignes sont a ignorer ou non\n",
    "\n",
    "- Step 5: Remplissage observations manquantes\n",
    "\t- Récupération informations selon `Origin` (`Stock` ou `NEW`) pour compléter les valeurs manquantes des `EVT` \n",
    "    \n",
    "    \n",
    "    \n",
    "## Tables Athena:\n",
    "\n",
    "- ets_evt_2017: [Step 2](https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/01_Athena_concatenate_ETS.md#step-2-concatenation-data)\n",
    "- ets_evt_2018: [Step 2](https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/01_Athena_concatenate_ETS.md#step-2-concatenation-data)\n",
    "- ets_evt_2019: [Step 2](https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/01_Athena_concatenate_ETS.md#step-2-concatenation-data)\n",
    "- ets_initial: [Step 2](https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/01_Athena_concatenate_ETS.md#step-2-concatenation-data)\n",
    "- ets_new_2017: [Step 2](https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/01_Athena_concatenate_ETS.md#step-2-concatenation-data)\n",
    "- ets_new_2018: [Step 2](https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/01_Athena_concatenate_ETS.md#step-2-concatenation-data)\n",
    "- ets_new_2019: [Step 2](https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/01_Athena_concatenate_ETS.md#step-2-concatenation-data)\n",
    "- ets_partiel_2018: [Step 2](https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/01_Athena_concatenate_ETS.md#step-2-concatenation-data)\n",
    "- ets_partiel_2019: [Step 2](https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/01_Athena_concatenate_ETS.md#step-2-concatenation-data)\n",
    "- initial_partiel_evt_new_etb: [Step 3](https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/01_Athena_concatenate_ETS.md#step-3-creation-table-initialpartielevtnew)\n",
    "- initial_partiel_evt_new_ets_status: [Step 4](https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/01_Athena_concatenate_ETS.md#step-4-creation-statut-partiel)\n",
    "- initial_partiel_evt_new_ets_status_final: [Step 5](https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/01_Athena_concatenate_ETS.md#step-5-remplissage-observations-manquantes)\n",
    "\n",
    "## Regles de gestion considérées\n",
    "\n",
    "*  Une séquence est un classement chronologique pour le triplet (quadruplet pour les Etablissements) suivant:\n",
    "  * siren + code greffe + numero gestion (+ ID établissement pour les Etablissements)\n",
    "  * Dans la table finale, le classement a cette forme : siren,Nom_Greffe, \"code greffe\",numero_gestion, (id_etablissement pour les Etablissements) , file_timestamp\n",
    "* Une ligne ``partiel`` va rendre caduque l'ensemble des séquences précédentes.\n",
    "* Une ligne ``événement`` ne modifie que le champs comportant la modification. Les champs non modifiés vont être remplis par la ligne t-1\n",
    "    * Le remplissage doit se faire de deux façons\n",
    "        * une première fois avec la date de transmission (plusieurs informations renseignées pour une meme date de transmission pour une même séquence). La dernière ligne remplie des valeurs précédentes de la séquence\n",
    "        * une seconde fois en ajoutant les valeurs non renseignées pour cet évènement, en se basant sur les informations des lignes précédentes du triplet (quadruplet pour les Etablissements). Les lignes précédentes ont une date de transmission différente et/ou initial, partiel et création."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des librairies S3\n",
    "from awsPy.aws_authorization import aws_connector\n",
    "from awsPy.aws_s3 import service_s3\n",
    "from awsPy.aws_athena import service_athena\n",
    "import os, time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connexion S3\n",
    "bucket = 'calfdata'\n",
    "path = os.getcwd()\n",
    "parent_path = str(Path(path).parent)\n",
    "path_cred = \"{}/credential_AWS.json\".format(parent_path)\n",
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                        region = 'eu-west-3')\n",
    "client= con.client_boto()\n",
    "s3 = service_s3.connect_S3(client = client,\n",
    "                      bucket = 'calfdata') \n",
    "athena = service_athena.connect_athena(client = client,\n",
    "                      bucket = 'calfdata') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Parametres et queries\n",
    "\n",
    "Pour faciliter l'ingestion de données en batch, on prépare un json ``dic_`` avec les paths où récupérer la data, le nom des tables, les origines, mais aussi un champ pour récupérer l'ID de l'execution dans Athena. En effet, chaque execution donne lieu a un ID. Certaines queries peuvent prendre plusieurs minutes. Athena crée un CSV dans un folder output prédéfini dont le nom est l'ID de la query. Notre process utilise la concaténation automatique d'Athena pour créer les tables. Il faut nécessairement déplacer les csv dans des dossiers destination en vue de la concatenation. Le stockage de l'ID est donc indispensable pour copier l'objet, surtout lorsque la query prend du temps d'execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparation json parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_ = {\n",
    "    'global':{\n",
    "        'database':'inpi',\n",
    "        'output':'INPI/sql_output',\n",
    "        'output_preparation':'INPI/sql_output_preparation',\n",
    "        'ETS_step4_id':[],\n",
    "        'table_final_id':{\n",
    "            'ETS':{\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'Stock': {\n",
    "        'INITIAL':{\n",
    "            'ETS': {\n",
    "                'path':'s3://calfdata/INPI/TC_1/01_donnee_source/Stock/Stock_Initial/2017/ETS',\n",
    "                'tables':'ets_initial',\n",
    "                'origin':'INITIAL',\n",
    "                'output_id':[]\n",
    "            }\n",
    "        },\n",
    "        'PARTIEL':{\n",
    "            'ETS': {\n",
    "                'path':[\n",
    "                    's3://calfdata/INPI/TC_1/01_donnee_source/Stock/Stock_Partiel/2018/ETS',\n",
    "                    's3://calfdata/INPI/TC_1/01_donnee_source/Stock/Stock_Partiel/2019/ETS'\n",
    "                       ],\n",
    "                'tables':[\n",
    "                    'ets_partiel_2018',\n",
    "                    'ets_partiel_2019'],\n",
    "                'origin':'PARTIEL',\n",
    "                'output_id':[]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'Flux': {\n",
    "        'NEW':{\n",
    "            'ETS': {\n",
    "                'path':[\n",
    "                    's3://calfdata/INPI/TC_1/01_donnee_source/Flux/2017/ETS/NEW',\n",
    "                    's3://calfdata/INPI/TC_1/01_donnee_source/Flux/2018/ETS/NEW',\n",
    "                    's3://calfdata/INPI/TC_1/01_donnee_source/Flux/2019/ETS/NEW'\n",
    "                       ],\n",
    "                'tables':[\n",
    "                    'ets_new_2017',\n",
    "                    'ets_new_2018',\n",
    "                    'ets_new_2019'],\n",
    "                'origin':'NEW',\n",
    "                'output_id':[]\n",
    "            }\n",
    "        },\n",
    "        'EVT':{\n",
    "            'ETS': {\n",
    "                'path':[\n",
    "                    's3://calfdata/INPI/TC_1/01_donnee_source/Flux/2017/ETS/EVT',\n",
    "                    's3://calfdata/INPI/TC_1/01_donnee_source/Flux/2018/ETS/EVT',\n",
    "                    's3://calfdata/INPI/TC_1/01_donnee_source/Flux/2019/ETS/EVT',\n",
    "                ],\n",
    "                'tables':[\n",
    "                    'ets_evt_2017',\n",
    "                    'ets_evt_2018',\n",
    "                    'ets_evt_2019'],\n",
    "                'origin':'EVT',\n",
    "                'output_id':[]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query préparation table\n",
    "\n",
    "On prédéfini les requêtes qui seront à éxecuter dans Athena. Les paramètres des queries seront récupérés dans ``dic_`` au moment de l'éxecution de la query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### query_db = \"CREATE DATABASE IF NOT EXISTS {};\"\n",
    "\n",
    "query_tb = \\\n",
    "    \"\"\"CREATE EXTERNAL TABLE IF NOT EXISTS {0}.{1} (\n",
    "`Code_Greffe`                     string,\n",
    "`Nom_Greffe`                      string,\n",
    "`Numero_Gestion`                  string,\n",
    "`Siren`                            string,\n",
    "`Type`                            string,\n",
    "`Siège_PM`                        string,\n",
    "`RCS_Registre`                   string,\n",
    "`Adresse_Ligne1`                  string,\n",
    "`Adresse_Ligne2`                  string,\n",
    "`Adresse_Ligne3`                  string,\n",
    "`Code_Postal`                    string,\n",
    "`Ville`                           string,\n",
    "`Code_Commune`                   string,\n",
    "`Pays`                            string,\n",
    "`Domiciliataire_Nom`              string,\n",
    "`Domiciliataire_Siren`           string,\n",
    "`Domiciliataire_Greffe`           string,\n",
    "`Domiciliataire_Complément`      string,\n",
    "`Siege_Domicile_Représentant`     string,\n",
    "`Nom_Commercial`                  string,\n",
    "`Enseigne`                       string,\n",
    "`Activité_Ambulante`              string,\n",
    "`Activité_Saisonnière`            string,\n",
    "`Activité_Non_Sédentaire`         string,\n",
    "`Date_Début_Activité`             string,\n",
    "`Activité`                        string,\n",
    "`Origine_Fonds`                   string,\n",
    "`Origine_Fonds_Info`              string,\n",
    "`Type_Exploitation`               string,\n",
    "`ID_Etablissement`                 string,\n",
    "`Date_Greffe`                     string,\n",
    "`Libelle_Evt`                     string,\n",
    "`csv_source` string,\n",
    "`nature` string,\n",
    "`type_data` string,\n",
    "`origin` string,\n",
    "`file_timestamp` string\n",
    "    )\n",
    "     ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
    "    WITH SERDEPROPERTIES (\n",
    "   'separatorChar' = '{3}',\n",
    "   'quoteChar' = '\"'\n",
    "   )\n",
    "     LOCATION '{2}'\n",
    "     TBLPROPERTIES ('has_encrypted_data'='false',\n",
    "              'skip.header.line.count'='1');\"\"\"\n",
    "\n",
    "query_table_concat = \\\n",
    "    \"\"\"CREATE EXTERNAL TABLE IF NOT EXISTS {0}.{1} (\n",
    "`siren` string, \n",
    "`code_greffe` int, \n",
    "`Nom_Greffe` string, \n",
    "`numero_gestion` string, \n",
    "`id_etablissement` string,  \n",
    "`file_timestamp` string, \n",
    "`Libelle_Evt` string, \n",
    "`Date_Greffe` string,     \n",
    "`Type` string, \n",
    "`Siège_PM` string, \n",
    "`RCS_Registre` string, \n",
    "`Adresse_Ligne1` string, \n",
    "`Adresse_Ligne2` string, \n",
    "`Adresse_Ligne3` string, \n",
    "`Code_Postal` string, \n",
    "`Ville` string, \n",
    "`Code_Commune` string, \n",
    "`Pays` string, \n",
    "`Domiciliataire_Nom` string, \n",
    "`Domiciliataire_Siren` string, \n",
    "`Domiciliataire_Greffe` string, \n",
    "`Domiciliataire_Complément` string, \n",
    "`Siege_Domicile_Représentant` string, \n",
    "`Nom_Commercial` string, \n",
    "`Enseigne` string, \n",
    "`Activité_Ambulante` string, \n",
    "`Activité_Saisonnière` string, \n",
    "`Activité_Non_Sédentaire` string, \n",
    "`Date_Début_Activité` string, \n",
    "`Activité` string, \n",
    "`Origine_Fonds` string, \n",
    "`Origine_Fonds_Info` string, \n",
    "`Type_Exploitation` string, \n",
    "`csv_source` string, \n",
    "`origin` string\n",
    "    )\n",
    "     ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
    "    WITH SERDEPROPERTIES (\n",
    "   'separatorChar' = ',',\n",
    "   'quoteChar' = '\"'\n",
    "   )\n",
    "     LOCATION '{2}'\n",
    "     TBLPROPERTIES ('has_encrypted_data'='false',\n",
    "              'skip.header.line.count'='1');\"\"\"\n",
    "\n",
    "query_drop = \"\"\" DROP TABLE `{}`;\"\"\"\n",
    "\n",
    "query_select = \"\"\"SELECT \n",
    "\"siren\",\n",
    "\"code_greffe\",\n",
    "\"Nom_Greffe\",\n",
    "\"numero_gestion\",\n",
    "\"id_etablissement\", \n",
    "\"file_timestamp\",\n",
    "\"Libelle_Evt\",\n",
    "\"Date_Greffe\",    \n",
    "\"Type\",\n",
    "\"Siège_PM\",\n",
    "\"RCS_Registre\",\n",
    "\"Adresse_Ligne1\",\n",
    "\"Adresse_Ligne2\",\n",
    "\"Adresse_Ligne3\",\n",
    "\"Code_Postal\",\n",
    "\"Ville\",\n",
    "\"Code_Commune\",\n",
    "\"Pays\",\n",
    "\"Domiciliataire_Nom\",\n",
    "\"Domiciliataire_Siren\",\n",
    "\"Domiciliataire_Greffe\",\n",
    "\"Domiciliataire_Complément\",\n",
    "\"Siege_Domicile_Représentant\",\n",
    "\"Nom_Commercial\",\n",
    "\"Enseigne\",\n",
    "\"Activité_Ambulante\",\n",
    "\"Activité_Saisonnière\",\n",
    "\"Activité_Non_Sédentaire\",\n",
    "\"Date_Début_Activité\",\n",
    "\"Activité\",\n",
    "\"Origine_Fonds\",\n",
    "\"Origine_Fonds_Info\",\n",
    "\"Type_Exploitation\",\n",
    "\"csv_source\",\n",
    "\"origin\"\n",
    "FROM \"inpi\".\"{}\"\n",
    "WHERE \"siren\" !=''\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query préparation événement\n",
    "\n",
    "La query est générée via un loop dans l'étape 3 afin d'éviter les copier/coller redondants. Dans l'ensemble, la query va reconstruire l'ensemble des valeurs manquantes pour chaque csv (ie date de transmission). A noter que la query va récupérer la dernière ligne du quadruplet `siren`,`code greffe`, `numero_gestion`, `id_etablissement`. \n",
    "\n",
    "La liste des champs pouvant être affectés par un changement est stockée dans `list_change`.\n",
    "\n",
    "#### Detail\n",
    "\n",
    "Il y a des événements ayant plusieurs entrées a des moments identiques affectant les mêmes établissements.\n",
    "Quelle est la règle à appliquer lorsqu'un événement est effectué le même jour, à la même heure pour un même établissement (même quadruplet: siren + code greffe + numero gestion + ID établissement)?\n",
    "\n",
    "Reponse INPI:\n",
    "  * \"Il faut modifier, en respectant l’ordre des lignes, uniquement les champs où il y a une valeur.\"\n",
    "  * Oui, il faut additionner les champs modifiés. Attention si un même champ est modifié (avec des valeurs différentes) en ligne 2, puis en ligne 3, il faudra privilégier les valeurs de la ligne 3.\n",
    "\n",
    "Exemple:\n",
    "\n",
    "* SIREN: [420844656](https://calfdata.s3.eu-west-3.amazonaws.com/INPI/TC_1/02_preparation_donnee/check_csv/420844656.csv)\n",
    "  * même établissement, plusieurs entrées\n",
    "    - exemple SIREN 420844656, évenement effectué le 2018/01/03 a 08:48:10. Nom dans le FTP `0101_163_20180103_084810_9_ets_nouveau_modifie_EVT.csv`\n",
    "    * fichier [source](https://calfdata.s3.eu-west-3.amazonaws.com/INPI/TC_1/01_donnee_source/Flux/2018/ETS/EVT/0101_163_20180103_084810_9_ets_nouveau_modifie_EVT.csv)\n",
    "    \n",
    "    \n",
    "#### Test de vérification\n",
    "\n",
    "* Exemple avec SIREN [513913657](https://calfdata.s3.eu-west-3.amazonaws.com/INPI/TC_1/02_preparation_donnee/check_csv/513913657.xlsx):\n",
    "  * Raw data dans S3\n",
    "```\n",
    "    * [\"INPI/TC_1/01_donnee_source/Flux/2018/ETS/EVT/3801_189_20180130_065752_9_ets_nouveau_modifie_EVT.csv\",\n",
    "    * \"INPI/TC_1/01_donnee_source/Flux/2018/ETS/EVT/3801_190_20180131_065908_9_ets_nouveau_modifie_EVT.csv\",\n",
    "    * \"INPI/TC_1/01_donnee_source/Flux/2018/ETS/EVT/3801_209_20180227_065600_9_ets_nouveau_modifie_EVT.csv\",\n",
    "    * \"INPI/TC_1/01_donnee_source/Flux/2018/ETS/EVT/3801_213_20180303_064240_9_ets_nouveau_modifie_EVT.csv\",\n",
    "    * \"INPI/TC_1/01_donnee_source/Flux/2018/ETS/EVT/3801_222_20180316_063210_9_ets_nouveau_modifie_EVT.csv\",\n",
    "    * \"INPI/TC_1/01_donnee_source/Flux/2018/ETS/EVT/3801_293_20180627_061209_9_ets_nouveau_modifie_EVT.csv\",\n",
    "    * \"INPI/TC_1/01_donnee_source/Flux/2018/ETS/EVT/3801_301_20180711_065600_9_ets_nouveau_modifie_EVT.csv\"]\n",
    "```    \n",
    "\n",
    "* Details\n",
    "    * La feuille FROM_FTP regroupe tous les événements affectants le siren  513913657 en 2018 uniquement (7 au total)\n",
    "```\n",
    "      *  3801_189_20180130_065752_9_ets_nouveau_modifie_EVT.csv\n",
    "      *  3801_190_20180131_065908_9_ets_nouveau_modifie_EVT.csv\n",
    "      *  3801_209_20180227_065600_9_ets_nouveau_modifie_EVT.csv\n",
    "      *  3801_213_20180303_064240_9_ets_nouveau_modifie_EVT.csv\n",
    "      *  3801_222_20180316_063210_9_ets_nouveau_modifie_EVT.csv\n",
    "      *  3801_293_20180627_061209_9_ets_nouveau_modifie_EVT.csv\n",
    "      *  3801_301_20180711_065600_9_ets_nouveau_modifie_EVT.csv\n",
    " ```\n",
    "* En tout, il y a 83 entrées. Dans la feuille `FROM_FTP`, chaque couleur représente un csv (regroupé par date de transmission). Comme indiqué par Mr Flament, il faut remplir les entrées d’un même csv par l’entrée n-1. La dernière entrée fait foi si différente avec n-1. Dans la feuille, c’est les ligne jaunes.\n",
    "* La feuille FILLIN va faire cette étape de remplissage, et la feuille FILTER récupère uniquement la dernière ligne par date de transmission. L’enseigne est indiqué comme supprimée dans la donnée brute a différentes dates de transmission mais supprimé lors de la dernière transmission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_change = [\n",
    "\"Libelle_Evt\",\n",
    "\"Date_Greffe\",    \n",
    "\"Type\",\n",
    "\"Siège_PM\",\n",
    "\"RCS_Registre\",\n",
    "\"Adresse_Ligne1\",\n",
    "\"Adresse_Ligne2\",\n",
    "\"Adresse_Ligne3\",\n",
    "\"Code_Postal\",\n",
    "\"Ville\",\n",
    "\"Code_Commune\",\n",
    "\"Pays\",\n",
    "\"Domiciliataire_Nom\",\n",
    "\"Domiciliataire_Siren\",\n",
    "\"Domiciliataire_Greffe\",\n",
    "\"Domiciliataire_Complément\",\n",
    "\"Siege_Domicile_Représentant\",\n",
    "\"Nom_Commercial\",\n",
    "\"Enseigne\",\n",
    "\"Activité_Ambulante\",\n",
    "\"Activité_Saisonnière\",\n",
    "\"Activité_Non_Sédentaire\",\n",
    "\"Date_Début_Activité\",\n",
    "\"Activité\",\n",
    "\"Origine_Fonds\",\n",
    "\"Origine_Fonds_Info\",\n",
    "\"Type_Exploitation\",\n",
    "\"csv_source\"#,\n",
    "#\"origin\"    \n",
    "#\"nature\",\n",
    "#\"type_data\",\n",
    "#\"\n",
    "]\n",
    "\n",
    "top = \"\"\"WITH createID AS (\n",
    "  SELECT \n",
    "   *, \n",
    "    ROW_NUMBER() OVER (\n",
    "      PARTITION BY \n",
    "      siren,\n",
    "      \"code_greffe\",\n",
    "      \"Nom_Greffe\",\n",
    "      numero_gestion,\n",
    "      id_etablissement, \n",
    "      file_timestamp\n",
    "    ) As row_ID, \n",
    "    DENSE_RANK () OVER (\n",
    "      ORDER BY \n",
    "        siren, \n",
    "        \"code_greffe\",\n",
    "        \"Nom_Greffe\",\n",
    "        numero_gestion, \n",
    "        id_etablissement, \n",
    "        file_timestamp\n",
    "    ) As ID \n",
    "  FROM \n",
    "    \"inpi\".\"{}\" \n",
    ") \n",
    "SELECT \n",
    "  * \n",
    "FROM \n",
    "  (\n",
    "    WITH filled AS (\n",
    "      SELECT \n",
    "        ID, \n",
    "        row_ID, \n",
    "        siren, \n",
    "        \"Nom_Greffe\",\n",
    "        \"code_greffe\", \n",
    "        numero_gestion, \n",
    "        id_etablissement, \n",
    "        file_timestamp, \n",
    "\"\"\"\n",
    "\n",
    "top_1 = \"\"\"first_value(\"{0}\") over (partition by ID, \"{0}_partition\" order by \n",
    "ID, row_ID\n",
    " ) as \"{0}\"\n",
    "\"\"\"\n",
    "\n",
    "middle = \"\"\"FROM \n",
    "        (\n",
    "          SELECT \n",
    "            *, \"\"\"\n",
    "\n",
    "middle_2 = \"\"\"sum(case when \"{0}\" = '' then 0 else 1 end) over (partition by ID \n",
    "order by  row_ID) as \"{0}_partition\" \n",
    "\"\"\"\n",
    "\n",
    "bottom = \"\"\" \n",
    "          FROM \n",
    "            createID \n",
    "          ORDER BY \n",
    "            ID, row_ID ASC\n",
    "        ) \n",
    "      ORDER BY \n",
    "        ID, \n",
    "        row_ID\n",
    "    ) \n",
    "    SELECT \"siren\",\n",
    "\"code_greffe\",\n",
    "\"Nom_Greffe\",\n",
    "\"numero_gestion\",\n",
    "\"id_etablissement\", \n",
    "\"file_timestamp\",\n",
    "\"Libelle_Evt\",\n",
    "\"Date_Greffe\",    \n",
    "\"Type\",\n",
    "\"Siège_PM\",\n",
    "\"RCS_Registre\",\n",
    "\"Adresse_Ligne1\",\n",
    "\"Adresse_Ligne2\",\n",
    "\"Adresse_Ligne3\",\n",
    "\"Code_Postal\",\n",
    "\"Ville\",\n",
    "\"Code_Commune\",\n",
    "\"Pays\",\n",
    "\"Domiciliataire_Nom\",\n",
    "\"Domiciliataire_Siren\",\n",
    "\"Domiciliataire_Greffe\",\n",
    "\"Domiciliataire_Complément\",\n",
    "\"Siege_Domicile_Représentant\",\n",
    "\"Nom_Commercial\",\n",
    "\"Enseigne\",\n",
    "\"Activité_Ambulante\",\n",
    "\"Activité_Saisonnière\",\n",
    "\"Activité_Non_Sédentaire\",\n",
    "\"Date_Début_Activité\",\n",
    "\"Activité\",\n",
    "\"Origine_Fonds\",\n",
    "\"Origine_Fonds_Info\",\n",
    "\"Type_Exploitation\",\n",
    "\"csv_source\",\n",
    "CASE WHEN siren IS NOT NULL THEN 'EVT' \n",
    "ELSE NULL END as origin\n",
    "    FROM \n",
    "      (\n",
    "        SELECT \n",
    "          *, \n",
    "          ROW_NUMBER() OVER(\n",
    "            PARTITION BY ID \n",
    "            ORDER BY \n",
    "              ID, row_ID DESC\n",
    "          ) AS max_value \n",
    "        FROM \n",
    "          filled\n",
    "      ) AS T \n",
    "    WHERE \n",
    "      max_value = 1\n",
    "  )ORDER BY siren,\"Nom_Greffe\", \"code_greffe\",\n",
    "      numero_gestion, id_etablissement, \n",
    "      file_timestamp\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_fillin = top.format('test')\n",
    "for x, val in enumerate(list_change):\n",
    "\n",
    "    if x != len(list_change) -1:\n",
    "        query_fillin+=top_1.format(val)+ \",\"\n",
    "    else:\n",
    "        query_fillin+=top_1.format(val)\n",
    "        query_fillin+= middle\n",
    "\n",
    "for x, val in enumerate(list_change):\n",
    "    if x != len(list_change) -1:\n",
    "        query_fillin+=middle_2.format(val)+ \",\"\n",
    "    else:\n",
    "        query_fillin+=middle_2.format(val)\n",
    "        query_fillin+=bottom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query préparation partiel\n",
    "\n",
    "Dans cette étape, il faut vérifier si un quadruplet `siren`,`code greffe`, `numero_gestion`, `id_etablissement` possède une ligne `Partiel`. Auquel cas, une nouvelle variable est recréée indiquant pour toutes les lignes précédant un `Partiel` les valeurs à ignorer. On prend la date maximum `date_max` des stocks partiels par quadruplet, si la date de transfert est inférieure a la `date_max`, alors on ignore.\n",
    "\n",
    "#### Détails\n",
    "\n",
    "* Définition Partiel:\n",
    "  * si csv dans le dossier Partiel, année > 2017, alors partiel\n",
    "    * la date d'ingestion est indiquée dans le path, ie comme les flux\n",
    "    \n",
    "1/ Est-ce que les csv dans le dossier Stock pour une date supérieure à 2017 peuvent être labélisés comme « partiel » rendant ainsi caduque toutes les valeurs précédentes d’un établissement ?\n",
    "* OUI (Reponse Flament Lionel <lflament@inpi.fr>)\n",
    "\n",
    "2/ Pour identifier un établissement, il faut bien choisir la triplette siren + numero dossier (greffe) + ID établissement ?\n",
    "* → il faut choisir le quadruplet siren + code greffe + numero gestion + ID établissement  (Reponse Flament Lionel <lflament@inpi.fr>)\n",
    "\n",
    "\n",
    "#### Test de vérification\n",
    "\n",
    "On utilise dans un Excel un exemple avec les valeurs du siren 428689392 ayant des ID établissements identiques pour des adresses différentes. J’ai souligné en bleu les valeurs qui potentiellement amendent la ligne n-1 (ex ligne 10 amende la ligne 9) -> fait référence au point 1/\n",
    "Pour le point 2, il y a par exemple, l’ID établissement 10 qui appartient à la fois a Rennes, mais aussi Nanterre. De fait, il faut bien distinguer le greffe, car ce sont 2 établissements différents.\n",
    "\n",
    "* Colonne id_etablissement \n",
    "  * Pour les événements, il faut trier avec la pair nom_greffe et ou numero_gestion  \n",
    "  * Exemple: [428689392](s3://calfdata/INPI/TC_1/02_preparation_donnee/check_csv/428689392.xlsx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_partiel = \"\"\"WITH to_date AS (\n",
    "  SELECT \n",
    "\"siren\",\n",
    "\"code_greffe\",\n",
    "\"Nom_Greffe\",\n",
    "\"numero_gestion\",\n",
    "\"id_etablissement\", \n",
    "\"Libelle_Evt\",\n",
    "\"Date_Greffe\",    \n",
    "\"Type\",\n",
    "\"Siège_PM\",\n",
    "\"RCS_Registre\",\n",
    "\"Adresse_Ligne1\",\n",
    "\"Adresse_Ligne2\",\n",
    "\"Adresse_Ligne3\",\n",
    "\"Code_Postal\",\n",
    "\"Ville\",\n",
    "\"Code_Commune\",\n",
    "\"Pays\",\n",
    "\"Domiciliataire_Nom\",\n",
    "\"Domiciliataire_Siren\",\n",
    "\"Domiciliataire_Greffe\",\n",
    "\"Domiciliataire_Complément\",\n",
    "\"Siege_Domicile_Représentant\",\n",
    "\"Nom_Commercial\",\n",
    "\"Enseigne\",\n",
    "\"Activité_Ambulante\",\n",
    "\"Activité_Saisonnière\",\n",
    "\"Activité_Non_Sédentaire\",\n",
    "\"Date_Début_Activité\",\n",
    "\"Activité\",\n",
    "\"Origine_Fonds\",\n",
    "\"Origine_Fonds_Info\",\n",
    "\"Type_Exploitation\",\n",
    "\"csv_source\",\n",
    "\"origin\", Coalesce(try(cast(file_timestamp as timestamp)))  as file_timestamp\n",
    "FROM \"inpi\".\"initial_partiel_evt_new_etb\"\n",
    "WHERE siren !='' AND file_timestamp !=''\n",
    "                 )\n",
    "SELECT *\n",
    "FROM (\n",
    "  WITH max_date_partiel AS(\n",
    "SELECT siren, \"code_greffe\", nom_greffe, numero_gestion, id_etablissement,\n",
    "MAX(file_timestamp) as max_partiel\n",
    "FROM to_date\n",
    "WHERE origin = 'Partiel'\n",
    "GROUP BY  siren, \"code_greffe\", nom_greffe, numero_gestion, id_etablissement\n",
    "    )\n",
    "  SELECT \n",
    "  to_date.\"siren\",\n",
    "to_date.\"code_greffe\",\n",
    "to_date.\"Nom_Greffe\",\n",
    "to_date.\"numero_gestion\",\n",
    "to_date.\"id_etablissement\", \n",
    "to_date.\"file_timestamp\",\n",
    "max_date_partiel.max_partiel,\n",
    "CASE WHEN to_date.\"file_timestamp\" <  max_date_partiel.max_partiel \n",
    "  THEN 'IGNORE' ELSE NULL END AS status, \n",
    "to_date.\"origin\" ,\n",
    "to_date.\"Libelle_Evt\",\n",
    "to_date.\"Date_Greffe\",    \n",
    "to_date.\"Type\",\n",
    "to_date.\"Siège_PM\",\n",
    "to_date.\"RCS_Registre\",\n",
    "to_date.\"Adresse_Ligne1\",\n",
    "to_date.\"Adresse_Ligne2\",\n",
    "to_date.\"Adresse_Ligne3\",\n",
    "to_date.\"Code_Postal\",\n",
    "to_date.\"Ville\",\n",
    "to_date.\"Code_Commune\",\n",
    "to_date.\"Pays\",\n",
    "to_date.\"Domiciliataire_Nom\",\n",
    "to_date.\"Domiciliataire_Siren\",\n",
    "to_date.\"Domiciliataire_Greffe\",\n",
    "to_date.\"Domiciliataire_Complément\",\n",
    "to_date.\"Siege_Domicile_Représentant\",\n",
    "to_date.\"Nom_Commercial\",\n",
    "to_date.\"Enseigne\",\n",
    "to_date.\"Activité_Ambulante\",\n",
    "to_date.\"Activité_Saisonnière\",\n",
    "to_date.\"Activité_Non_Sédentaire\",\n",
    "to_date.\"Date_Début_Activité\",\n",
    "to_date.\"Activité\",\n",
    "to_date.\"Origine_Fonds\",\n",
    "to_date.\"Origine_Fonds_Info\",\n",
    "to_date.\"Type_Exploitation\",\n",
    "to_date.\"csv_source\"\n",
    "  FROM to_date\n",
    "  LEFT JOIN max_date_partiel on\n",
    "  to_date.siren =max_date_partiel.siren AND\n",
    "  to_date.\"code_greffe\" =max_date_partiel.\"code_greffe\" AND\n",
    "  to_date.nom_greffe =max_date_partiel.nom_greffe AND\n",
    "  to_date.numero_gestion =max_date_partiel.numero_gestion AND\n",
    "  to_date.id_etablissement =max_date_partiel.id_etablissement\n",
    "  ORDER BY siren, \"code_greffe\", nom_greffe, numero_gestion,\n",
    "  id_etablissement, file_timestamp\n",
    "  )\"\"\"\n",
    "\n",
    "query_table_all = \"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS {0}.{1} (\n",
    "`siren` string,\n",
    "`code_greffe` int,\n",
    "`Nom_Greffe` string,\n",
    "`numero_gestion` string,\n",
    "`id_etablissement` string ,\n",
    "`file_timestamp` string,\n",
    "`max_partiel` string,\n",
    "`status` string,\n",
    "`origin` string,\n",
    "`Libelle_Evt` string,\n",
    "`Date_Greffe` string    ,\n",
    "`Type` string,\n",
    "`Siège_PM` string,\n",
    "`RCS_Registre` string,\n",
    "`Adresse_Ligne1` string,\n",
    "`Adresse_Ligne2` string,\n",
    "`Adresse_Ligne3` string,\n",
    "`Code_Postal` string,\n",
    "`Ville` string,\n",
    "`Code_Commune` string,\n",
    "`Pays` string,\n",
    "`Domiciliataire_Nom` string,\n",
    "`Domiciliataire_Siren` string,\n",
    "`Domiciliataire_Greffe` string,\n",
    "`Domiciliataire_Complément` string,\n",
    "`Siege_Domicile_Représentant` string,\n",
    "`Nom_Commercial` string,\n",
    "`Enseigne` string,\n",
    "`Activité_Ambulante` string,\n",
    "`Activité_Saisonnière` string,\n",
    "`Activité_Non_Sédentaire` string,\n",
    "`Date_Début_Activité` string,\n",
    "`Activité` string,\n",
    "`Origine_Fonds` string,\n",
    "`Origine_Fonds_Info` string,\n",
    "`Type_Exploitation` string,\n",
    "`csv_source` string\n",
    ")\n",
    "     ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
    "    WITH SERDEPROPERTIES (\n",
    "   'separatorChar' = ',',\n",
    "   'quoteChar' = '\"'\n",
    "   )\n",
    "     LOCATION '{2}'\n",
    "     TBLPROPERTIES ('has_encrypted_data'='false',\n",
    "              'skip.header.line.count'='1');\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query remplissage EVT via initial, partiel, creation\n",
    "\n",
    "Il y a deux étapes à suivre. \n",
    "\n",
    "Pour remplir les événements, il faut prendre la ligne t-1, et compléter les champs manquants. En effet, l'INPI ne transmet que les champs modifiés, les champs non modifiés sont transmis vides.\n",
    "Dans l'[étape 2](https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/01_Athena_concatenate_ETS.md#step-2-concatenation-data), nous avons pu remplir puis garder le dernier événement par date de transmission. Toutefois, dans la majeure partie des cas, les champs sont vides, car ils n'ont pas d'antécédents. L'antécédent provient soit d'un événement initial, soit d'un partiel ou création. Dans le cas de figure ou l'événement est une création.\n",
    "\n",
    "Finalement, il faut reconstituer les valeurs manquantes des evenements en utilisant les informations qui ne sont pas communiquées dans les csv événements. En effet, le csv événement ne renseigne que les valeurs obligatoires et les modifications, laissant vides les autres champs. Pour récupérer les champs manquants, il faut utliser la valeur précédente pour le quadruplet `siren`,`code greffe`, `numero_gestion`, `id_etablissement`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle_3 = \"\"\"sum(case when origin = 'EVT' AND \"{0}\" = '' then 0 else 1 end) \n",
    "over (partition by ID \n",
    "order by  row_ID) as \"{0}_partition\" \n",
    "\"\"\"\n",
    "\n",
    "bottom_1 = \"\"\" \n",
    "         FROM \n",
    "            createID \n",
    "          ORDER BY \n",
    "            ID, row_ID ASC\n",
    "        ) \n",
    "      ORDER BY \n",
    "        ID, \n",
    "        row_ID\n",
    "    ) \n",
    "    SELECT \"siren\",\n",
    "\"code_greffe\",\n",
    "\"Nom_Greffe\",\n",
    "\"numero_gestion\",\n",
    "\"id_etablissement\", \n",
    "\"status\",\n",
    "CASE WHEN siren IS NOT NULL THEN 'EVT' \n",
    "ELSE NULL END as origin,    \n",
    "\"Date_Greffe\",  \n",
    "\"file_timestamp\",\n",
    "\"Libelle_Evt\",  \n",
    "\"Type\",\n",
    "\"Siège_PM\",\n",
    "\"RCS_Registre\",\n",
    "\"Adresse_Ligne1\",\n",
    "\"Adresse_Ligne2\",\n",
    "\"Adresse_Ligne3\",\n",
    "\"Code_Postal\",\n",
    "\"Ville\",\n",
    "\"Code_Commune\",\n",
    "\"Pays\",\n",
    "\"Domiciliataire_Nom\",\n",
    "\"Domiciliataire_Siren\",\n",
    "\"Domiciliataire_Greffe\",\n",
    "\"Domiciliataire_Complément\",\n",
    "\"Siege_Domicile_Représentant\",\n",
    "\"Nom_Commercial\",\n",
    "\"Enseigne\",\n",
    "\"Activité_Ambulante\",\n",
    "\"Activité_Saisonnière\",\n",
    "\"Activité_Non_Sédentaire\",\n",
    "\"Date_Début_Activité\",\n",
    "\"Activité\",\n",
    "\"Origine_Fonds\",\n",
    "\"Origine_Fonds_Info\",\n",
    "\"Type_Exploitation\",\n",
    "\"csv_source\"\n",
    "    FROM filled\n",
    "  )ORDER BY siren,\"Nom_Greffe\", \"code_greffe\",\n",
    "      numero_gestion, id_etablissement, \n",
    "      file_timestamp\n",
    "  )\n",
    "    )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_evt_2 = \"\"\"WITH convert AS (\n",
    "  SELECT siren,\n",
    "\"code_greffe\",\"Nom_Greffe\", numero_gestion, id_etablissement, origin, \"status\",\n",
    "Coalesce(\n",
    "         try(date_parse(file_timestamp, '%Y-%m-%d')),\n",
    "         try(date_parse(file_timestamp, '%Y-%m-%d %hh:%mm:%ss.SSS')),\n",
    "         try(date_parse(file_timestamp, '%Y-%m-%d %hh:%mm:%ss')),\n",
    "         try(cast(file_timestamp as timestamp))\n",
    "       )  as file_timestamp,\n",
    "\n",
    "Coalesce(\n",
    "         try(date_parse(date_greffe, '%Y-%m-%d')),\n",
    "         try(date_parse(date_greffe, '%Y/%m/%d')),\n",
    "         try(date_parse(date_greffe, '%d %M %Y')),\n",
    "         try(date_parse(date_greffe, '%d/%m/%Y')),\n",
    "         try(date_parse(date_greffe, '%d-%m-%Y'))\n",
    "  )\n",
    "  as date_greffe,libelle_evt, \"Type\",\"Siège_PM\",\"RCS_Registre\",\"Adresse_Ligne1\",\n",
    "\"Adresse_Ligne2\",\"Adresse_Ligne3\",\"Code_Postal\",\"Ville\",\"Code_Commune\",\n",
    "\"Pays\",\"Domiciliataire_Nom\",\"Domiciliataire_Siren\",\"Domiciliataire_Greffe\",\n",
    "\"Domiciliataire_Complément\",\"Siege_Domicile_Représentant\",\"Nom_Commercial\",\n",
    "\"Enseigne\",\"Activité_Ambulante\",\"Activité_Saisonnière\",\n",
    "\"Activité_Non_Sédentaire\",\"Date_Début_Activité\",\"Activité\",\"Origine_Fonds\",\n",
    "\"Origine_Fonds_Info\",\"Type_Exploitation\", \"max_partiel\",\"csv_source\"\n",
    "  FROM \"inpi\".\"{}\"\n",
    "  /*WHERE siren ='449361179' AND id_etablissement = '7' AND \n",
    "  numero_gestion = '2011B00329' AND file_timestamp != ''*/\n",
    "  )SELECT * \n",
    "  FROM (\n",
    "    WITH temp AS (\n",
    "                 SELECT siren,\n",
    "                 \"code_greffe\",\n",
    "                 \"Nom_Greffe\",\n",
    "                 numero_gestion,\n",
    "                 id_etablissement,\n",
    "                 origin, \n",
    "                 \"status\",\n",
    "                 file_timestamp,\n",
    "                 date_greffe, libelle_evt,\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 Concatenation data\n",
    "                \n",
    "### Details Steps:\n",
    "\n",
    "L'ID de la query `creation_csv` est stocké dans le `dic_` car il faut plusieurs minutes pour lire les tables et sauvegarder en csv.\n",
    "\n",
    "A noter que la query `query_csv` ne prend pas toutes les variables (celles crééent lors de l'extraction du FTP) car manque de mémoire lors de la préparation des événements.\n",
    "\n",
    "- Stock\n",
    "    - Initial:\n",
    "        - Création table en concatenant tous les fichiers de ce dossier [Stock_initial/ETS](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/TC_1/01_donnee_source/Stock/Stock_initial/ETS/)\n",
    "            - Output: [INPI/sql_output/](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/sql_output/)\n",
    "        - Création csv\n",
    "            - Output stocké dans le dictionaire des paramètres key `output_id`\n",
    "    - Partiel:\n",
    "        - Création table en concatenant tous les fichiers de ce dossier [Stock/Stock_partiel](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/TC_1/01_donnee_source/Stock/Stock_partiel/)\n",
    "            - Output: [INPI/sql_output/](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/sql_output/)\n",
    "        - Création csv\n",
    "            - Output stocké dans le dictionaire des paramètres key `output_id`\n",
    "- Flux\n",
    "    - NEW:\n",
    "        - Création table en concatenant tous les fichiers de ce dossier [ETS/NEW](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/TC_1/01_donnee_source/Flux/2017/ETS/NEW/)\n",
    "            - Output: [INPI/sql_output/](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/sql_output/)\n",
    "        - Création csv\n",
    "            - Output stocké dans le dictionaire des paramètres key `output_id`\n",
    "    - EVT\n",
    "        - Création table en concatenant tous les fichiers de ce dossier [ETS/EVT](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/TC_1/01_donnee_source/Flux/2017/ETS/EVT/)\n",
    "            - Output: [INPI/sql_output/](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/sql_output/)\n",
    "        - Création csv: Run [query](https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/01_Athena_concatenate_ETS.md#query-pr%C3%A9paration-%C3%A9v%C3%A9nement) pour remplir les valeurs manquantes et extraire l'entrée max par jour/heure de transmission.\n",
    "            - Output stocké dans le dictionaire des paramètres key `output_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop previous tables\n",
    "for i in ['ets_initial', 'ets_partiel_2018', 'ets_partiel_2019',\n",
    "          'ets_new_2017', 'ets_new_2018', 'ets_new_2019', 'ets_evt_2017',\n",
    "         'ets_evt_2018', 'ets_evt_2019', \"initial_partiel_evt_new_etb\",\n",
    "          'initial_partiel_evt_new_ets_status', 'initial_partiel_evt_new_ets_status_final'\n",
    "         ]:\n",
    "    query = \"DROP TABLE `{}`\".format(i)\n",
    "    output = athena.run_query(\n",
    "                        query=query,\n",
    "                        database=dic_['global']['database'],\n",
    "                        s3_output=dic_['global']['output']\n",
    "                    )\n",
    "    print(output['QueryExecutionId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tables\n",
    "for nature, values in dic_.items():\n",
    "    if nature != 'global':\n",
    "        for origin, val in dic_[nature].items():\n",
    "            for type_, v in dic_[nature][origin].items():\n",
    "                if origin == 'INITIAL':\n",
    "                    #### Creation table\n",
    "                    create_table = query_tb.format(\n",
    "                        dic_['global']['database'],\n",
    "                        v['tables'],\n",
    "                        v['path'],\n",
    "                        \";\"\n",
    "                    )\n",
    "                    time.sleep(2)\n",
    "                    athena.run_query(\n",
    "                        query=create_table,\n",
    "                        database=dic_['global']['database'],\n",
    "                        s3_output=dic_['global']['output'])\n",
    "                    \n",
    "                    #### Creation CSV\n",
    "                    time.sleep(1)\n",
    "                    query = query_select.format(\n",
    "                        v['tables'])\n",
    "                    \n",
    "                    output = athena.run_query(\n",
    "                        query=query,\n",
    "                        database=dic_['global']['database'],\n",
    "                        s3_output=dic_['global']['output']\n",
    "                    )\n",
    "                    \n",
    "                    v['output_id'].append(output['QueryExecutionId'])\n",
    "\n",
    "                else:\n",
    "                    for i in range(0,len(v['tables'])):\n",
    "                        create_table = query_tb.format(\n",
    "                                dic_['global']['database'],\n",
    "                                v['tables'][i],\n",
    "                                v['path'][i], \n",
    "                                \";\"\n",
    "                            )\n",
    "                        \n",
    "                        time.sleep(2)\n",
    "                        athena.run_query(\n",
    "                        query=create_table,\n",
    "                        database=dic_['global']['database'],\n",
    "                        s3_output=dic_['global']['output'])\n",
    "                        \n",
    "                        time.sleep(1)\n",
    "                        \n",
    "                        if origin != 'EVT':\n",
    "                            query = query_select.format(\n",
    "                            v['tables'][i])\n",
    "                        \n",
    "                            output = athena.run_query(\n",
    "                            query=query,\n",
    "                            database=dic_['global']['database'],\n",
    "                            s3_output=dic_['global']['output']\n",
    "                        )\n",
    "                            v['output_id'].append(output['QueryExecutionId'])\n",
    "                        ### Dealing avec les evenements    \n",
    "                        else:\n",
    "                            query_fillin = top.format(v['tables'][i])\n",
    "                            for x, val in enumerate(list_change):\n",
    "\n",
    "                                if x != len(list_change) -1:\n",
    "                                    query_fillin+=top_1.format(val)+ \",\"\n",
    "                                else:\n",
    "                                    query_fillin+=top_1.format(val)\n",
    "                                    query_fillin+= middle\n",
    "\n",
    "                            for x, val in enumerate(list_change):\n",
    "                                if x != len(list_change) -1:\n",
    "                                    query_fillin+=middle_2.format(val)+ \",\"\n",
    "                                else:\n",
    "                                    query_fillin+=middle_2.format(val)\n",
    "                                    query_fillin+=bottom \n",
    "                                    \n",
    "                            output = athena.run_query(\n",
    "                                query=query_fillin,\n",
    "                                database=dic_['global']['database'],\n",
    "                                s3_output=dic_['global']['output']\n",
    "                            )\n",
    "                            v['output_id'].append(output['QueryExecutionId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 Bis: Copier csv\n",
    "\n",
    "Dans l'étape 1, nous avons stocké les ID dans le dictionaire de paramètre. Il faut environ 10/15 minutes pour préparer tous les csv. \n",
    "\n",
    "Dans cette étape, on va simplement récupérer les nouveaux csv du dossier [INPI/sql_output/](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/sql_output/) nommés par id d'execution, pour les déplacer dans le nouveau dossier [INPI/sql_output_preparation/](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/sql_output_preparation/) avec un nom générique.\n",
    "\n",
    "Le [dossier](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/sql_output_preparation/) va contenir les csv suivants:\n",
    "\n",
    "- INPI/sql_output_preparation/ets_initial.csv\n",
    "- INPI/sql_output_preparation/ets_partiel_2018.csv\n",
    "- INPI/sql_output_preparation/ets_partiel_2019.csv\n",
    "- INPI/sql_output_preparation/ets_new_2017.csv\n",
    "- INPI/sql_output_preparation/ets_new_2018.csv\n",
    "- INPI/sql_output_preparation/ets_new_2019.csv\n",
    "- INPI/sql_output_preparation/ets_evt_2017.csv\n",
    "- INPI/sql_output_preparation/ets_evt_2018.csv\n",
    "- INPI/sql_output_preparation/ets_evt_2019.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move and rename files\n",
    "\n",
    "for nature, values in dic_.items():\n",
    "    if nature != 'global':\n",
    "        for origin, val in dic_[nature].items():\n",
    "            for type_, v in dic_[nature][origin].items():\n",
    "                for i, id_ in enumerate(v['output_id']):\n",
    "                    source_key = \"{}/{}.csv\".format(\n",
    "                        dic_['global']['output'],\n",
    "                        id_\n",
    "                               )\n",
    "                    if origin == 'INITIAL':\n",
    "                        destination_key = \"{}/{}.csv\".format(\n",
    "                        dic_['global']['output_preparation'],\n",
    "                        v['tables']\n",
    "                    )\n",
    "                    else:\n",
    "                        destination_key = \"{}/{}.csv\".format(\n",
    "                        dic_['global']['output_preparation'],\n",
    "                        v['tables'][i]\n",
    "                    )\n",
    "                    results = s3.copy_object_s3(\n",
    "                        source_key = source_key,\n",
    "                        destination_key = destination_key,\n",
    "                        remove = True\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Creation table Initial/Partiel/EVT/NEW\n",
    "\n",
    "Pour cette étape, on récupère les csv de ce [dossier](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/sql_output_preparation/), qu'on aggrège avant de préparer les valeurs manquantes.\n",
    "\n",
    "La table agrégée s'appelle `initial_partiel_evt_new_etb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = 'initial_partiel_evt_new_etb'\n",
    "create_table = query_table_concat.format(\n",
    "    dic_['global']['database'],\n",
    "    table,\n",
    "    \"s3://calfdata/{}\".format(\n",
    "        dic_['global']['output_preparation'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athena.run_query(\n",
    "    query=create_table,\n",
    "    database=dic_['global']['database'],\n",
    "    s3_output=dic_['global']['output']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Creation statut partiel\n",
    "\n",
    "Dans cette étape, on crée une colonne `status`, qui indique si les lignes sont a ignorer (IGNORE) ou non (Vide). La logique c'est de prendre la date maximum des stocks partiels par quadruplet, si la date de transfert est inférieure a la date max, alors on ignore. La query prend quelques minutes.\n",
    "\n",
    "Output de la query va dans ce dossier [INPI/sql_output_status](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/sql_output_status/?region=eu-west-3&tab=overview)\n",
    "La table avec `status` s'appelle `initial_partiel_evt_new_ets_status`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = athena.run_query(\n",
    "    query=query_partiel,\n",
    "    database=dic_['global']['database'],\n",
    "    s3_output=dic_['global']['output']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_['global']['ETS_step4_id'] = output['QueryExecutionId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = 'initial_partiel_evt_new_ets_status'\n",
    "source_key = \"{}/{}.csv\".format(\n",
    "                        dic_['global']['output'],\n",
    "                        dic_['global']['ETS_step4_id']\n",
    "                               )\n",
    "\n",
    "destination_key = \"{}/{}.csv\".format(\n",
    "                        'INPI/sql_output_status',\n",
    "                        table\n",
    "                    )\n",
    "results = s3.copy_object_s3(\n",
    "                        source_key = source_key,\n",
    "                        destination_key = destination_key,\n",
    "                        remove = True\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_status = query_table_all.format(\n",
    "    dic_['global']['database'], \n",
    "    table,\n",
    "     \"s3://calfdata/{}\".format('INPI/sql_output_status')\n",
    ")\n",
    "\n",
    "athena.run_query(\n",
    "    query=query_status,\n",
    "    database=dic_['global']['database'],\n",
    "    s3_output=dic_['global']['output']\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Remplissage observations manquantes\n",
    "\n",
    "On fait les opérations suivantes :\n",
    "\n",
    "- Remplissage des valeurs manquantes pour les observations.\n",
    "    - Si `origin` est égale a `EVT`, alors on trie sur `siren,'code greffe', numero_gestion, id_etablissement,date_greffe_temp_` et on récupère les valeurs n - 1.\n",
    "    - Remplissage des champs manquants pour les événements séquentiels, uniquement dans le cas des événements\n",
    "        - Output: [INPI/sql_output/](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/sql_output_final_ets/)\n",
    "        - Output stocké dans le dictionaire des paramaitres key `['global']['table_final_id']['ETS']['EVT']`\n",
    "- Filtre table pour le champ `origin` autre que `EVT`\n",
    "   - Output: [INPI/sql_output/](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/sql_output_final_ets/)\n",
    "        - Output stocké dans le dictionaire des paramètres key `['global']['table_final_id']['ETS']['Not_EVT']`\n",
    "- Concaténation des deux précédentes steps `EVT` et `Not_EVT`.\n",
    "    - problème de mémoire pour faire une jointure des deux steps précédentes\n",
    "    - Output: [TC_1/02_preparation_donnee/ETS](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/TC_1/02_preparation_donnee/ETS/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = 'initial_partiel_evt_new_ets_status'\n",
    "list_change = [\n",
    "\"Type\",\n",
    "\"Siège_PM\",\n",
    "\"RCS_Registre\",\n",
    "\"Adresse_Ligne1\",\n",
    "\"Adresse_Ligne2\",\n",
    "\"Adresse_Ligne3\",\n",
    "\"Code_Postal\",\n",
    "\"Ville\",\n",
    "\"Code_Commune\",\n",
    "\"Pays\",\n",
    "\"Domiciliataire_Nom\",\n",
    "\"Domiciliataire_Siren\",\n",
    "\"Domiciliataire_Greffe\",\n",
    "\"Domiciliataire_Complément\",\n",
    "\"Siege_Domicile_Représentant\",\n",
    "\"Nom_Commercial\",\n",
    "\"Enseigne\",\n",
    "\"Activité_Ambulante\",\n",
    "\"Activité_Saisonnière\",\n",
    "\"Activité_Non_Sédentaire\",\n",
    "\"Date_Début_Activité\",\n",
    "\"Activité\",\n",
    "\"Origine_Fonds\",\n",
    "\"Origine_Fonds_Info\",\n",
    "\"Type_Exploitation\",\n",
    "\"max_partiel\",\n",
    "\"csv_source\"\n",
    "]\n",
    "\n",
    "for x, value in enumerate(list_change):\n",
    "    query = \"\"\"CASE WHEN origin = 'EVT' AND status != 'IGNORE' AND \"{0}\" = '' THEN \n",
    "LAG (\"{0}\", 1) OVER (  PARTITION BY siren,\"code_greffe\", numero_gestion, id_etablissement \n",
    " ORDER BY siren,'code_greffe', numero_gestion, id_etablissement,file_timestamp ) ELSE \"{0}\" END AS \"{0}\" \n",
    "\"\"\".format(value)\n",
    "    if  x != len(list_change)-1:\n",
    "        query_evt_2 +=query +\",\"\n",
    "    else:\n",
    "        query_evt_2 +=query\n",
    "        end = \"\"\"FROM convert\n",
    "ORDER BY siren,'code_greffe', numero_gestion, id_etablissement,file_timestamp)\n",
    "SELECT *\n",
    "FROM (\n",
    "  WITH createID AS (\n",
    "    SELECT  \n",
    "    ROW_NUMBER() OVER (\n",
    "      PARTITION BY \n",
    "      siren,\n",
    "      \"code_greffe\",\n",
    "      \"Nom_Greffe\",\n",
    "      numero_gestion,\n",
    "      id_etablissement, \n",
    "      date_greffe\n",
    "    ) As row_ID, \n",
    "    DENSE_RANK () OVER (\n",
    "      ORDER BY \n",
    "        siren, \n",
    "        \"code_greffe\",\n",
    "        \"Nom_Greffe\",\n",
    "        numero_gestion, \n",
    "        id_etablissement, \n",
    "        date_greffe\n",
    "    ) As ID, *\n",
    "    FROM temp\n",
    "    WHERE origin = 'EVT'\n",
    "    )\n",
    "SELECT \n",
    "  * \n",
    "FROM \n",
    "  (\n",
    "    WITH filled AS (\n",
    "      SELECT \n",
    "        ID, \n",
    "        row_ID, \n",
    "        siren, \n",
    "        \"Nom_Greffe\",\n",
    "        \"code_greffe\", \n",
    "        numero_gestion, \n",
    "        id_etablissement, \n",
    "        \"status\",\n",
    "        date_greffe,\n",
    "        file_timestamp,\n",
    "        libelle_evt,\n",
    "        Enseigne_partition,\n",
    "\"\"\"\n",
    "        query_evt_2 += end\n",
    "for x, val in enumerate(list_change):\n",
    "\n",
    "    if x != len(list_change) -1:\n",
    "        query_evt_2+=top_1.format(val)+ \",\"\n",
    "    else:\n",
    "        query_evt_2+=top_1.format(val)\n",
    "        query_evt_2+= middle\n",
    "\n",
    "for x, val in enumerate(list_change):\n",
    "    if x != len(list_change) -1:\n",
    "        query_evt_2+=middle_2.format(val)+ \",\"\n",
    "    else:\n",
    "        query_evt_2+=middle_2.format(val)\n",
    "        query_evt_2+=bottom_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(query_evt_2.format(table))\n",
    "#dic_['global']['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = athena.run_query(\n",
    "    query=query_evt_2.format(table),\n",
    "    database=dic_['global']['database'],\n",
    "    s3_output=dic_['global']['output']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_['global']['table_final_id']['ETS']['EVT'] =  output['QueryExecutionId']\n",
    "dic_['global']['table_final_id']['ETS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_key = \"{}/{}.csv\".format(dic_['global']['output'],\n",
    "                                dic_['global']['table_final_id']['ETS']['EVT']\n",
    "                               )\n",
    "destination_key = \"{}/{}.csv\".format(\"INPI/sql_output_final_ets\",\n",
    "                                          'initial_partiel_evt_new_ets_status_EVT'\n",
    "                                         )\n",
    "results = s3.copy_object_s3(source_key = source_key,\n",
    "             destination_key = destination_key,\n",
    "             remove = False\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not Evt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = 'initial_partiel_evt_new_ets_status'\n",
    "query = \"\"\"SELECT \n",
    "\"siren\",\n",
    "\"code_greffe\",\n",
    "\"Nom_Greffe\",\n",
    "\"numero_gestion\",\n",
    "\"id_etablissement\", \n",
    "\"status\",\n",
    "\"origin\",\n",
    "Coalesce(\n",
    "         try(date_parse(date_greffe, '%Y-%m-%d')),\n",
    "         try(date_parse(date_greffe, '%Y/%m/%d')),\n",
    "         try(date_parse(date_greffe, '%d %M %Y')),\n",
    "         try(date_parse(date_greffe, '%d/%m/%Y')),\n",
    "         try(date_parse(date_greffe, '%d-%m-%Y'))\n",
    "  ) as date_greffe,\n",
    "Coalesce(\n",
    "         try(date_parse(file_timestamp, '%Y-%m-%d')),\n",
    "         try(date_parse(file_timestamp, '%Y-%m-%d %hh:%mm:%ss.SSS')),\n",
    "         try(date_parse(file_timestamp, '%Y-%m-%d %hh:%mm:%ss')),\n",
    "         try(cast(file_timestamp as timestamp))\n",
    "       )  as file_timestamp,\n",
    "\"Libelle_Evt\",  \n",
    "\"Type\",\n",
    "\"Siège_PM\",\n",
    "\"RCS_Registre\",\n",
    "\"Adresse_Ligne1\",\n",
    "\"Adresse_Ligne2\",\n",
    "\"Adresse_Ligne3\",\n",
    "\"Code_Postal\",\n",
    "\"Ville\",\n",
    "\"Code_Commune\",\n",
    "\"Pays\",\n",
    "\"Domiciliataire_Nom\",\n",
    "\"Domiciliataire_Siren\",\n",
    "\"Domiciliataire_Greffe\",\n",
    "\"Domiciliataire_Complément\",\n",
    "\"Siege_Domicile_Représentant\",\n",
    "\"Nom_Commercial\",\n",
    "\"Enseigne\",\n",
    "\"Activité_Ambulante\",\n",
    "\"Activité_Saisonnière\",\n",
    "\"Activité_Non_Sédentaire\",\n",
    "\"Date_Début_Activité\",\n",
    "\"Activité\",\n",
    "\"Origine_Fonds\",\n",
    "\"Origine_Fonds_Info\",\n",
    "\"Type_Exploitation\",\n",
    "\"csv_source\"\n",
    "FROM {}\n",
    "WHERE origin != 'EVT'\n",
    "\"\"\"\n",
    "\n",
    "output = athena.run_query(\n",
    "    query=query.format(table),\n",
    "    database=dic_['global']['database'],\n",
    "    s3_output=dic_['global']['output']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_['global']['table_final_id']['ETS']['Not_EVT'] =  output['QueryExecutionId']\n",
    "dic_['global']['table_final_id']['ETS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_key = \"{}/{}.csv\".format(dic_['global']['output'],\n",
    "                                dic_['global']['table_final_id']['ETS']['Not_EVT']\n",
    "                               )\n",
    "destination_key = \"{}/{}.csv\".format(\"INPI/sql_output_final_ets\",\n",
    "                                          'initial_partiel_evt_new_ets_status_no_EVT'\n",
    "                                         )\n",
    "results = s3.copy_object_s3(source_key = source_key,\n",
    "             destination_key = destination_key,\n",
    "             remove = False\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table finale dans Athena\n",
    "\n",
    "La dernière étape du programme consiste a récupérer tous les csv du [dossier](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/sql_output_final_ets/) afin de recréer une table appelée `initial_partiel_evt_new_ets_status_final`. A noter que les variables sont renommées (i.e lower case, tiret du bas) puis les variables sont triées dans un nouvel ordre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = 'initial_partiel_evt_new_ets_status_final'\n",
    "list_var = [\n",
    "\"siren\",\n",
    "\"code_greffe\",\n",
    "\"Nom_Greffe\",\n",
    "\"numero_gestion\",\n",
    "\"id_etablissement\", \n",
    "\"status\",\n",
    "\"origin\",\n",
    "\"date_greffe\",\n",
    "\"file_timestamp\",\n",
    "\"Libelle_Evt\",  \n",
    "\"Type\",\n",
    "\"Siège_PM\",\n",
    "\"RCS_Registre\",\n",
    "\"Adresse_Ligne1\",\n",
    "\"Adresse_Ligne2\",\n",
    "\"Adresse_Ligne3\",\n",
    "\"Code_Postal\",\n",
    "\"Ville\",\n",
    "\"Code_Commune\",\n",
    "\"Pays\",\n",
    "\"Domiciliataire_Nom\",\n",
    "\"Domiciliataire_Siren\",\n",
    "\"Domiciliataire_Greffe\",\n",
    "\"Domiciliataire_Complément\",\n",
    "\"Siege_Domicile_Représentant\",\n",
    "\"Nom_Commercial\",\n",
    "\"Enseigne\",\n",
    "\"Activité_Ambulante\",\n",
    "\"Activité_Saisonnière\",\n",
    "\"Activité_Non_Sédentaire\",\n",
    "\"Date_Début_Activité\",\n",
    "\"Activité\",\n",
    "\"Origine_Fonds\",\n",
    "\"Origine_Fonds_Info\",\n",
    "\"Type_Exploitation\",\n",
    "\"csv_source\"\n",
    "]\n",
    "\n",
    "query_ = \"\"\"CREATE EXTERNAL TABLE IF NOT EXISTS %s.%s (\"\"\"% (dic_['global']['database'],\n",
    "                                                   table)\n",
    "for x, value in enumerate(list_var):\n",
    "    if  x != len(list_var)-1:\n",
    "        q = \"`{}` string,\".format(value)\n",
    "        query_+=q\n",
    "    else:\n",
    "        q = \"`{}` string\".format(value)\n",
    "        query_+=q\n",
    "        end = \"\"\")\n",
    "     ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
    "    WITH SERDEPROPERTIES (\n",
    "   'separatorChar' = ',',\n",
    "   'quoteChar' = '\"'\n",
    "   )\n",
    "     LOCATION '%s'\n",
    "     TBLPROPERTIES ('has_encrypted_data'='false',\n",
    "              'skip.header.line.count'='1')\"\"\" % (\"s3://calfdata/{}\".format(\n",
    "                                                       \"INPI/sql_output_final_ets\")\n",
    "                                                 )\n",
    "        query_+=end\n",
    "athena.run_query(\n",
    "    query=query_,\n",
    "    database=dic_['global']['database'],\n",
    "    s3_output=dic_['global']['output']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrer les dates de greffe\n",
    "\n",
    "1. Filtrer les dates de greffes avec plusieurs transmissions, c’est a dire, ne garder que la dernière date connue et remplie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"/*add filter and code postal mat*/\n",
    "CREATE TABLE inpi.ets_test_filtered\n",
    "WITH (\n",
    "  format='PARQUET'\n",
    ") AS\n",
    "select *,CASE\n",
    "WHEN code_postal = '' THEN REGEXP_EXTRACT(ville, '\\d{5}')\n",
    "WHEN LENGTH(code_postal) = 5 THEN code_postal\n",
    "ELSE NULL END AS code_postal_matching\n",
    "from (select *,\n",
    "             row_number() over(PARTITION BY siren,\"code_greffe\", numero_gestion ,id_etablissement, date_greffe \n",
    " ORDER BY siren,'code_greffe', numero_gestion,id_etablissement, file_timestamp ) as rn\n",
    "      from initial_partiel_evt_new_ets_status_final ) as T\n",
    "where rn = 1 \n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = athena.run_query(\n",
    "    query=query,\n",
    "    database=dic_['global']['database'],\n",
    "    s3_output=dic_['global']['output']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_['global']['table_final_id']['ETS']['filtered'] =  output['QueryExecutionId']\n",
    "dic_['global']['table_final_id']['ETS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"SELECT \n",
    "\"siren\",\n",
    "\"code_greffe\",\n",
    "\"Nom_Greffe\",\n",
    "\"numero_gestion\",\n",
    "\"id_etablissement\", \n",
    "'status',\n",
    "\"origin\",\n",
    "Coalesce(\n",
    "         try(date_parse(file_timestamp, '%Y-%m-%d')),\n",
    "         try(date_parse(file_timestamp, '%Y-%m-%d %hh:%mm:%ss.SSS')),\n",
    "         try(date_parse(file_timestamp, '%Y-%m-%d %hh:%mm:%ss')),\n",
    "         try(cast(file_timestamp as timestamp))\n",
    "       )  as file_timestamp,\n",
    "\n",
    "Coalesce(\n",
    "         try(date_parse(date_greffe, '%Y-%m-%d')),\n",
    "         try(date_parse(date_greffe, '%Y-%m-%d %hh:%mm:%ss.SSS')),\n",
    "         try(date_parse(date_greffe, '%Y-%m-%d %hh:%mm:%ss')),\n",
    "         try(cast(date_greffe as timestamp))\n",
    "  ) as date_greffe,\n",
    "\"Libelle_Evt\",  \n",
    "\"Type\",\n",
    "\"Siège_PM\",\n",
    "\"RCS_Registre\",\n",
    "\"Adresse_Ligne1\",\n",
    "\"Adresse_Ligne2\",\n",
    "\"Adresse_Ligne3\",\n",
    "\"Code_Postal\",\n",
    "code_postal_matching,\n",
    "\"Ville\",\n",
    "\"Code_Commune\",\n",
    "\"Pays\",\n",
    "\"Domiciliataire_Nom\",\n",
    "\"Domiciliataire_Siren\",\n",
    "\"Domiciliataire_Greffe\",\n",
    "\"Domiciliataire_Complément\",\n",
    "\"Siege_Domicile_Représentant\",\n",
    "\"Nom_Commercial\",\n",
    "\"Enseigne\",\n",
    "\"Activité_Ambulante\",\n",
    "\"Activité_Saisonnière\",\n",
    "\"Activité_Non_Sédentaire\",\n",
    "\"Date_Début_Activité\",\n",
    "\"Activité\",\n",
    "\"Origine_Fonds\",\n",
    "\"Origine_Fonds_Info\",\n",
    "\"Type_Exploitation\",\n",
    "\"csv_source\"\n",
    "FROM {}\n",
    "ORDER BY siren,\"Nom_Greffe\", \"code_greffe\",\n",
    "      numero_gestion, id_etablissement, \n",
    "      file_timestamp\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = 'ets_test_filtered'\n",
    "#print(query.format(table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution ID: d59d1481-6455-44d7-9047-3d060679dae5\n"
     ]
    }
   ],
   "source": [
    "output = athena.run_query(\n",
    "    query=query.format(table),\n",
    "    database=dic_['global']['database'],\n",
    "    s3_output=dic_['global']['output']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'combined': 'd59d1481-6455-44d7-9047-3d060679dae5'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_['global']['table_final_id']['ETS']['combined']  =  output['QueryExecutionId']\n",
    "dic_['global']['table_final_id']['ETS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'INPI/TC_1/02_preparation_donnee/ETS/initial_partiel_evt_new_ets_status_final.csv'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_s3 = 'initial_partiel_evt_new_ets_status_final'\n",
    "source_key = \"{}/{}.csv\".format(dic_['global']['output'],\n",
    "                               dic_['global']['table_final_id']['ETS']['combined']\n",
    "                               )\n",
    "destination_key = \"{}/{}.csv\".format(\"INPI/TC_1/02_preparation_donnee/ETS\",\n",
    "                                     name_s3\n",
    "                                         )\n",
    "destination_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = s3.copy_object_s3(source_key = source_key,\n",
    "             destination_key = destination_key,\n",
    "             remove = False\n",
    "                      )"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
