{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation INPI-PM\n",
    "\n",
    "Dans ce notebook, on prepare la donnée PM afin d'être concatenée, puis envoyée dans le S3.\n",
    "- https://docs.aws.amazon.com/athena/latest/ug/csv.html\n",
    "\n",
    "Le process est détaillé dans le [notebook des Etablissements](https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/01_Athena_concatenate_ETS.ipynb), il est le même pour chacune des catégories.\n",
    "\n",
    "\n",
    "Dossiers source pour les PM:\n",
    "\n",
    "- Stock:\n",
    "    - [Stock initial](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/TC_1/01_donnee_source/Stock/Stock_initial/PM/)\n",
    "    - [Stock partiel 2018](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/TC_1/01_donnee_source/Stock/Stock_partiel/2018/PM)\n",
    "- Flux\n",
    "    - [NEW 2017](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/TC_1/01_donnee_source/Flux/2017/PM/NEW/)\n",
    "    - [EVT 2017](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/TC_1/01_donnee_source/Flux/2017/PM/EVT/)\n",
    "\n",
    "\n",
    "## Steps: Benchmark PM\n",
    "\n",
    "- Step 1: Parametre et queries\n",
    "\t- Préparation json parameters\n",
    "\t- Query préparation table\n",
    "\t- Query preparation partiel\n",
    "\t- Query remplissage EVT\n",
    "\n",
    "- Step 2: Concatenation data\n",
    "\t- Stock\n",
    "\t\t- [Initial](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/TC_1/01_donnee_source/Stock/Stock_Initial/)\n",
    "\t\t- [Partiel](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/TC_1/01_donnee_source/Stock/Stock_Partiel/)\n",
    "\t- Flux\n",
    "\t\t- [NEW](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/TC_1/01_donnee_source/Flux/2017/PM/NEW/)\n",
    "\t\t- [EVT](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/TC_1/01_donnee_source/Flux/2017/PM/EVT/)\n",
    "\t\t\t- Remplissage valeur manquante\n",
    "\n",
    "- Step 3: Creation table Initial/Partiel/EVT/NEW\n",
    "\n",
    "- Step 4: Creation statut partiel\n",
    "\t- Création colonne status qui indique si les lignes sont a ignorer ou non\n",
    "\n",
    "- Step 5: Remplissage observations manquantes\n",
    "\t- Récupération information selon `Origin` (`Stock` ou `NEW`) pour compléter les valeurs manquantes des `EVT` \n",
    "    \n",
    "## Table Athena:\n",
    "\n",
    "- pm_evt_2017: Step 2\n",
    "- pm_evt_2018: Step 2\n",
    "- pm_evt_2019: Step 2\n",
    "- pm_initial: Step 2\n",
    "- pm_new_2017: Step 2\n",
    "- pm_new_2018: Step 2\n",
    "- pm_new_2019: Step 2\n",
    "- pm_partiel_2018: Step 2\n",
    "- pm_partiel_2019: Step 2\n",
    "- initial_partiel_evt_new_pm: Step 3\n",
    "- initial_partiel_evt_new_pm_status: Step 4\n",
    "- initial_partiel_evt_new_pm_status_final Step 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsPy.aws_authorization import aws_connector\n",
    "from awsPy.aws_s3 import service_s3\n",
    "from awsPy.aws_athena import service_athena\n",
    "import os, time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'calfdata'\n",
    "path = os.getcwd()\n",
    "parent_path = str(Path(path).parent)\n",
    "path_cred = \"{}/credential_AWS.json\".format(parent_path)\n",
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                        region = 'eu-west-3')\n",
    "client= con.client_boto()\n",
    "s3 = service_s3.connect_S3(client = client,\n",
    "                      bucket = 'calfdata') \n",
    "athena = service_athena.connect_athena(client = client,\n",
    "                      bucket = 'calfdata') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Parametres et queries\n",
    "\n",
    "Pour faciliter l'ingestion de données en batch, on prépare un json ``dic_`` avec les paths où récupérer la data, le nom des tables, les origines, mais aussi un champ pour récupérer l'ID de l'execution dans Athena. En effet, chaque execution donne lieu a un ID. Certaines queries peuvent prendre plusieurs minutes. Athena crée un CSV dans un folder output prédéfini dont le nom est l'ID de la query. Notre process utilise la concaténation automatique d'Athena pour créer les tables. Il faut nécessairement déplacer les csv dans des dossiers destination en vue de la concatenation. Le stockage de l'ID est donc indispensable pour copier l'objet, surtout lorsque la query prend du temps d'execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparation json parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_ = {\n",
    "    'global':{\n",
    "        'database':'inpi',\n",
    "        'output':'INPI/sql_output',\n",
    "        'output_preparation':'INPI/sql_output_preparation_pm',\n",
    "        'PM_step4_id':[],\n",
    "        'table_final_id':{\n",
    "            'PM':{\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'Stock': {\n",
    "        'INITIAL':{\n",
    "            'PM': {\n",
    "                'path':'s3://calfdata/INPI/TC_1/01_donnee_source/Stock/Stock_Initial/2017/PM',\n",
    "                'tables':'pm_initial',\n",
    "                'origin':'INITIAL',\n",
    "                'output_id':[]\n",
    "            }\n",
    "        },\n",
    "        'PARTIEL':{\n",
    "            'PM': {\n",
    "                'path':[\n",
    "                    's3://calfdata/INPI/TC_1/01_donnee_source/Stock/Stock_Partiel/2018/PM',\n",
    "                    's3://calfdata/INPI/TC_1/01_donnee_source/Stock/Stock_Partiel/2019/PM'\n",
    "                       ],\n",
    "                'tables':[\n",
    "                    'pm_partiel_2018',\n",
    "                    'pm_partiel_2019'],\n",
    "                'origin':'PARTIEL',\n",
    "                'output_id':[]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'Flux': {\n",
    "        'NEW':{\n",
    "            'PM': {\n",
    "                'path':[\n",
    "                    's3://calfdata/INPI/TC_1/01_donnee_source/Flux/2017/PM/NEW',\n",
    "                    's3://calfdata/INPI/TC_1/01_donnee_source/Flux/2018/PM/NEW',\n",
    "                    's3://calfdata/INPI/TC_1/01_donnee_source/Flux/2019/PM/NEW'\n",
    "                       ],\n",
    "                'tables':[\n",
    "                    'pm_new_2017',\n",
    "                    'pm_new_2018',\n",
    "                    'pm_new_2019'],\n",
    "                'origin':'NEW',\n",
    "                'output_id':[]\n",
    "            }\n",
    "        },\n",
    "        'PM':{\n",
    "            'PM': {\n",
    "                'path':[\n",
    "                    's3://calfdata/INPI/TC_1/01_donnee_source/Flux/2017/PM/EVT',\n",
    "                    's3://calfdata/INPI/TC_1/01_donnee_source/Flux/2018/PM/EVT',\n",
    "                    's3://calfdata/INPI/TC_1/01_donnee_source/Flux/2019/PM/EVT',\n",
    "                ],\n",
    "                'tables':[\n",
    "                    'pm_evt_2017',\n",
    "                    'pm_evt_2018',\n",
    "                    'pm_evt_2019'],\n",
    "                'origin':'PM',\n",
    "                'output_id':[]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query préparation table\n",
    "\n",
    "On prédéfini les requêtes qui seront à éxecuter dans Athena. Les paramètres des queries seront récupérés dans ``dic_`` au moment de l'éxecution de la query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### query_db = \"CREATE DATABASE IF NOT EXISTS {};\"\n",
    "\n",
    "query_tb = \\\n",
    "    \"\"\"CREATE EXTERNAL TABLE IF NOT EXISTS {0}.{1} (\n",
    "`Code Greffe`                   string,\n",
    "`Nom_Greffe`                    string,\n",
    "`Numero_Gestion`                string,\n",
    "`Siren`                         string,\n",
    "`Type_Inscription`              string,\n",
    "`Date_Immatriculation`          string,\n",
    "`Date_1re_Immatriculation`      string,\n",
    "`Date_Radiation`                string,\n",
    "`Date_Transfert`                string,\n",
    "`Sans_Activité`                 string,\n",
    "`Date_Debut_Activité`           string,\n",
    "`Date_Début_1re_Activité`       string,\n",
    "`Date_Cessation_Activité`       string,\n",
    "`Denomination`                  string,\n",
    "`Sigle`                         string,\n",
    "`Forme_Juridique`               string,\n",
    "`Associé_Unique`                string,\n",
    "`Activité_Principale`           string,\n",
    "`Type_Capital`                  string,\n",
    "`Capital`                       string,\n",
    "`Capital_Actuel`                string,\n",
    "`Devise`                        string,\n",
    "`Date_Cloture`                  string,\n",
    "`Date_Cloture_Except`           string,\n",
    "`Economie_Sociale_Solidaire`    string,\n",
    "`Durée_PM`                      string,\n",
    "`Date_Greffe`                   string,\n",
    "`Libelle_Evt`                   string,\n",
    "`csv_source` string,\n",
    "`nature` string,\n",
    "`type_data` string,\n",
    "`origin` string,\n",
    "`file_timestamp` string\n",
    "    )\n",
    "     ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
    "    WITH SERDEPROPERTIES (\n",
    "   'separatorChar' = '{3}',\n",
    "   'quoteChar' = '\"'\n",
    "   )\n",
    "     LOCATION '{2}'\n",
    "     TBLPROPERTIES ('has_encrypted_data'='false',\n",
    "              'skip.header.line.count'='1');\"\"\"\n",
    "\n",
    "query_table_concat = \\\n",
    "    \"\"\"CREATE EXTERNAL TABLE IF NOT EXISTS {0}.{1} (\n",
    "`Code Greffe` string, \n",
    "`Nom_Greffe` string, \n",
    "`Numero_Gestion` string, \n",
    "`Siren` string, \n",
    "`file_timestamp` string, \n",
    "`Type_Inscription` string, \n",
    "`Date_Immatriculation` string, \n",
    "`Date_1re_Immatriculation` string, \n",
    "`Date_Radiation` string, \n",
    "`Date_Transfert` string, \n",
    "`Sans_Activité` string, \n",
    "`Date_Debut_Activité` string, \n",
    "`Date_Début_1re_Activité` string, \n",
    "`Date_Cessation_Activité` string, \n",
    "`Denomination` string, \n",
    "`Sigle` string, \n",
    "`Forme_Juridique` string, \n",
    "`Associé_Unique` string, \n",
    "`Activité_Principale` string, \n",
    "`Type_Capital` string, \n",
    "`Capital` string, \n",
    "`Capital_Actuel` string, \n",
    "`Devise` string, \n",
    "`Date_Cloture` string, \n",
    "`Date_Cloture_Except` string, \n",
    "`Economie_Sociale_Solidaire` string, \n",
    "`Durée_PM` string, \n",
    "`Date_Greffe` string, \n",
    "`Libelle_Evt` string, \n",
    "`csv_source` string, \n",
    "`origin` string\n",
    "    )\n",
    "     ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
    "    WITH SERDEPROPERTIES (\n",
    "   'separatorChar' = ',',\n",
    "   'quoteChar' = '\"'\n",
    "   )\n",
    "     LOCATION '{2}'\n",
    "     TBLPROPERTIES ('has_encrypted_data'='false',\n",
    "              'skip.header.line.count'='1');\"\"\"\n",
    "\n",
    "\n",
    "query_drop = \"\"\" DROP TABLE `{}`;\"\"\"\n",
    "\n",
    "query_select = \"\"\"SELECT \n",
    "\"Code Greffe\",\n",
    "\"Nom_Greffe\",\n",
    "\"Numero_Gestion\",\n",
    "\"Siren\",\n",
    "\"file_timestamp\", \n",
    "\"Type_Inscription\",\n",
    "\"Date_Immatriculation\",\n",
    "\"Date_1re_Immatriculation\",\n",
    "\"Date_Radiation\",\n",
    "\"Date_Transfert\",\n",
    "\"Sans_Activité\",\n",
    "\"Date_Debut_Activité\",\n",
    "\"Date_Début_1re_Activité\",\n",
    "\"Date_Cessation_Activité\",\n",
    "\"Denomination\",\n",
    "\"Sigle\",\n",
    "\"Forme_Juridique\",\n",
    "\"Associé_Unique\",\n",
    "\"Activité_Principale\",\n",
    "\"Type_Capital\",\n",
    "\"Capital\",\n",
    "\"Capital_Actuel\",\n",
    "\"Devise\",\n",
    "\"Date_Cloture\",\n",
    "\"Date_Cloture_Except\",\n",
    "\"Economie_Sociale_Solidaire\",\n",
    "\"Durée_PM\",\n",
    "\"Date_Greffe\",\n",
    "\"Libelle_Evt\",\n",
    "\"csv_source\",\n",
    "\"origin\"\n",
    "FROM \"inpi\".\"{}\"\n",
    "WHERE \"siren\" !=''\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query préparation événement\n",
    "\n",
    "La query est générée via un loop dans l'étape 3 afin d'éviter les copier/coller redondants. Dans l'ensemble, la query va reconstruire l'ensemble des valeurs manquantes pour chaque csv (ie date de transmission). A noter que la query va récupérer la dernière ligne du quadruplet `siren`,`code greffe`, `numero_gestion`, `id_etablissement`. \n",
    "\n",
    "La liste des champs pouvant être affectés par un changement est stockée dans `list_change`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_change = [\n",
    "\"Type_Inscription\",\n",
    "\"Date_Immatriculation\",\n",
    "\"Date_1re_Immatriculation\",\n",
    "\"Date_Radiation\",\n",
    "\"Date_Transfert\",\n",
    "\"Sans_Activité\",\n",
    "\"Date_Debut_Activité\",\n",
    "\"Date_Début_1re_Activité\",\n",
    "\"Date_Cessation_Activité\",\n",
    "\"Denomination\",\n",
    "\"Sigle\",\n",
    "\"Forme_Juridique\",\n",
    "\"Associé_Unique\",\n",
    "\"Activité_Principale\",\n",
    "\"Type_Capital\",\n",
    "\"Capital\",\n",
    "\"Capital_Actuel\",\n",
    "\"Devise\",\n",
    "\"Date_Cloture\",\n",
    "\"Date_Cloture_Except\",\n",
    "\"Economie_Sociale_Solidaire\",\n",
    "\"Durée_PM\",\n",
    "\"Date_Greffe\",\n",
    "\"Libelle_Evt\",\n",
    "\"csv_source\"\n",
    "]\n",
    "\n",
    "top = \"\"\"WITH createID AS (\n",
    "  SELECT \n",
    "   *, \n",
    "    ROW_NUMBER() OVER (\n",
    "      PARTITION BY \n",
    "      siren,\n",
    "      \"code greffe\",\n",
    "      \"Nom_Greffe\",\n",
    "      numero_gestion,\n",
    " \n",
    "      file_timestamp\n",
    "    ) As row_ID, \n",
    "    DENSE_RANK () OVER (\n",
    "      ORDER BY \n",
    "        siren, \n",
    "        \"code greffe\",\n",
    "        \"Nom_Greffe\",\n",
    "        numero_gestion, \n",
    "        file_timestamp\n",
    "    ) As ID \n",
    "  FROM \n",
    "    \"inpi\".\"{}\" \n",
    ") \n",
    "SELECT \n",
    "  * \n",
    "FROM \n",
    "  (\n",
    "    WITH filled AS (\n",
    "      SELECT \n",
    "        ID, \n",
    "        row_ID, \n",
    "        siren, \n",
    "        \"Nom_Greffe\",\n",
    "        \"code greffe\", \n",
    "        numero_gestion, \n",
    "        file_timestamp, \n",
    "\"\"\"\n",
    "\n",
    "top_1 = \"\"\"first_value(\"{0}\") over (partition by ID, \"{0}_partition\" order by \n",
    "ID, row_ID\n",
    " ) as \"{0}\"\n",
    "\"\"\"\n",
    "\n",
    "middle = \"\"\"FROM \n",
    "        (\n",
    "          SELECT \n",
    "            *, \"\"\"\n",
    "\n",
    "middle_2 = \"\"\"sum(case when \"{0}\" = '' then 0 else 1 end) over (partition by ID \n",
    "order by  row_ID) as \"{0}_partition\" \n",
    "\"\"\"\n",
    "\n",
    "bottom = \"\"\" \n",
    "          FROM \n",
    "            createID \n",
    "          ORDER BY \n",
    "            ID, row_ID ASC\n",
    "        ) \n",
    "      ORDER BY \n",
    "        ID, \n",
    "        row_ID\n",
    "    ) \n",
    "    SELECT \"Code Greffe\",\n",
    "\"Nom_Greffe\",\n",
    "\"Numero_Gestion\",\n",
    "\"Siren\",\n",
    "file_timestamp,\n",
    "\"Type_Inscription\",\n",
    "\"Date_Immatriculation\",\n",
    "\"Date_1re_Immatriculation\",\n",
    "\"Date_Radiation\",\n",
    "\"Date_Transfert\",\n",
    "\"Sans_Activité\",\n",
    "\"Date_Debut_Activité\",\n",
    "\"Date_Début_1re_Activité\",\n",
    "\"Date_Cessation_Activité\",\n",
    "\"Denomination\",\n",
    "\"Sigle\",\n",
    "\"Forme_Juridique\",\n",
    "\"Associé_Unique\",\n",
    "\"Activité_Principale\",\n",
    "\"Type_Capital\",\n",
    "\"Capital\",\n",
    "\"Capital_Actuel\",\n",
    "\"Devise\",\n",
    "\"Date_Cloture\",\n",
    "\"Date_Cloture_Except\",\n",
    "\"Economie_Sociale_Solidaire\",\n",
    "\"Durée_PM\",\n",
    "\"Date_Greffe\",\n",
    "\"Libelle_Evt\",\n",
    "\"csv_source\",\n",
    "CASE WHEN Siren IS NOT NULL THEN 'EVT' \n",
    "ELSE NULL END as origin\n",
    "    FROM \n",
    "      (\n",
    "        SELECT \n",
    "          *, \n",
    "          ROW_NUMBER() OVER(\n",
    "            PARTITION BY ID \n",
    "            ORDER BY \n",
    "              ID, row_ID DESC\n",
    "          ) AS max_value \n",
    "        FROM \n",
    "          filled\n",
    "      ) AS T \n",
    "    WHERE \n",
    "      max_value = 1\n",
    "  )ORDER BY siren,\"Nom_Greffe\", \"code greffe\",\n",
    "      numero_gestion,\n",
    "      file_timestamp\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_fillin = top.format('test')\n",
    "for x, val in enumerate(list_change):\n",
    "\n",
    "    if x != len(list_change) -1:\n",
    "        query_fillin+=top_1.format(val)+ \",\"\n",
    "    else:\n",
    "        query_fillin+=top_1.format(val)\n",
    "        query_fillin+= middle\n",
    "\n",
    "for x, val in enumerate(list_change):\n",
    "    if x != len(list_change) -1:\n",
    "        query_fillin+=middle_2.format(val)+ \",\"\n",
    "    else:\n",
    "        query_fillin+=middle_2.format(val)\n",
    "        query_fillin+=bottom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query préparation partiel\n",
    "\n",
    "Dans cette étape, il faut vérifier si un quadruplet `siren`,`code greffe`, `numero_gestion`, `id_etablissement` possède une ligne `Partiel`. Auquel cas, une nouvelle variable est recréée indiquant pour toutes les lignes précédant un `Partiel` les valeurs à ignorer. On prend la date maximum `date_max` des stocks partiels par quadruplet, si la date de transfert est inférieure a la `date_max`, alors on ignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_partiel = \"\"\"WITH to_date AS (\n",
    "  SELECT \n",
    "\"Code Greffe\",\n",
    "\"Nom_Greffe\",\n",
    "\"Numero_Gestion\",\n",
    "\"Siren\",\n",
    "\"Type_Inscription\",\n",
    "\"Date_Immatriculation\",\n",
    "\"Date_1re_Immatriculation\",\n",
    "\"Date_Radiation\",\n",
    "\"Date_Transfert\",\n",
    "\"Sans_Activité\",\n",
    "\"Date_Debut_Activité\",\n",
    "\"Date_Début_1re_Activité\",\n",
    "\"Date_Cessation_Activité\",\n",
    "\"Denomination\",\n",
    "\"Sigle\",\n",
    "\"Forme_Juridique\",\n",
    "\"Associé_Unique\",\n",
    "\"Activité_Principale\",\n",
    "\"Type_Capital\",\n",
    "\"Capital\",\n",
    "\"Capital_Actuel\",\n",
    "\"Devise\",\n",
    "\"Date_Cloture\",\n",
    "\"Date_Cloture_Except\",\n",
    "\"Economie_Sociale_Solidaire\",\n",
    "\"Durée_PM\",\n",
    "\"Date_Greffe\",\n",
    "\"Libelle_Evt\",\n",
    "\"csv_source\",\n",
    "\"origin\", Coalesce(try(cast(file_timestamp as timestamp)))  as file_timestamp\n",
    "FROM \"inpi\".\"initial_partiel_evt_new_pm\"\n",
    "WHERE siren !='' AND file_timestamp !=''\n",
    "                 )\n",
    "SELECT *\n",
    "FROM (\n",
    "  WITH max_date_partiel AS(\n",
    "SELECT siren, \"code greffe\", nom_greffe, numero_gestion, \n",
    "MAX(file_timestamp) as max_partiel\n",
    "FROM to_date\n",
    "WHERE origin = 'Partiel'\n",
    "GROUP BY  siren, \"code greffe\", nom_greffe, numero_gestion\n",
    "    )\n",
    "  SELECT \n",
    "  to_date.\"Code Greffe\",\n",
    "to_date.\"Nom_Greffe\",\n",
    "to_date.\"Numero_Gestion\",\n",
    "to_date.\"Siren\",\n",
    "to_date.\"file_timestamp\",\n",
    "max_date_partiel.max_partiel,\n",
    "CASE WHEN to_date.\"file_timestamp\" <  max_date_partiel.max_partiel \n",
    "  THEN 'IGNORE' ELSE NULL END AS status, \n",
    "to_date.\"origin\" ,\n",
    "to_date.\"Type_Inscription\",\n",
    "to_date.\"Date_Immatriculation\",\n",
    "to_date.\"Date_1re_Immatriculation\",\n",
    "to_date.\"Date_Radiation\",\n",
    "to_date.\"Date_Transfert\",\n",
    "to_date.\"Sans_Activité\",\n",
    "to_date.\"Date_Debut_Activité\",\n",
    "to_date.\"Date_Début_1re_Activité\",\n",
    "to_date.\"Date_Cessation_Activité\",\n",
    "to_date.\"Denomination\",\n",
    "to_date.\"Sigle\",\n",
    "to_date.\"Forme_Juridique\",\n",
    "to_date.\"Associé_Unique\",\n",
    "to_date.\"Activité_Principale\",\n",
    "to_date.\"Type_Capital\",\n",
    "to_date.\"Capital\",\n",
    "to_date.\"Capital_Actuel\",\n",
    "to_date.\"Devise\",\n",
    "to_date.\"Date_Cloture\",\n",
    "to_date.\"Date_Cloture_Except\",\n",
    "to_date.\"Economie_Sociale_Solidaire\",\n",
    "to_date.\"Durée_PM\",\n",
    "to_date.\"Date_Greffe\",\n",
    "to_date.\"Libelle_Evt\",\n",
    "to_date.\"csv_source\"\n",
    "  FROM to_date\n",
    "  LEFT JOIN max_date_partiel on\n",
    "  to_date.siren =max_date_partiel.siren AND\n",
    "  to_date.\"code greffe\" =max_date_partiel.\"code greffe\" AND\n",
    "  to_date.nom_greffe =max_date_partiel.nom_greffe AND\n",
    "  to_date.numero_gestion =max_date_partiel.numero_gestion\n",
    "  ORDER BY siren, \"code greffe\", nom_greffe, numero_gestion,\n",
    "  file_timestamp\n",
    "  )\"\"\"\n",
    "\n",
    "query_table_all = \"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS {0}.{1} (\n",
    "`code greffe` string,\n",
    "`Nom_Greffe` string,\n",
    "`Numero_Gestion` string,\n",
    "`Siren` string,\n",
    "`file_timestamp` string,\n",
    "`max_partiel` string,\n",
    "`status` string,\n",
    "`origin` string,\n",
    "`Type_Inscription` string,\n",
    "`Date_Immatriculation` string,\n",
    "`Date_1re_Immatriculation` string,\n",
    "`Date_Radiation` string,\n",
    "`Date_Transfert` string,\n",
    "`Sans_Activité` string,\n",
    "`Date_Debut_Activité` string,\n",
    "`Date_Début_1re_Activité` string,\n",
    "`Date_Cessation_Activité` string,\n",
    "`Denomination` string,\n",
    "`Sigle` string,\n",
    "`Forme_Juridique` string,\n",
    "`Associé_Unique` string,\n",
    "`Activité_Principale` string,\n",
    "`Type_Capital` string,\n",
    "`Capital` string,\n",
    "`Capital_Actuel` string,\n",
    "`Devise` string,\n",
    "`Date_Cloture` string,\n",
    "`Date_Cloture_Except` string,\n",
    "`Economie_Sociale_Solidaire` string,\n",
    "`Durée_PM` string,\n",
    "`Date_Greffe` string,\n",
    "`Libelle_Evt` string,\n",
    "`csv_source` string\n",
    ")\n",
    "     ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
    "    WITH SERDEPROPERTIES (\n",
    "   'separatorChar' = ',',\n",
    "   'quoteChar' = '\"'\n",
    "   )\n",
    "     LOCATION '{2}'\n",
    "     TBLPROPERTIES ('has_encrypted_data'='false',\n",
    "              'skip.header.line.count'='1');\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query remplissage EVT via initial, partiel, creation\n",
    "\n",
    "Il y a deux étapes à suivre. \n",
    "\n",
    "Pour remplir les événements, il faut prendre la ligne t-1, et compléter les champs manquants. En effet, l'INPI ne transmet que les champs modifiés, les champs non modifiés sont transmis vides.\n",
    "Dans l'[étape 2](https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/01_Athena_concatenate_ETS.md#step-2-concatenation-data), nous avons pu remplir puis garder le dernier événement par date de transmission. Toutefois, dans la majeure partie des cas, les champs sont vides, car ils n'ont pas d'antécédents. L'antécédent provient soit d'un événement initial, soit d'un partiel ou création. Dans le cas de figure ou l'événement est une création.\n",
    "\n",
    "Finalement, il faut reconstituer les valeurs manquantes des evenements en utilisant les informations qui ne sont pas communiquées dans les csv événements. En effet, le csv événement ne renseigne que les valeurs obligatoires et les modifications, laissant vides les autres champs. Pour récupérer les champs manquants, il faut utliser la valeur précédente pour le quadruplet `siren`,`code greffe`, `numero_gestion`, `id_etablissement`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle_3 = \"\"\"sum(case when origin = 'EVT' AND \"{0}\" = '' then 0 else 1 end) \n",
    "over (partition by ID \n",
    "order by  row_ID) as \"{0}_partition\" \n",
    "\"\"\"\n",
    "\n",
    "bottom_1 = \"\"\" \n",
    "         FROM \n",
    "            createID \n",
    "          ORDER BY \n",
    "            ID, row_ID ASC\n",
    "        ) \n",
    "      ORDER BY \n",
    "        ID, \n",
    "        row_ID\n",
    "    ) \n",
    "    SELECT \n",
    "\"Code Greffe\",\n",
    "\"Nom_Greffe\",\n",
    "\"Numero_Gestion\",\n",
    "\"Siren\",    \n",
    "\"status\",\n",
    "CASE WHEN siren IS NOT NULL THEN 'EVT' \n",
    "ELSE NULL END as origin,    \n",
    "file_timestamp,\n",
    "\"Type_Inscription\",\n",
    "\"Date_Immatriculation\",\n",
    "\"Date_1re_Immatriculation\",\n",
    "\"Date_Radiation\",\n",
    "\"Date_Transfert\",\n",
    "\"Sans_Activité\",\n",
    "\"Date_Debut_Activité\",\n",
    "\"Date_Début_1re_Activité\",\n",
    "\"Date_Cessation_Activité\",\n",
    "\"Denomination\",\n",
    "\"Sigle\",\n",
    "\"Forme_Juridique\",\n",
    "\"Associé_Unique\",\n",
    "\"Activité_Principale\",\n",
    "\"Type_Capital\",\n",
    "\"Capital\",\n",
    "\"Capital_Actuel\",\n",
    "\"Devise\",\n",
    "\"Date_Cloture\",\n",
    "\"Date_Cloture_Except\",\n",
    "\"Economie_Sociale_Solidaire\",\n",
    "\"Durée_PM\",\n",
    "\"Date_Greffe\",\n",
    "\"Libelle_Evt\",\n",
    "\"csv_source\"\n",
    "    FROM filled\n",
    "  )ORDER BY siren,\"Nom_Greffe\", \"code greffe\",\n",
    "      numero_gestion, \n",
    "      file_timestamp\n",
    "  )\n",
    "    )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 Concatenation data\n",
    "                \n",
    "### Steps:\n",
    "\n",
    "L'ID de la query creation_csv est stocké dans le `dic_` car il faut plusieurs minutes pour lire les tables et sauvegarder en csv.\n",
    "\n",
    "A noter que la query `query_csv` ne prend pas toutes les variables (celles crééent lors de l'extraction du FTP) car manque de mémoire lors de la préparation des événements.\n",
    "\n",
    "\n",
    " \n",
    "- Stock\n",
    "    - Initial:\n",
    "        - Création table en concatenant tous les fichiers de ce dossier [Initial](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/TC_1/01_donnee_source/Stock/Stock_initial/ETS/)\n",
    "            - Output: [INPI/sql_output/](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/sql_output/)\n",
    "        - Création csv\n",
    "            - Output stocké dans le dictionaire des paramaitres key `output_id`\n",
    "    - Partiel:\n",
    "        - Création table en concatenant tous les fichiers de ce dossier [Partiel](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/TC_1/01_donnee_source/Stock/Stock_partiel/)\n",
    "            - Output: [INPI/sql_output/](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/sql_output/)\n",
    "        - Création csv\n",
    "            - Output stocké dans le dictionaire des paramaitres key `output_id`\n",
    "- Flux\n",
    "    - NEW:\n",
    "        - Création table en concatenant tous les fichiers de ce dossier [Flux-new](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/TC_1/01_donnee_source/Flux/2017/ETS/NEW/)\n",
    "            - Output: [INPI/sql_output/](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/sql_output/)\n",
    "        - Création csv\n",
    "            - Output stocké dans le dictionaire des paramaitres key `output_id`\n",
    "    - EVT\n",
    "        - Création table en concatenant tous les fichiers de ce dossier [Flux-EVT](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/TC_1/01_donnee_source/Flux/2017/ETS/EVT/)\n",
    "            - Output: [INPI/sql_output/](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/sql_output/)\n",
    "        - Création csv: Run query pour remplir les valeurs manquantes et extraire l'entrée max par jour/heure de transmission.\n",
    "            - Output stocké dans le dictionaire des paramaitres key `output_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROP TABLES\n",
    "for i in ['pm_initial', 'pm_partiel_2018', 'pm_partiel_2019',\n",
    "          'pm_new_2017', 'pm_new_2018', 'pm_new_2019', 'pm_evt_2017',\n",
    "         'pm_evt_2018', 'pm_evt_2019']:\n",
    "    query = \"DROP TABLE `{}`\".format(i)\n",
    "    output = athena.run_query(\n",
    "                        query=query,\n",
    "                        database=dic_['global']['database'],\n",
    "                        s3_output=dic_['global']['output']\n",
    "                    )\n",
    "    print(output['QueryExecutionId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nature, values in dic_.items():\n",
    "    if nature != 'global':\n",
    "        for origin, val in dic_[nature].items():\n",
    "            for type_, v in dic_[nature][origin].items():\n",
    "                if origin == 'INITIAL':\n",
    "                    #### Creation table\n",
    "                    create_table = query_tb.format(\n",
    "                        dic_['global']['database'],\n",
    "                        v['tables'],\n",
    "                        v['path'],\n",
    "                        \";\"\n",
    "                    )\n",
    "                    time.sleep(2)\n",
    "                    athena.run_query(\n",
    "                        query=create_table,\n",
    "                        database=dic_['global']['database'],\n",
    "                        s3_output=dic_['global']['output'])\n",
    "                    \n",
    "                    #### Creation CSV\n",
    "                    time.sleep(1)\n",
    "                    query = query_select.format(\n",
    "                        v['tables'])\n",
    "                    \n",
    "                    output = athena.run_query(\n",
    "                        query=query,\n",
    "                        database=dic_['global']['database'],\n",
    "                        s3_output=dic_['global']['output']\n",
    "                    )\n",
    "                    \n",
    "                    v['output_id'].append(output['QueryExecutionId'])\n",
    "\n",
    "                else:\n",
    "                    for i in range(0,len(v['tables'])):\n",
    "                        create_table = query_tb.format(\n",
    "                                dic_['global']['database'],\n",
    "                                v['tables'][i],\n",
    "                                v['path'][i], \n",
    "                                \";\"\n",
    "                            )\n",
    "                        \n",
    "                        time.sleep(2)\n",
    "                        athena.run_query(\n",
    "                        query=create_table,\n",
    "                        database=dic_['global']['database'],\n",
    "                        s3_output=dic_['global']['output'])\n",
    "                        \n",
    "                        time.sleep(1)\n",
    "                        \n",
    "                        if origin != 'EVT':\n",
    "                            query = query_select.format(\n",
    "                            v['tables'][i])\n",
    "                        \n",
    "                            output = athena.run_query(\n",
    "                            query=query,\n",
    "                            database=dic_['global']['database'],\n",
    "                            s3_output=dic_['global']['output']\n",
    "                        )\n",
    "                            v['output_id'].append(output['QueryExecutionId'])\n",
    "                        ### Dealing avec les evenements    \n",
    "                        else:\n",
    "                            query_fillin = top.format(v['tables'][i])\n",
    "                            for x, val in enumerate(list_change):\n",
    "\n",
    "                                if x != len(list_change) -1:\n",
    "                                    query_fillin+=top_1.format(val)+ \",\"\n",
    "                                else:\n",
    "                                    query_fillin+=top_1.format(val)\n",
    "                                    query_fillin+= middle\n",
    "\n",
    "                            for x, val in enumerate(list_change):\n",
    "                                if x != len(list_change) -1:\n",
    "                                    query_fillin+=middle_2.format(val)+ \",\"\n",
    "                                else:\n",
    "                                    query_fillin+=middle_2.format(val)\n",
    "                                    query_fillin+=bottom \n",
    "                                    \n",
    "                            output = athena.run_query(\n",
    "                                query=query_fillin,\n",
    "                                database=dic_['global']['database'],\n",
    "                                s3_output=dic_['global']['output']\n",
    "                            )\n",
    "                            v['output_id'].append(output['QueryExecutionId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 Bis: Copier csv\n",
    "\n",
    "Dans l'étape 1, nous avons stocké les ID dans le dictionaire de paramètre. Il faut environ 10/15 minutes pour préparer tous les csv. \n",
    "\n",
    "Dans cette étape, on va simplement récuperer les csv créés dans le dossier [INPI/sql_output/](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/sql_output/) pour les déplacer dans le nouveau dossier [INPI/sql_output_preparation_pm/](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/sql_output_preparation_pm/)\n",
    "\n",
    "Le dossier va contenir les csv suivants:\n",
    "\n",
    "- INPI/sql_output_preparation_pm/pm_initial.csv\n",
    "- INPI/sql_output_preparation_pm/pm_partiel_2018.csv\n",
    "- INPI/sql_output_preparation_pm/pm_partiel_2019.csv\n",
    "- INPI/sql_output_preparation_pm/pm_new_2017.csv\n",
    "- INPI/sql_output_preparation_pm/pm_new_2018.csv\n",
    "- INPI/sql_output_preparation_pm/pm_new_2019.csv\n",
    "- INPI/sql_output_preparation_pm/pm_evt_2017.csv\n",
    "- INPI/sql_output_preparation_pm/pm_evt_2018.csv\n",
    "- INPI/sql_output_preparation_pm/pm_evt_2019.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nature, values in dic_.items():\n",
    "    if nature != 'global':\n",
    "        for origin, val in dic_[nature].items():\n",
    "            for type_, v in dic_[nature][origin].items():\n",
    "                for i, id_ in enumerate(v['output_id']):\n",
    "                    source_key = \"{}/{}.csv\".format(\n",
    "                        dic_['global']['output'],\n",
    "                        id_\n",
    "                               )\n",
    "\n",
    "                    if origin == 'INITIAL':\n",
    "                        destination_key = \"{}/{}.csv\".format(\n",
    "                        dic_['global']['output_preparation'],\n",
    "                        v['tables']\n",
    "                    )\n",
    "                    else:\n",
    "                        destination_key = \"{}/{}.csv\".format(\n",
    "                        dic_['global']['output_preparation'],\n",
    "                        v['tables'][i]\n",
    "                    )\n",
    "                    results = s3.copy_object_s3(\n",
    "                        source_key = source_key,\n",
    "                        destination_key = destination_key,\n",
    "                        remove = True\n",
    "                    )\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Creation table Initial/Partiel/EVT/NEW\n",
    "\n",
    "Pour cette étape, on récupère les csv de ce [dossier](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/sql_output_preparation/), qu'on aggrège avant de préparer les valeurs manquantes.\n",
    "\n",
    "La table agrégée s'appelle `initial_partiel_evt_new_etb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = 'initial_partiel_evt_new_pm'\n",
    "create_table = query_table_concat.format(\n",
    "    dic_['global']['database'],\n",
    "    table,\n",
    "    \"s3://calfdata/{}\".format(\n",
    "        dic_['global']['output_preparation'])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athena.run_query(\n",
    "    query=create_table,\n",
    "    database=dic_['global']['database'],\n",
    "    s3_output=dic_['global']['output']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Creation statut partiel\n",
    "\n",
    "Dans cette étape, on crée une colonne `status`, qui indique si les lignes sont a ignorer (IGNORE) ou non (Vide). La logique c'est de prendre la date maximum des stocks partiels par quadruplet, si la date de transfert est inférieure a la date max, alors on ignore. La query prend quelques minutes.\n",
    "\n",
    "Output de la query va dans ce dossier [INPI/sql_output_status](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/sql_output_status/?region=eu-west-3&tab=overview)\n",
    "La table avec `status` s'appelle `initial_partiel_evt_new_ets_status`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = athena.run_query(\n",
    "    query=query_partiel,\n",
    "    database=dic_['global']['database'],\n",
    "    s3_output=dic_['global']['output']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_['global']['PM_step4_id'] = output['QueryExecutionId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = 'initial_partiel_evt_new_pm_status'\n",
    "source_key = \"{}/{}.csv\".format(\n",
    "                        dic_['global']['output'],\n",
    "                        dic_['global']['PM_step4_id']\n",
    "                               )\n",
    "print(source_key)\n",
    "\n",
    "destination_key = \"{}/{}.csv\".format(\n",
    "                        'INPI/sql_output_status_pm',\n",
    "                        table\n",
    "                    )\n",
    "results = s3.copy_object_s3(\n",
    "                        source_key = source_key,\n",
    "                        destination_key = destination_key,\n",
    "                        remove = True\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_status = query_table_all.format(\n",
    "    dic_['global']['database'], \n",
    "    table,\n",
    "     \"s3://calfdata/{}\".format('INPI/sql_output_status_pm')\n",
    ")\n",
    "\n",
    "athena.run_query(\n",
    "    query=query_status,\n",
    "    database=dic_['global']['database'],\n",
    "    s3_output=dic_['global']['output']\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Remplissage observations manquantes\n",
    "\n",
    "Il y a deux étapes a suivre. Pour remplir les événements, il faut prendre la ligne t-1, et compléter les champs manquants. En effet, l'INPI ne renseigne que les modifications. Dans l'étape 2, nous avons pu remplir puis garder le dernier événement pour date de transmission. Toutefois, dans la majeur partie des cas, les champs sont vides, car ils n'ont pas d'antécédents. L'antécédent provient soit d'un événement initial, soit d'un partiel ou création. Dans le cas de figure ou l'événement est une création\n",
    "\n",
    "- Remplissage des valeurs manquantes pour les observations.\n",
    "    - Si `origin` es égale a `EVT`, alors trie sur `siren,'code greffe', numero_gestion, date_greffe_temp_` et récupère valeur - 1\n",
    "    - Remplissage des champs manquants pour les événements séquentiels, uniquement événements\n",
    "        - Output: [INPI/sql_output/](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/sql_output_final_pm/)\n",
    "        - Output stocké dans le dictionaire des paramaitres key `['global']['table_final_id']['PM']['EVT']`\n",
    "- Filtre table XX pour le champ origin autre que EVT\n",
    "   - Output: [INPI/sql_output/](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/sql_output_final_pm/)\n",
    "        - Output stocké dans le dictionaire des paramètres key `['global']['table_final_id']['PM']['Not_EVT']`\n",
    "- Concaténation deux précédentes step.\n",
    "    - Output: [TC_1/02_preparation_donnee/PM](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/TC_1/02_preparation_donnee/PM/)\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = 'initial_partiel_evt_new_pm_status'\n",
    "list_change = [\n",
    "\"Type_Inscription\",\n",
    "\"Date_Immatriculation\",\n",
    "\"Date_1re_Immatriculation\",\n",
    "\"Date_Radiation\",\n",
    "\"Date_Transfert\",\n",
    "\"Sans_Activité\",\n",
    "\"Date_Debut_Activité\",\n",
    "\"Date_Début_1re_Activité\",\n",
    "\"Date_Cessation_Activité\",\n",
    "\"Denomination\",\n",
    "\"Sigle\",\n",
    "\"Forme_Juridique\",\n",
    "\"Associé_Unique\",\n",
    "\"Activité_Principale\",\n",
    "\"Type_Capital\",\n",
    "\"Capital\",\n",
    "\"Capital_Actuel\",\n",
    "\"Devise\",\n",
    "\"Date_Cloture\",\n",
    "\"Date_Cloture_Except\",\n",
    "\"Economie_Sociale_Solidaire\",\n",
    "\"Durée_PM\",\n",
    "\"max_partiel\",\n",
    "\"csv_source\"\n",
    "]\n",
    "\n",
    "query_ = \"\"\"WITH convert AS (\n",
    "  SELECT \n",
    "    siren,\n",
    "      \"code greffe\",\n",
    "      \"Nom_Greffe\",\n",
    "      numero_gestion,\n",
    "origin, \"status\",\n",
    "Coalesce(\n",
    "         try(date_parse(file_timestamp, '%Y-%m-%d')),\n",
    "         try(date_parse(file_timestamp, '%Y-%m-%d %hh:%mm:%ss.SSS')),\n",
    "         try(date_parse(file_timestamp, '%Y-%m-%d %hh:%mm:%ss')),\n",
    "         try(cast(file_timestamp as timestamp))\n",
    "       )  as file_timestamp,\n",
    "\n",
    "Coalesce(\n",
    "         try(date_parse(date_greffe, '%Y-%m-%d')),\n",
    "         try(date_parse(date_greffe, '%Y/%m/%d')),\n",
    "         try(date_parse(date_greffe, '%d %M %Y')),\n",
    "         try(date_parse(date_greffe, '%d/%m/%Y')),\n",
    "         try(date_parse(date_greffe, '%d-%m-%Y'))\n",
    "  )\n",
    "  as date_greffe,\n",
    "  \n",
    "  libelle_evt,\n",
    "  \n",
    "\"Type_Inscription\",\n",
    "\"Date_Immatriculation\",\n",
    "\"Date_1re_Immatriculation\",\n",
    "\"Date_Radiation\",\n",
    "\"Date_Transfert\",\n",
    "\"Sans_Activité\",\n",
    "\"Date_Debut_Activité\",\n",
    "\"Date_Début_1re_Activité\",\n",
    "\"Date_Cessation_Activité\",\n",
    "\"Denomination\",\n",
    "\"Sigle\",\n",
    "\"Forme_Juridique\",\n",
    "\"Associé_Unique\",\n",
    "\"Activité_Principale\",\n",
    "\"Type_Capital\",\n",
    "\"Capital\",\n",
    "\"Capital_Actuel\",\n",
    "\"Devise\",\n",
    "\"Date_Cloture\",\n",
    "\"Date_Cloture_Except\",\n",
    "\"Economie_Sociale_Solidaire\",\n",
    "\"Durée_PM\",\n",
    "\n",
    "\"max_partiel\",\"csv_source\"\n",
    "  FROM \"inpi\".\"{}\"\n",
    "\n",
    "  )SELECT * \n",
    "  FROM (\n",
    "    WITH temp AS (\n",
    "                 SELECT           siren,\n",
    "      \"code greffe\",\n",
    "      \"Nom_Greffe\",\n",
    "      numero_gestion,\n",
    "                  origin, \n",
    "                 \"status\",\n",
    "                 file_timestamp,\n",
    "                 date_greffe, libelle_evt,\"\"\"\n",
    "\n",
    "for x, value in enumerate(list_change):\n",
    "    query = \"\"\"CASE WHEN origin = 'EVT' AND status != 'IGNORE' AND \"{0}\" = '' THEN \n",
    "LAG (\"{0}\", 1) OVER (  PARTITION BY     siren,      \"code greffe\",      numero_gestion\n",
    " ORDER BY siren,'code greffe', numero_gestion, file_timestamp\n",
    " ) ELSE \"{0}\" END AS \"{0}\" \n",
    "\"\"\".format(value)\n",
    "    if  x != len(list_change)-1:\n",
    "        query_ +=query +\",\"\n",
    "    else:\n",
    "        query_ +=query\n",
    "        end = \"\"\"FROM convert\n",
    "ORDER BY siren,'code greffe', numero_gestion, file_timestamp\n",
    ")\n",
    "SELECT *\n",
    "FROM (\n",
    "  WITH createID AS (\n",
    "    SELECT  \n",
    "    ROW_NUMBER() OVER (\n",
    "      PARTITION BY \n",
    "    siren,\n",
    "      \"code greffe\",\n",
    "      \"Nom_Greffe\",\n",
    "      numero_gestion,\n",
    "      date_greffe\n",
    "    ) As row_ID, \n",
    "    DENSE_RANK () OVER (\n",
    "      ORDER BY \n",
    "    siren,\n",
    "      \"code greffe\",\n",
    "      \"Nom_Greffe\",\n",
    "      numero_gestion,\n",
    "        date_greffe\n",
    "    ) As ID, *\n",
    "    FROM temp\n",
    "    WHERE origin = 'EVT'\n",
    "    )\n",
    "SELECT \n",
    "  * \n",
    "FROM \n",
    "  (\n",
    "    WITH filled AS (\n",
    "      SELECT \n",
    "        ID, \n",
    "        row_ID, \n",
    "        siren, \n",
    "        \"Nom_Greffe\",\n",
    "        \"code greffe\", \n",
    "        numero_gestion, \n",
    "        \"status\",\n",
    "        date_greffe,\n",
    "        file_timestamp,\n",
    "        libelle_evt,\n",
    "\"\"\"\n",
    "        query_ += end\n",
    "for x, val in enumerate(list_change):\n",
    "\n",
    "    if x != len(list_change) -1:\n",
    "        query_+=top_1.format(val)+ \",\"\n",
    "    else:\n",
    "        query_+=top_1.format(val)\n",
    "        query_+= middle\n",
    "\n",
    "for x, val in enumerate(list_change):\n",
    "    if x != len(list_change) -1:\n",
    "        query_+=middle_2.format(val)+ \",\"\n",
    "    else:\n",
    "        query_+=middle_2.format(val)\n",
    "        query_+=bottom_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(query_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = athena.run_query(\n",
    "    query=query_.format(table),\n",
    "    database=dic_['global']['database'],\n",
    "    s3_output=dic_['global']['output']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_['global']['table_final_id']['PM']['EVT'] =  output['QueryExecutionId']\n",
    "dic_['global']['table_final_id']['PM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_key = \"{}/{}.csv\".format(dic_['global']['output'],\n",
    "                                dic_['global']['table_final_id']['PM']['EVT']\n",
    "                               )\n",
    "destination_key = \"{}/{}.csv\".format(\"INPI/sql_output_final_pm\",\n",
    "                                          'initial_partiel_evt_new_pm_status_EVT'\n",
    "                                         )\n",
    "results = s3.copy_object_s3(source_key = source_key,\n",
    "             destination_key = destination_key,\n",
    "             remove = False\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not Evt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = 'initial_partiel_evt_new_pm_status'\n",
    "query = \"\"\"SELECT \n",
    "\"Code Greffe\",\n",
    "\"Nom_Greffe\",\n",
    "\"Numero_Gestion\",\n",
    "\"Siren\",    \n",
    "\"status\",\n",
    "\"origin\",    \n",
    "Coalesce(\n",
    "         try(date_parse(file_timestamp, '%Y-%m-%d')),\n",
    "         try(date_parse(file_timestamp, '%Y-%m-%d %hh:%mm:%ss.SSS')),\n",
    "         try(date_parse(file_timestamp, '%Y-%m-%d %hh:%mm:%ss')),\n",
    "         try(cast(file_timestamp as timestamp))\n",
    "       )  as file_timestamp,\n",
    "\"Type_Inscription\",\n",
    "\"Date_Immatriculation\",\n",
    "\"Date_1re_Immatriculation\",\n",
    "\"Date_Radiation\",\n",
    "\"Date_Transfert\",\n",
    "\"Sans_Activité\",\n",
    "\"Date_Debut_Activité\",\n",
    "\"Date_Début_1re_Activité\",\n",
    "\"Date_Cessation_Activité\",\n",
    "\"Denomination\",\n",
    "\"Sigle\",\n",
    "\"Forme_Juridique\",\n",
    "\"Associé_Unique\",\n",
    "\"Activité_Principale\",\n",
    "\"Type_Capital\",\n",
    "\"Capital\",\n",
    "\"Capital_Actuel\",\n",
    "\"Devise\",\n",
    "\"Date_Cloture\",\n",
    "\"Date_Cloture_Except\",\n",
    "\"Economie_Sociale_Solidaire\",\n",
    "\"Durée_PM\",\n",
    "Coalesce(\n",
    "         try(date_parse(date_greffe, '%Y-%m-%d')),\n",
    "         try(date_parse(date_greffe, '%Y/%m/%d')),\n",
    "         try(date_parse(date_greffe, '%d %M %Y')),\n",
    "         try(date_parse(date_greffe, '%d/%m/%Y')),\n",
    "         try(date_parse(date_greffe, '%d-%m-%Y'))\n",
    "  )\n",
    "  as date_greffe,\n",
    "\"Libelle_Evt\",\n",
    "\"csv_source\"\n",
    "\n",
    "FROM {}\n",
    "WHERE origin != 'EVT'\n",
    "\"\"\"\n",
    "\n",
    "output = athena.run_query(\n",
    "    query=query.format(table),\n",
    "    database=dic_['global']['database'],\n",
    "    s3_output=dic_['global']['output']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_['global']['table_final_id']['PM']['Not_EVT'] =  output['QueryExecutionId']\n",
    "dic_['global']['table_final_id']['PM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_key = \"{}/{}.csv\".format(dic_['global']['output'],\n",
    "                                dic_['global']['table_final_id']['PM']['Not_EVT']\n",
    "                               )\n",
    "destination_key = \"{}/{}.csv\".format(\"INPI/sql_output_final_pm\",\n",
    "                                          'initial_partiel_evt_new_pm_status_no_EVT'\n",
    "                                         )\n",
    "results = s3.copy_object_s3(source_key = source_key,\n",
    "             destination_key = destination_key,\n",
    "             remove = False\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrer les dates de greffe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "CREATE TABLE inpi.pm_test_filtered\n",
    "WITH (\n",
    "  format='PARQUET'\n",
    ") AS\n",
    "\n",
    "select \n",
    "  initial_partiel_evt_new_pm_status_final.siren, \n",
    "  initial_partiel_evt_new_pm_status_final.\"code greffe\" as code_greffe, \n",
    "  initial_partiel_evt_new_pm_status_final.nom_greffe, \n",
    "  initial_partiel_evt_new_pm_status_final.numero_gestion, \n",
    "  status, \n",
    "  origin, \n",
    "  initial_partiel_evt_new_pm_status_final.date_greffe, \n",
    "  file_timestamp, \n",
    "  max_timestamp, \n",
    "  type_inscription, \n",
    "  date_immatriculation, \n",
    "  date_1re_immatriculation, \n",
    "  date_radiation, \n",
    "  date_transfert, \n",
    "  \"sans_activité\", \n",
    "  \"date_debut_activité\", \n",
    "  \"date_début_1re_activité\", \n",
    "  \"date_cessation_activité\", \n",
    "  denomination, \n",
    "  sigle, \n",
    "  forme_juridique, \n",
    "  \"associé_unique\", \n",
    "  \"activité_principale\", \n",
    "  type_capital, \n",
    "  capital, \n",
    "  capital_actuel, \n",
    "  devise, \n",
    "  date_cloture, \n",
    "  date_cloture_except, \n",
    "  economie_sociale_solidaire, \n",
    "  \"durée_pm\", \n",
    "  libelle_evt, \n",
    "  csv_source \n",
    "FROM \n",
    "  initial_partiel_evt_new_pm_status_final \n",
    "  LEFT JOIN (\n",
    "    select \n",
    "      siren, \n",
    "      \"code greffe\", \n",
    "      numero_gestion, \n",
    "      date_greffe, \n",
    "      max(file_timestamp) as max_timestamp \n",
    "    from \n",
    "      initial_partiel_evt_new_pm_status_final \n",
    "    GROUP BY \n",
    "      siren, \n",
    "      \"code greffe\", \n",
    "      numero_gestion, \n",
    "      date_greffe\n",
    "  ) as max_time \n",
    "  ON initial_partiel_evt_new_pm_status_final.siren = max_time.siren \n",
    "  AND initial_partiel_evt_new_pm_status_final.\"code greffe\" = max_time.\"code greffe\" \n",
    "  AND initial_partiel_evt_new_pm_status_final.numero_gestion = max_time.numero_gestion \n",
    "  AND initial_partiel_evt_new_pm_status_final.date_greffe = max_time.date_greffe \n",
    "WHERE \n",
    "  file_timestamp = max_timestamp \n",
    "ORDER BY \n",
    "  siren, \n",
    "  code_greffe, \n",
    "  numero_gestion, \n",
    "  date_greffe\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table finale dans Athena\n",
    "\n",
    "La dernière étape du programme consiste a récupérer tous les csv du [dossier](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/sql_output_final_pm/) afin de recréer une table appelée `initial_partiel_evt_new_pm_status_final`. A noter que les variables sont renommées (i.e lower case, tiret du bas) puis les variables sont triées dans un nouvel ordre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = 'pm_test_filtered'\n",
    "list_var = [\n",
    "\"Code Greffe\",\n",
    "\"Nom_Greffe\",\n",
    "\"Numero_Gestion\",\n",
    "\"Siren\",    \n",
    "\"status\",\n",
    "\"origin\",    \n",
    "\"file_timestamp\",\n",
    "\"Type_Inscription\",\n",
    "\"Date_Immatriculation\",\n",
    "\"Date_1re_Immatriculation\",\n",
    "\"Date_Radiation\",\n",
    "\"Date_Transfert\",\n",
    "\"Sans_Activité\",\n",
    "\"Date_Debut_Activité\",\n",
    "\"Date_Début_1re_Activité\",\n",
    "\"Date_Cessation_Activité\",\n",
    "\"Denomination\",\n",
    "\"Sigle\",\n",
    "\"Forme_Juridique\",\n",
    "\"Associé_Unique\",\n",
    "\"Activité_Principale\",\n",
    "\"Type_Capital\",\n",
    "\"Capital\",\n",
    "\"Capital_Actuel\",\n",
    "\"Devise\",\n",
    "\"Date_Cloture\",\n",
    "\"Date_Cloture_Except\",\n",
    "\"Economie_Sociale_Solidaire\",\n",
    "\"Durée_PM\",\n",
    "\"Date_Greffe\",\n",
    "\"Libelle_Evt\",\n",
    "\"csv_source\"\n",
    "    \n",
    "]\n",
    "\n",
    "query_ = \"\"\"CREATE EXTERNAL TABLE IF NOT EXISTS %s.%s (\"\"\"% (dic_['global']['database'],\n",
    "                                                   table)\n",
    "for x, value in enumerate(list_var):\n",
    "    if  x != len(list_var)-1:\n",
    "        q = \"`{}` string,\".format(value)\n",
    "        query_+=q\n",
    "    else:\n",
    "        q = \"`{}` string\".format(value)\n",
    "        query_+=q\n",
    "        end = \"\"\")\n",
    "     ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
    "    WITH SERDEPROPERTIES (\n",
    "   'separatorChar' = ',',\n",
    "   'quoteChar' = '\"'\n",
    "   )\n",
    "     LOCATION '%s'\n",
    "     TBLPROPERTIES ('has_encrypted_data'='false',\n",
    "              'skip.header.line.count'='1')\"\"\" % (\"s3://calfdata/{}\".format(\n",
    "                                                       \"INPI/sql_output_final_pm\")\n",
    "                                                 )\n",
    "        query_+=end\n",
    "athena.run_query(\n",
    "    query=query_,\n",
    "    database=dic_['global']['database'],\n",
    "    s3_output=dic_['global']['output']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"SELECT \n",
    "\"Code Greffe\",\n",
    "\"Nom_Greffe\",\n",
    "\"Numero_Gestion\",\n",
    "\"Siren\",\n",
    "\n",
    "'status',\n",
    "\"origin\",\n",
    "\n",
    "Coalesce(\n",
    "         try(date_parse(file_timestamp, '%Y-%m-%d')),\n",
    "         try(date_parse(file_timestamp, '%Y-%m-%d %hh:%mm:%ss.SSS')),\n",
    "         try(date_parse(file_timestamp, '%Y-%m-%d %hh:%mm:%ss')),\n",
    "         try(cast(file_timestamp as timestamp))\n",
    "       )  as file_timestamp,\n",
    "\n",
    "Coalesce(\n",
    "         try(date_parse(date_greffe, '%Y-%m-%d')),\n",
    "         try(date_parse(date_greffe, '%Y-%m-%d %hh:%mm:%ss.SSS')),\n",
    "         try(date_parse(date_greffe, '%Y-%m-%d %hh:%mm:%ss')),\n",
    "         try(cast(date_greffe as timestamp))\n",
    "  ) as date_greffe,\n",
    "    \n",
    "\"Libelle_Evt\",  \n",
    "\n",
    "\"Type_Inscription\",\n",
    "\"Date_Immatriculation\",\n",
    "\"Date_1re_Immatriculation\",\n",
    "\"Date_Radiation\",\n",
    "\"Date_Transfert\",\n",
    "\"Sans_Activité\",\n",
    "\"Date_Debut_Activité\",\n",
    "\"Date_Début_1re_Activité\",\n",
    "\"Date_Cessation_Activité\",\n",
    "\"Denomination\",\n",
    "\"Sigle\",\n",
    "\"Forme_Juridique\",\n",
    "\"Associé_Unique\",\n",
    "\"Activité_Principale\",\n",
    "\"Type_Capital\",\n",
    "\"Capital\",\n",
    "\"Capital_Actuel\",\n",
    "\"Devise\",\n",
    "\"Date_Cloture\",\n",
    "\"Date_Cloture_Except\",\n",
    "\"Economie_Sociale_Solidaire\",\n",
    "\"Durée_PM\",\n",
    "\n",
    "\"csv_source\"\n",
    "\n",
    "FROM {}\n",
    "ORDER BY \"Siren\",\"Nom_Greffe\", \"Code Greffe\",\n",
    "      \"Numero_Gestion\",\n",
    "      file_timestamp      \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = 'initial_partiel_evt_new_pm_status_final'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = athena.run_query(\n",
    "    query=query.format(table),\n",
    "    database=dic_['global']['database'],\n",
    "    s3_output=dic_['global']['output']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_['global']['table_final_id']['PM']['combined']  =  output['QueryExecutionId']\n",
    "dic_['global']['table_final_id']['PM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_key = \"{}/{}.csv\".format(dic_['global']['output'],\n",
    "                               dic_['global']['table_final_id']['PM']['combined']\n",
    "                               )\n",
    "destination_key = \"{}/{}.csv\".format(\"INPI/TC_1/02_preparation_donnee/PM\",\n",
    "                                     table\n",
    "                                         )\n",
    "destination_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = s3.copy_object_s3(source_key = source_key,\n",
    "             destination_key = destination_key,\n",
    "             remove = False\n",
    "                      )"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
