{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repliquer calcul cosine en Spark\n",
    "\n",
    "# Objective(s)\n",
    "\n",
    "Repliquer le code du calcul de la distance entre 2 listes de mots en Spark. Plus précisément, faire:\n",
    "* créer 6 variables qui vont permettre a la réalisation des tests test_similarite_exception_words et test_distance_levhenstein_exception_words. Les six variables sont les suivantes:\n",
    "  * unzip_inpi: Mot comparé coté inpi\n",
    "  * unzip_insee: Mot comparé coté insee\n",
    "  * max_cosine_distance: Score de similarité entre le mot compaté coté inpi et coté insee\n",
    "  * levenshtein_distance: Nombre d'édition qu'il faut réaliser pour arriver à reproduire les deux mots\n",
    "  * key_except_to_test: Champs clé-valeur pour toutes les possibiltés des mots qui ne sont pas en communs entre l'insee et l'inp\n",
    "\n",
    "Metadata\n",
    "* Epic: Epic 1\n",
    "* US: US 1\n",
    "* Date Begin: 10/13/2020\n",
    "* Duration Task: 4\n",
    "* Description: Traduire les codes SQL du calcul du Cosine en Spark\n",
    "* Step type:  \n",
    "* Status: Active\n",
    "* Source URL: US 01 Transfert Spark\n",
    "* Task type: Jupyter Notebook\n",
    "* Users: Thomas Pernet\n",
    "* Watchers: Thomas Pernet\n",
    "* User Account: https://937882855452.signin.aws.amazon.com/console\n",
    "* Estimated Log points: 10\n",
    "* Task tag: #computation,#spark\n",
    "* Toggl Tag: #data-preparation\n",
    "\n",
    "# Knowledge\n",
    "\n",
    "## List of candidates\n",
    "* Calcul from scratch de la distance de cosine entre deux listes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connexion serveur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsPy.aws_authorization import aws_connector\n",
    "from awsPy.aws_s3 import service_s3\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os, shutil, json\n",
    "\n",
    "path = os.getcwd()\n",
    "parent_path = str(Path(path).parent)\n",
    "\n",
    "path_cred = r\"{}/credential_AWS.json\".format(parent_path)\n",
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                       region = 'eu-west-3')\n",
    "\n",
    "region = 'eu-west-3'\n",
    "bucket = 'calfdata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                       region = region)\n",
    "client= con.client_boto()\n",
    "s3 = service_s3.connect_S3(client = client,\n",
    "                      bucket = bucket, verbose = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_setting = True\n",
    "if pandas_setting:\n",
    "    cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation tables\n",
    "\n",
    "## Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession \n",
    "    .builder \n",
    "    .appName(\"Python Spark SQL basic example\") \n",
    "    .config('spark.executor.memory', '4G') \n",
    "    .getOrCreate()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>inpi_except</th>\n",
       "      <th>insee_except</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>[RUE, CHARLES, GILLE]</td>\n",
       "      <td>[BOULEVARD, PREUILLY]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>[JB]</td>\n",
       "      <td>[JEAN, BAPTISTE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>[JB]</td>\n",
       "      <td>[JEAN, BAPTISTE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>[MARCELIN, BERTHELOT, CENTRE, D, ENTREPRISES]</td>\n",
       "      <td>[PROSPER, LEGOUTE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>[CHEMIN, BEL, AIR]</td>\n",
       "      <td>[RUE, VICTOR, HUGO]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id                                    inpi_except  \\\n",
       "0       5                          [RUE, CHARLES, GILLE]   \n",
       "1       7                                           [JB]   \n",
       "2       8                                           [JB]   \n",
       "3      10  [MARCELIN, BERTHELOT, CENTRE, D, ENTREPRISES]   \n",
       "4      12                             [CHEMIN, BEL, AIR]   \n",
       "\n",
       "            insee_except  \n",
       "0  [BOULEVARD, PREUILLY]  \n",
       "1       [JEAN, BAPTISTE]  \n",
       "2       [JEAN, BAPTISTE]  \n",
       "3     [PROSPER, LEGOUTE]  \n",
       "4    [RUE, VICTOR, HUGO]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_output = 'SQL_OUTPUT_ATHENA'\n",
    "database = 'ets_siretisation'\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT row_id, inpi_except, insee_except\n",
    "FROM \"ets_siretisation\".\"ets_insee_inpi_statut_cas\"\n",
    "WHERE inpi_except IS NOT NULL AND insee_except IS NOT NULL\n",
    "LIMIT 10\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "output = s3.run_query(\n",
    "            query=query,\n",
    "            database=database,\n",
    "            s3_output=s3_output,\n",
    "  filename = 'test_list_spark', ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'row_id': 5,\n",
       "  'inpi_except': '[RUE, CHARLES, GILLE]',\n",
       "  'insee_except': '[BOULEVARD, PREUILLY]'},\n",
       " {'row_id': 7, 'inpi_except': '[JB]', 'insee_except': '[JEAN, BAPTISTE]'},\n",
       " {'row_id': 8, 'inpi_except': '[JB]', 'insee_except': '[JEAN, BAPTISTE]'},\n",
       " {'row_id': 10,\n",
       "  'inpi_except': '[MARCELIN, BERTHELOT, CENTRE, D, ENTREPRISES]',\n",
       "  'insee_except': '[PROSPER, LEGOUTE]'},\n",
       " {'row_id': 12,\n",
       "  'inpi_except': '[CHEMIN, BEL, AIR]',\n",
       "  'insee_except': '[RUE, VICTOR, HUGO]'},\n",
       " {'row_id': 19,\n",
       "  'inpi_except': '[A, E]',\n",
       "  'insee_except': '[AIME, EUGENIE, ZI, NORD]'},\n",
       " {'row_id': 21, 'inpi_except': '[ST]', 'insee_except': '[SAINT]'},\n",
       " {'row_id': 23,\n",
       "  'inpi_except': '[LOTISSEMENT, N]',\n",
       "  'insee_except': '[BOULEVARD, RAYMOND, POINCARE, PALAIS, ORIENTAL]'},\n",
       " {'row_id': 24,\n",
       "  'inpi_except': '[LOTISSEMENT, N]',\n",
       "  'insee_except': '[PLACE, AMIRAL, ORTOLI]'},\n",
       " {'row_id': 25,\n",
       "  'inpi_except': '[RUE, FURSANNES]',\n",
       "  'insee_except': '[COLLINE]'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = output.to_json(orient=\"records\")\n",
    "parsed = json.loads(result)\n",
    "parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ = []\n",
    "for key, value in enumerate(parsed):\n",
    "    dic = {\n",
    "        'row_id':value['row_id'],\n",
    "        'inpi_except':value['inpi_except'].strip('][').split(', ') ,\n",
    "        'insee_except': value['insee_except'].strip('][').split(', ')\n",
    "    }\n",
    "    list_.append(dic)\n",
    "with open('test_list.json', 'w') as outfile:\n",
    "    json.dump(list_, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recupération premier ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = parsed[0]['row_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PERNETTH\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py:381: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- inpi_except: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- insee_except: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- row_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(list_)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(inpi_except=['RUE', 'CHARLES', 'GILLE'], insee_except=['BOULEVARD', 'PREUILLY'], row_id=5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, ArrayType, StringType, FloatType, MapType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.mllib.linalg import DenseVector, Vectors, VectorUDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_vector_udf = F.udf(lambda x: Vectors.dense(x), VectorUDT())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('words', 'string'), ('list_weights', 'array<float>')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_list = 'word2vec_weights_100_v2.csv'\n",
    "schema  = (\n",
    "    StructType()\n",
    "    .add('words', StringType(),True)\n",
    "    .add(\"list_weigths\", ArrayType(FloatType(), True))\n",
    ")\n",
    "\n",
    "cols = [str(i) for i in range(1, 101)]\n",
    "weights = (spark.read.csv(path_list, header = True)\n",
    "           .select('0',(F.array(cols)).cast(ArrayType(FloatType(), True)).alias('list_weights'))\n",
    "           .withColumnRenamed(\"0\",\"words\")\n",
    "           #.select('words', list_to_vector_udf(\"list_weights\").alias('list_weights'))\n",
    "          )\n",
    "weights.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|    words|        list_weights|\n",
      "+---------+--------------------+\n",
      "|      RUE|[-0.86694837, -0....|\n",
      "|   AVENUE|[-2.206969, -0.84...|\n",
      "|    ROUTE|[0.75751305, -0.2...|\n",
      "|   CHEMIN|[-0.33908606, 0.7...|\n",
      "|        D|[1.1837989, -1.35...|\n",
      "|        L|[-1.9471866, -0.5...|\n",
      "|BOULEVARD|[-1.4547851, 0.14...|\n",
      "|    PLACE|[-0.21757227, 0.1...|\n",
      "|      BIS|[0.76434, -0.1137...|\n",
      "|    SAINT|[0.7002688, -2.22...|\n",
      "|     LIEU|[-0.5478822, 1.27...|\n",
      "|      DIT|[-1.040085, -0.21...|\n",
      "|        A|[-1.2238628, -0.0...|\n",
      "|     ZONE|[-1.985939, -1.25...|\n",
      "|        B|[-0.30842575, 0.0...|\n",
      "|    ALLEE|[-0.8832805, -2.1...|\n",
      "|     JEAN|[-2.4126835, 1.26...|\n",
      "|   CENTRE|[-3.6606867, -4.1...|\n",
      "|RESIDENCE|[-0.3581616, -0.9...|\n",
      "|      BAT|[-1.2430978, 0.25...|\n",
      "+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weights.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function UDF\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=udf#pyspark.sql.functions.udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "slen = F.udf(lambda s: len(s), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+\n",
      "|    words|<lambda>(words)|\n",
      "+---------+---------------+\n",
      "|      RUE|              3|\n",
      "|   AVENUE|              6|\n",
      "|    ROUTE|              5|\n",
      "|   CHEMIN|              6|\n",
      "|        D|              1|\n",
      "|        L|              1|\n",
      "|BOULEVARD|              9|\n",
      "|    PLACE|              5|\n",
      "|      BIS|              3|\n",
      "|    SAINT|              5|\n",
      "|     LIEU|              4|\n",
      "|      DIT|              3|\n",
      "|        A|              1|\n",
      "|     ZONE|              4|\n",
      "|        B|              1|\n",
      "|    ALLEE|              5|\n",
      "|     JEAN|              4|\n",
      "|   CENTRE|              6|\n",
      "|RESIDENCE|              9|\n",
      "|      BAT|              3|\n",
      "+---------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    weights\n",
    "    .select(\n",
    "        'words',\n",
    "        slen('words')\n",
    "    )\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@F.udf(returnType=FloatType())\n",
    "#def dot(x, y):\n",
    "#    return Vectors.dense(x).dot(Vectors.dense(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcul Cosine depuis deux listes en Spark 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme la fonction du cosine est assez simple, il n'y a pas besoin de créer une fonction (et le décorateur). Une fonction lambda est amplement suffisante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine = F.udf(lambda x, y: \n",
    "               (np.dot(x, y)/ (np.linalg.norm(x) * np.linalg.norm(y))).item(),\n",
    "               FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = (\n",
    "    df\n",
    "    .filter(\"row_id = {}\".format(test_id))\n",
    "    .select(\n",
    "        'row_id',\n",
    "        F.expr(\n",
    "        \"\"\"\n",
    "explode(\n",
    "map_from_entries(    \n",
    " arrays_zip(\n",
    "  inpi_except, \n",
    "  transform(\n",
    "    sequence(\n",
    "      1, \n",
    "      size(inpi_except)\n",
    "    ), \n",
    "    x -> insee_except\n",
    "    )\n",
    "    )\n",
    "  )\n",
    ")\n",
    "      \"\"\"\n",
    "                        )\n",
    "         .alias(\"inpi\", \"value\")\n",
    "    )\n",
    "       \n",
    "    .select(\n",
    "        'row_id',\n",
    "        \"inpi\",\n",
    "        F.explode_outer(\"value\")\n",
    "        .alias(\"insee\")\n",
    "   )\n",
    "    .join((weights.withColumnRenamed(\"words\",\"inpi\")),\n",
    "        on = ['inpi'], how = 'left')\n",
    "    .withColumnRenamed(\"list_weights\",\"list_weights_inpi\")\n",
    "    .join((weights.withColumnRenamed(\"words\",\"insee\")),\n",
    "       on = ['insee'], how = 'left')\n",
    "    .withColumnRenamed(\"list_weights\",\"list_weights_insee\")\n",
    "    .select('row_id',\n",
    "            'inpi',\n",
    "            'insee',\n",
    "            \"list_weights_inpi\",\n",
    "            \"list_weights_insee\",\n",
    "            cosine(\"list_weights_inpi\", \"list_weights_insee\").alias(\"cosine\"),\n",
    "           )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.show(truncate = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = test.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcul Cosine depuis deux listes en Spark, version < 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'RUE': ['BOULEVARD', 'PREUILLY']},\n",
       " {'CHARLES': ['BOULEVARD', 'PREUILLY']},\n",
       " {'GILLE': ['BOULEVARD', 'PREUILLY']}]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_a = [\"RUE\", \"CHARLES\", \"GILLE\"]\n",
    "list_b = [\"BOULEVARD\", \"PREUILLY\"]\n",
    "\n",
    "test_list =[dict(zip([i], [list_b])) for i in list_a]\n",
    "test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_except = F.udf(lambda x, y: [dict(zip([i], [y])) for i in x],\n",
    "                   ArrayType(MapType(StringType(), ArrayType(StringType()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine = F.udf(lambda x, y: \n",
    "               (np.dot(x, y)/ (np.linalg.norm(x) * np.linalg.norm(y))).item(),\n",
    "               FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = (\n",
    "    df\n",
    "    .filter(\"row_id = {}\".format(test_id))\n",
    "    .select(\n",
    "        'row_id',\n",
    "        'inpi_except',\n",
    "        'insee_except',\n",
    "        F.explode(zip_except(\"inpi_except\",\"insee_except\")).alias(\"zip_except\")\n",
    "    )\n",
    "    .select(\n",
    "    'row_id',\n",
    "        'inpi_except',\n",
    "        'insee_except',\n",
    "        F.explode(\"zip_except\").alias(\"inpi\", \"value\")\n",
    "    )\n",
    "    .select(\n",
    "    'row_id',\n",
    "        'inpi_except',\n",
    "        'insee_except',\n",
    "        'inpi',\n",
    "         F.explode(\"value\")\n",
    "        .alias(\"insee\")\n",
    "    )\n",
    "    .join((weights.withColumnRenamed(\"words\",\"inpi\")),\n",
    "        on = ['inpi'], how = 'left')\n",
    "    .withColumnRenamed(\"list_weights\",\"list_weights_inpi\")\n",
    "    .join((weights.withColumnRenamed(\"words\",\"insee\")),\n",
    "       on = ['insee'], how = 'left')\n",
    "    .withColumnRenamed(\"list_weights\",\"list_weights_insee\")\n",
    "    .select('row_id',\n",
    "            'inpi',\n",
    "            'insee',\n",
    "            \"list_weights_inpi\",\n",
    "            \"list_weights_insee\",\n",
    "            cosine(\"list_weights_inpi\", \"list_weights_insee\").alias(\"cosine\"),\n",
    "           )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[row_id: bigint, inpi: string, insee: string, list_weights_inpi: array<float>, list_weights_insee: array<float>, cosine: float]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---------+--------------------+--------------------+-----------+\n",
      "|row_id|   inpi|    insee|   list_weights_inpi|  list_weights_insee|     cosine|\n",
      "+------+-------+---------+--------------------+--------------------+-----------+\n",
      "|     5|    RUE|BOULEVARD|[-0.86694837, -0....|[-1.4547851, 0.14...| 0.40306154|\n",
      "|     5|    RUE| PREUILLY|[-0.86694837, -0....|[0.026656773, -0....|0.096528575|\n",
      "|     5|CHARLES|BOULEVARD|[-1.1762805, -0.5...|[-1.4547851, 0.14...| 0.09133629|\n",
      "|     5|CHARLES| PREUILLY|[-1.1762805, -0.5...|[0.026656773, -0....| 0.10189664|\n",
      "|     5|  GILLE|BOULEVARD|[0.34494784, -0.2...|[-1.4547851, 0.14...| 0.03590281|\n",
      "|     5|  GILLE| PREUILLY|[0.34494784, -0.2...|[0.026656773, -0....| 0.22824264|\n",
      "+------+-------+---------+--------------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show(truncate =True)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nteract": {
   "version": "0.24.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
