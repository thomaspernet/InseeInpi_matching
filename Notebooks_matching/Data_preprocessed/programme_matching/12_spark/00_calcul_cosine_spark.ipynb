{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repliquer calcul cosine en Spark\n",
    "\n",
    "# Objective(s)\n",
    "\n",
    "Repliquer le code du calcul de la distance entre 2 listes de mots en Spark. Plus précisément, faire:\n",
    "* créer 6 variables qui vont permettre a la réalisation des tests test_similarite_exception_words et test_distance_levhenstein_exception_words. Les six variables sont les suivantes:\n",
    "  * unzip_inpi: Mot comparé coté inpi\n",
    "  * unzip_insee: Mot comparé coté insee\n",
    "  * max_cosine_distance: Score de similarité entre le mot compaté coté inpi et coté insee\n",
    "  * levenshtein_distance: Nombre d'édition qu'il faut réaliser pour arriver à reproduire les deux mots\n",
    "  * key_except_to_test: Champs clé-valeur pour toutes les possibiltés des mots qui ne sont pas en communs entre l'insee et l'inp\n",
    "\n",
    "Metadata\n",
    "* Epic: Epic 1\n",
    "* US: US 1\n",
    "* Date Begin: 10/13/2020\n",
    "* Duration Task: 4\n",
    "* Description: Traduire les codes SQL du calcul du Cosine en Spark\n",
    "* Step type:  \n",
    "* Status: Active\n",
    "* Source URL: US 01 Transfert Spark\n",
    "* Task type: Jupyter Notebook\n",
    "* Users: Thomas Pernet\n",
    "* Watchers: Thomas Pernet\n",
    "* User Account: https://937882855452.signin.aws.amazon.com/console\n",
    "* Estimated Log points: 10\n",
    "* Task tag: #computation,#spark\n",
    "* Toggl Tag: #data-preparation\n",
    "\n",
    "# Knowledge\n",
    "\n",
    "## List of candidates\n",
    "* Calcul from scratch de la distance de cosine entre deux listes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connexion serveur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsPy.aws_authorization import aws_connector\n",
    "from awsPy.aws_s3 import service_s3\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os, shutil, json\n",
    "\n",
    "path = os.getcwd()\n",
    "parent_path = str(Path(path).parent)\n",
    "\n",
    "path_cred = r\"{}/credential_AWS.json\".format(parent_path)\n",
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                       region = 'eu-west-3')\n",
    "\n",
    "region = 'eu-west-3'\n",
    "bucket = 'calfdata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                       region = region)\n",
    "client= con.client_boto()\n",
    "s3 = service_s3.connect_S3(client = client,\n",
    "                      bucket = bucket, verbose = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_setting = True\n",
    "if pandas_setting:\n",
    "    cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation tables\n",
    "\n",
    "## Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession \n",
    "    .builder \n",
    "    .appName(\"Python Spark SQL basic example\") \n",
    "    .config('spark.executor.memory', '4G') \n",
    "    .getOrCreate()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output = 'SQL_OUTPUT_ATHENA'\n",
    "database = 'ets_siretisation'\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT row_id, inpi_except, insee_except\n",
    "FROM \"ets_siretisation\".\"ets_insee_inpi_statut_cas\"\n",
    "WHERE inpi_except IS NOT NULL AND insee_except IS NOT NULL\n",
    "LIMIT 10\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "output = s3.run_query(\n",
    "            query=query,\n",
    "            database=database,\n",
    "            s3_output=s3_output,\n",
    "  filename = 'test_list_spark', ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_json('test_cosine_inpi_insee.json',orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_cosine_inpi_insee.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ = []\n",
    "for key, value in enumerate(data):\n",
    "    dic = {\n",
    "        'row_id':value['row_id'],\n",
    "        'inpi_except':value['inpi_except'].strip('][').split(', ') ,\n",
    "        'insee_except': value['insee_except'].strip('][').split(', ')\n",
    "    }\n",
    "    list_.append(dic)\n",
    "with open('test_cosine_inpi_insee_clean.json', 'w') as outfile:\n",
    "    json.dump(list_, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recupération premier ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = parsed[0]['row_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(list_)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, ArrayType, StringType, FloatType, MapType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.mllib.linalg import DenseVector, Vectors, VectorUDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_list = 'word2vec_weights_100_v2.csv'\n",
    "schema  = (\n",
    "    StructType()\n",
    "    .add('words', StringType(),True)\n",
    "    .add(\"list_weigths\", ArrayType(FloatType(), True))\n",
    ")\n",
    "\n",
    "cols = [str(i) for i in range(1, 101)]\n",
    "weights = (spark.read.csv(path_list, header = True)\n",
    "           .select('0',(F.array(cols)).cast(ArrayType(FloatType(), True)).alias('list_weights'))\n",
    "           .withColumnRenamed(\"0\",\"words\")\n",
    "           #.select('words', list_to_vector_udf(\"list_weights\").alias('list_weights'))\n",
    "          )\n",
    "weights.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function UDF\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=udf#pyspark.sql.functions.udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slen = F.udf(lambda s: len(s), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    weights\n",
    "    .select(\n",
    "        'words',\n",
    "        slen('words')\n",
    "    )\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@F.udf(returnType=FloatType())\n",
    "#def dot(x, y):\n",
    "#    return Vectors.dense(x).dot(Vectors.dense(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcul Cosine depuis deux listes en Spark 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme la fonction du cosine est assez simple, il n'y a pas besoin de créer une fonction (et le décorateur). Une fonction lambda est amplement suffisante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine = F.udf(lambda x, y: \n",
    "               (np.dot(x, y)/ (np.linalg.norm(x) * np.linalg.norm(y))).item(),\n",
    "               FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = (\n",
    "    df\n",
    "    .filter(\"row_id = {}\".format(test_id))\n",
    "    .select(\n",
    "        'row_id',\n",
    "        F.expr(\n",
    "        \"\"\"\n",
    "explode(\n",
    "map_from_entries(    \n",
    " arrays_zip(\n",
    "  inpi_except, \n",
    "  transform(\n",
    "    sequence(\n",
    "      1, \n",
    "      size(inpi_except)\n",
    "    ), \n",
    "    x -> insee_except\n",
    "    )\n",
    "    )\n",
    "  )\n",
    ")\n",
    "      \"\"\"\n",
    "                        )\n",
    "         .alias(\"inpi\", \"value\")\n",
    "    )\n",
    "       \n",
    "    .select(\n",
    "        'row_id',\n",
    "        \"inpi\",\n",
    "        F.explode_outer(\"value\")\n",
    "        .alias(\"insee\")\n",
    "   )\n",
    "    .join((weights.withColumnRenamed(\"words\",\"inpi\")),\n",
    "        on = ['inpi'], how = 'left')\n",
    "    .withColumnRenamed(\"list_weights\",\"list_weights_inpi\")\n",
    "    .join((weights.withColumnRenamed(\"words\",\"insee\")),\n",
    "       on = ['insee'], how = 'left')\n",
    "    .withColumnRenamed(\"list_weights\",\"list_weights_insee\")\n",
    "    .select('row_id',\n",
    "            'inpi',\n",
    "            'insee',\n",
    "            \"list_weights_inpi\",\n",
    "            \"list_weights_insee\",\n",
    "            cosine(\"list_weights_inpi\", \"list_weights_insee\").alias(\"cosine\"),\n",
    "           )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.show(truncate = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = test.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcul Cosine depuis deux listes en Spark, version < 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_a = [\"RUE\", \"CHARLES\", \"GILLE\"]\n",
    "list_b = [\"BOULEVARD\", \"PREUILLY\"]\n",
    "\n",
    "test_list =[dict(zip([i], [list_b])) for i in list_a]\n",
    "test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_except = F.udf(lambda x, y: [dict(zip([i], [y])) for i in x],\n",
    "                   ArrayType(MapType(StringType(), ArrayType(StringType()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine = F.udf(lambda x, y: \n",
    "               (np.dot(x, y)/ (np.linalg.norm(x) * np.linalg.norm(y))).item(),\n",
    "               FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = (\n",
    "    df\n",
    "    .filter(\"row_id = {}\".format(test_id))\n",
    "    .select(\n",
    "        'row_id',\n",
    "        'inpi_except',\n",
    "        'insee_except',\n",
    "        F.explode(zip_except(\"inpi_except\",\"insee_except\")).alias(\"zip_except\")\n",
    "    )\n",
    "    .select(\n",
    "    'row_id',\n",
    "        'inpi_except',\n",
    "        'insee_except',\n",
    "        F.explode(\"zip_except\").alias(\"inpi\", \"value\")\n",
    "    )\n",
    "    .select(\n",
    "    'row_id',\n",
    "        'inpi_except',\n",
    "        'insee_except',\n",
    "        'inpi',\n",
    "         F.explode(\"value\")\n",
    "        .alias(\"insee\")\n",
    "    )\n",
    "    .join((weights.withColumnRenamed(\"words\",\"inpi\")),\n",
    "        on = ['inpi'], how = 'left')\n",
    "    .withColumnRenamed(\"list_weights\",\"list_weights_inpi\")\n",
    "    .join((weights.withColumnRenamed(\"words\",\"insee\")),\n",
    "       on = ['insee'], how = 'left')\n",
    "    .withColumnRenamed(\"list_weights\",\"list_weights_insee\")\n",
    "    .select('row_id',\n",
    "            'inpi',\n",
    "            'insee',\n",
    "            \"list_weights_inpi\",\n",
    "            \"list_weights_insee\",\n",
    "            cosine(\"list_weights_inpi\", \"list_weights_insee\").alias(\"cosine\"),\n",
    "           )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.show(truncate =True)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nteract": {
   "version": "0.24.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
