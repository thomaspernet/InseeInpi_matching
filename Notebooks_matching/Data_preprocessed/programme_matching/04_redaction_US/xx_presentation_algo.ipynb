{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programme de Matching\n",
    "\n",
    "## Agenda\n",
    "\n",
    "1. Demonstration full pipeline\n",
    "2. Demonstration pas a pas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "os.chdir('../')\n",
    "current_dir = os.getcwd()\n",
    "from inpi_insee import preparation_data\n",
    "import pandas as pd\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation INPI ETS\n",
    "\n",
    "Afin d'accelerer les calcules, nous avons pris un échantillon aléatoire de 100.000 observations depuis la donnée préparée de l'INPI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparation des dossiers destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "files = glob.glob('data/output/InitialPartielEVTNEW/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "try:\n",
    "    os.rmdir('data/output/InitialPartielEVTNEW')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "files = glob.glob('data/input/INPI/special_treatment/InitialPartielEVTNEW/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "try:\n",
    "    os.rmdir('data/input/INPI/special_treatment/InitialPartielEVTNEW')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "files = glob.glob('data/input/SIREN_INPI/InitialPartielEVTNEW/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "try:\n",
    "    os.rmdir('data/input/SIREN_INPI/InitialPartielEVTNEW')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "files = glob.glob('data/input/INSEE/InitialPartielEVTNEW/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "try:\n",
    "    os.rmdir('data/input/INSEE/InitialPartielEVTNEW')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#etb_ex = 'https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/raw'\\\n",
    "#'/master/Notebooks_matching/Data_preprocessed/programme_matching/data/RawData' \\\n",
    "#'/INPI/Stock/initial_partiel_evt_new_ets_status_final_exemple_1.csv'\n",
    "\n",
    "commune = 'https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/raw' \\\n",
    "'/master/Notebooks_matching/Data_preprocessed/programme_matching/data/input' \\\n",
    "'/Parameters/communes_france.csv'\n",
    "\n",
    "voie = 'https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/raw' \\\n",
    "'/master/Notebooks_matching/Data_preprocessed/programme_matching/data/input' \\\n",
    "'/Parameters/voie.csv'\n",
    "\n",
    "stopword ='https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/raw' \\\n",
    "'/master/Notebooks_matching/Data_preprocessed/programme_matching/data/input' \\\n",
    "'/Parameters/upper_stop.csv'\n",
    "\n",
    "test = 'C:\\\\Users\\\\PERNETTH\\\\Documents\\\\Projects\\\\InseeInpi_matching\\\\' \\\n",
    "'Notebooks_matching\\\\Data_preprocessed\\\\programme_matching\\\\data\\\\RawData\\\\INPI\\\\Stock\\\\' \\\n",
    "'inpi_ets_exemple_2.csv'\n",
    "\n",
    "param = {\n",
    "    'communes_insee': commune,\n",
    "    'upper_word':stopword,\n",
    "     \"voie\": voie,\n",
    "    'insee':  \"data/RawData/INSEE/Stock/ETS/StockEtablissement_utf8.csv\",\n",
    "    'inpi_etb': test,\n",
    "    'date_end':\"2020-01-01\"\n",
    "}\n",
    "prep_data = preparation_data.preparation(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La préparation de la donnée exclue tous les sirens qui n'ont pas de valeurs pour l'ensemble des champs de matching. De plus nous allons récupérer à l'INSEE, seulement les SIREN qui nous interessent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "prep_data.normalize_inpi(\n",
    "    origin =['Initial','Partiel','EVT','NEW'],\n",
    "    save_gz = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "path = 'data\\\\input\\\\SIREN_INPI\\\\InitialPartielEVTNEW\\\\' \\\n",
    "'inpi_SIREN_inpi_ets_exemple_2_InitialPartielEVTNEW.csv'\n",
    "\n",
    "prep_data.normalize_insee(\n",
    "   path,\n",
    "    save_gz = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detail siretisation\n",
    "\n",
    "L'algorithme de SIRETISATION fonctionne avec l'aide de trois fonctions:\n",
    "\n",
    "- `step_one`: permet d'écarter les doublons du merge et d'appliquer les premières règles afin de connaitre l'origine de la siretisation\n",
    "- `step_two_assess_test`: détermine l'origine du matching, a savoir la date, adresse, voie, numéro de voie\n",
    "- `step_two_duplication`: permet de récuperer des SIRET sur les doublons émanant du merge avec l'INSEE\n",
    "\n",
    "Dans premier temps, on crée un dictionnaire avec toutes les variables de matching. L'algorithme va utiliser séquentiellement les variables suivantes:\n",
    "\n",
    "```\n",
    " {'ncc', 'Code_Postal', 'Code_Commune', 'INSEE', 'digit_inpi'},\n",
    " {'ncc', 'Code_Postal', 'Code_Commune', 'INSEE'},\n",
    " {'ncc', 'Code_Postal', 'Code_Commune', 'digit_inpi'},\n",
    " {'ncc', 'Code_Postal', 'Code_Commune'},   \n",
    " {'ncc', 'Code_Postal'},\n",
    " {'ncc'},\n",
    " {'Code_Postal'},\n",
    " {'Code_Commune'}\n",
    "```\n",
    "\n",
    "L'algorithme fonctionne de manière séquentielle, et utilise comme input un fichier de l'INPI a siretiser. De fait, après chaque séquence, l'algorithme sauvegarde un fichier gz contenant les siren a trouver. Cette étape de sauvegarde en gz permet de loader le fichier gz en input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.chdir('../')\n",
    "#current_dir = os.getcwd()\n",
    "from inpi_insee import siretisation\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from itertools import compress, product\n",
    "\n",
    "#def combinations(items):\n",
    "#    return ( set(compress(items,mask)) for \n",
    "#            mask in product(*[[0,1]]*len(items)))\n",
    "\n",
    "#all_list = ['ncc',\n",
    "#             'Code_Postal','Code_Commune',\n",
    "#             'INSEE','digit_inpi']\n",
    "#test = list(combinations(items = all_list))[1:]\n",
    "#sort_list = sorted(test[1:], key=lambda k: len(k), reverse=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_inpi = ['ncc','code_postal','code_commune','INSEE','digit_inpi']\n",
    "list_insee = ['libelleCommuneEtablissement',\n",
    "            'codePostalEtablissement', 'codeCommuneEtablissement',\n",
    "            'typeVoieEtablissement','numeroVoieEtablissement']\n",
    "\n",
    "sort_list = [\n",
    " {'ncc', 'code_postal', 'code_commune', 'INSEE', 'digit_inpi'},\n",
    " {'ncc', 'code_postal', 'code_commune', 'INSEE'},\n",
    " {'ncc', 'code_postal', 'code_commune', 'digit_inpi'},\n",
    " {'ncc', 'code_postal', 'code_commune'},   \n",
    " {'ncc', 'code_postal'},\n",
    " {'ncc'},\n",
    " {'code_postal'},\n",
    " {'code_commune'}\n",
    "]\n",
    "len(sort_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_possibilities = []\n",
    "for i in sort_list:\n",
    "    left =[]\n",
    "    right = []\n",
    "    for j in i:\n",
    "        left.append(j)\n",
    "        right.append(list_insee[list_inpi.index(j)])\n",
    "    left.insert(0,'siren')\n",
    "    right.insert(0,'siren')\n",
    "    \n",
    "    dic_ = {\n",
    "    'match':{\n",
    "        'inpi':left,\n",
    "        'insee':right,\n",
    "    }\n",
    "}\n",
    "    list_possibilities.append(dic_)\n",
    "list_possibilities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_possibilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indiquer le fichiers a siretiser. Si pas en local, le télécharger depuis le S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = 'inpi_initial_partiel_evt_new_ets_status_final_test_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from awsPy.aws_authorization import aws_connector\n",
    "#from awsPy.aws_s3 import service_s3\n",
    "#from pathlib import Path\n",
    "#import pandas as pd\n",
    "#bucket = 'calfdata'\n",
    "#path = os.getcwd()\n",
    "#parent_path = str(Path(path).parent)\n",
    "#path_cred = r\"{}/programme_matching/credential_AWS.json\".format(parent_path)\n",
    "#con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "#                                        region = 'eu-west-3')\n",
    "#client= con.client_boto()\n",
    "#s3 = service_s3.connect_S3(client = client,\n",
    "#                      bucket = 'calfdata') \n",
    "#s3.download_file(\n",
    "#    key= 'INPI/TC_1/02_preparation_donnee/Stock/ETB/{}_0.csv'.format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_ets = 'data/input/INPI/{}_{}.csv'.format(filename, key)\n",
    "#df_ets\n",
    "#'data/input/INPI/{}_{}.csv'.format(filename, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import shutil\n",
    "#try:\n",
    "#    os.remove(\"data/input/INPI/{}_0.gz\".format(filename))\n",
    "#except:\n",
    "#    pass\n",
    "#shutil.move(\"{}_0.gz\".format(filename),\n",
    "#            \"data/input/INPI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut prendre l'`origin` et `filename` que l'on souhaite sitetiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "files = glob.glob(os.path.join(current_dir,'data/logs/InitialPartielEVTNEW/*'))\n",
    "for f in files:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(current_dir,'programme_matching','data\\logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = \"InitialPartielEVTNEW\"\n",
    "#filename = \"inpi_initial_partiel_evt_new_ets_status_final_InitialPartielEVTNEW\" ####ETS\n",
    "filename = \"inpi_ets_exemple_2_InitialPartielEVTNEW\"\n",
    "#origin = \"NEW\"\n",
    "#filename = \"inpi_initial_partiel_evt_new_ets_status_final_NEW\"\n",
    "#### make dir\n",
    "parent_dir = os.path.join(current_dir,'data\\output')\n",
    "parent_dir_1 = os.path.join(current_dir,'data\\input\\INPI\\special_treatment')\n",
    "parent_dir_2 = os.path.join(current_dir,'data\\logs')\n",
    "\n",
    "for d in [parent_dir,parent_dir_1,parent_dir_2]:\n",
    "    path = os.path.join(d, origin) \n",
    "    try:\n",
    "        os.mkdir(path) \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'insee': 'data\\\\input\\\\INSEE\\\\InitialPartielEVTNEW\\\\insee_418560_InitialPartielEVTNEW.csv' ### ETS\n",
    "}\n",
    "# 4824158 SIREN a trouver!\n",
    "al_siret = siretisation.siretisation_inpi(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "inpi_col = ['siren',\n",
    "            'index',\n",
    "            'type',\n",
    "            'code_postal',\n",
    "            'ville',\n",
    "            'code_commune',\n",
    "            'pays',\n",
    "            'count_initial_inpi',\n",
    "            'ncc',\n",
    "            'adresse_new_clean_reg',\n",
    "            'adress_new',\n",
    "            'INSEE',\n",
    "            'date_debut_activite',\n",
    "            'digit_inpi',\n",
    "            'len_digit_address_inpi',\n",
    "            'list_digit_inpi'\n",
    "            ]\n",
    "\n",
    "inpi_dtype = {\n",
    "    'siren': 'object',\n",
    "    'index': 'int',\n",
    "    'type': 'object',\n",
    "    'code_postal': 'object',\n",
    "    'ville': 'object',\n",
    "    'code_commune': 'object',\n",
    "    'pays': 'object',\n",
    "    'count_initial_inpi': 'int',\n",
    "    'ncc': 'object',\n",
    "    'adresse_new_clean_reg': 'object',\n",
    "    'adress_new':'object',\n",
    "    'INSEE': 'object',\n",
    "    'date_debut_activite': 'object',\n",
    "    'digit_inpi': 'object',\n",
    "    'len_digit_address_inpi':'object'\n",
    "}\n",
    "\n",
    "\n",
    "### Debut du programme\n",
    "for key, values in enumerate(list_possibilities):\n",
    "    df_ets = 'data\\\\input\\\\INPI\\\\{0}\\\\{1}_{2}.csv'.format(origin, filename, key)\n",
    "\n",
    "    inpi = al_siret.import_dask(file=os.path.join(current_dir,\n",
    "                                                  #'Data_preprocessed\\programme_matching' ,\n",
    "                                                  df_ets),\n",
    "                                usecols=inpi_col,\n",
    "                                dtype=inpi_dtype,\n",
    "                                parse_dates=False)\n",
    "\n",
    "    df_no_duplication, df_duplication = al_siret.step_one(\n",
    "        df_input=inpi,\n",
    "        left_on=values['match']['inpi'],\n",
    "        right_on=values['match']['insee']\n",
    "    )\n",
    "\n",
    "    # Step 2: No duplication\n",
    "    pure_match = al_siret.step_two_assess_test(df=df_no_duplication,\n",
    "                                               var_group=values['match']['inpi'])\n",
    "    \n",
    "    path_pm = os.path.join(current_dir,\n",
    "                 #'Data_preprocessed\\programme_matching',\n",
    "                 'data\\\\output\\\\{0}\\\\{1}_{2}_pure_match.gz'.format(\n",
    "        origin,\n",
    "        key,\n",
    "        filename)\n",
    "                          )\n",
    "\n",
    "    pure_match.to_csv(path_pm,\n",
    "                      compression='gzip', index= False)\n",
    "    # Step 2: duplication\n",
    "    df_not_duplicate, sp = al_siret.step_two_duplication(df_duplication,\n",
    "                                                        var_group = \n",
    "                                                         values['match']['inpi'])\n",
    "                           \n",
    "    path_not_dup = os.path.join(current_dir,\n",
    "                #'Data_preprocessed\\programme_matching',\n",
    "                                'data\\\\output\\\\{0}\\\\{1}_{2}_not_duplicate.gz'.format(\n",
    "            origin,\n",
    "            key,\n",
    "            filename)\n",
    "                               )\n",
    "    \n",
    "    (df_not_duplicate\n",
    "        .to_csv(path_not_dup,\n",
    "                compression='gzip', index= False))\n",
    "    \n",
    "    path_sp = os.path.join(current_dir,\n",
    "                 #'Data_preprocessed\\programme_matching',\n",
    "                           'data\\\\input\\\\INPI\\\\special_treatment\\\\{0}\\\\{1}_{2}_special_treatment.gz'.format(\n",
    "        origin,key, filename)\n",
    "                          )\n",
    "\n",
    "    (sp.to_csv(\n",
    "        path_sp\n",
    "        ,compression='gzip', index= False))\n",
    "\n",
    "    # Input -> Save for the next loop \n",
    "    path_next = os.path.join(current_dir,\n",
    "                # 'Data_preprocessed\\programme_matching',\n",
    "                             'data\\\\input\\\\INPI\\\\{0}\\\\{1}_{2}.csv'.format(\n",
    "        origin,\n",
    "        filename,\n",
    "        key+1)\n",
    "                            )\n",
    "                             \n",
    "    inpi.loc[\n",
    "        (~inpi['index'].isin(pure_match['index'].unique()))\n",
    "        & (~inpi['index'].isin(df_not_duplicate['index'].unique()))\n",
    "        & (~inpi['index'].isin(sp['index'].unique()))\n",
    "    ].compute().to_csv(path_next,\n",
    "                       index= False)\n",
    "\n",
    "    #### Creation LOG\n",
    "    if key ==0:\n",
    "        total_to_siret_intial = inpi.compute().shape[0]\n",
    "        total_siren_initial = inpi.compute()['siren'].nunique()\n",
    "    \n",
    "    ### Total rows in df inpi to match\n",
    "    total_to_siret_current = inpi.compute().shape[0]\n",
    "    total_siren_current = inpi.compute()['siren'].nunique() # unique siren \n",
    "    \n",
    "    ### DF with no duplication after merge INSEE\n",
    "    total_rows_no_dup = df_no_duplication[\"index\"].nunique()\n",
    "    total_rows_no_dup_unique_siren = df_no_duplication['siren'].nunique()\n",
    "    \n",
    "    ### DF with duplication after merge INSEE\n",
    "    total_rows_dup = df_duplication[\"index\"].nunique() # total duplication\n",
    "    total_rows_dup_unique_siren = df_duplication[\"siren\"].nunique()\n",
    "    \n",
    "    total_rows_dup_matched = df_not_duplicate[\"index\"].nunique() #no duplication\n",
    "    total_rows_dup_matched_unique_siren = df_not_duplicate[\"siren\"].nunique()\n",
    "    \n",
    "    total_rows_dup_not_matched = sp[\"index\"].nunique() # special treatmnent\n",
    "    total_rows_dup_not_matched_unique_siren = sp[\"siren\"].nunique()\n",
    "    \n",
    "    ### compare with initial\n",
    "    total_match_rows_current = total_rows_no_dup + total_rows_dup_matched\n",
    "    perc_total_match_rows_initial = total_match_rows_current / \\\n",
    "    total_to_siret_intial\n",
    "    \n",
    "    total_match_siren_current = total_rows_no_dup_unique_siren + \\\n",
    "    total_rows_dup_matched_unique_siren\n",
    "    \n",
    "    perc_total_match_siren_initial = total_match_siren_current / \\\n",
    "    total_siren_initial \n",
    "    \n",
    "    ### compare with current\n",
    "    perc_total_match_rows_current = total_match_rows_current / \\\n",
    "    total_to_siret_current\n",
    "\n",
    "    perc_total_match_siren_current = total_match_siren_current / \\\n",
    "    total_siren_current\n",
    "    \n",
    "    \n",
    "    dic_ = {\n",
    "        'key':key,\n",
    "        'total_to_siret_intial':total_to_siret_intial,\n",
    "        'total_stotal_siren_initialiren': total_siren_initial,\n",
    "        'total_to_siret_current':total_to_siret_current,\n",
    "        'total_siren_current': total_siren_current,\n",
    "        'total_match_rows_current':total_match_rows_current,\n",
    "        'perc_total_match_rows_initial':perc_total_match_rows_initial,\n",
    "        'total_match_siren_current':total_match_siren_current,\n",
    "        'perc_total_match_siren_initial':perc_total_match_siren_initial,\n",
    "        'perc_total_match_rows_current':perc_total_match_rows_current,\n",
    "        'perc_total_match_siren_current':perc_total_match_siren_current,\n",
    "        'df_no_duplication': {\n",
    "            'nb_index': total_rows_no_dup,\n",
    "            'unique_siren':total_rows_no_dup_unique_siren\n",
    "        },\n",
    "        'df_duplication': {\n",
    "            'nb_index': total_rows_dup,\n",
    "            'unique_siren':total_rows_dup_unique_siren,\n",
    "            'df_not_duplicate_index': {\n",
    "                'nb_index':total_rows_dup_matched,\n",
    "               'unique_siren':total_rows_dup_unique_siren\n",
    "            },\n",
    "            'df_sp_index': {\n",
    "                'nb_index':total_rows_dup_not_matched,\n",
    "               'unique_siren':total_rows_dup_not_matched_unique_siren\n",
    "            }\n",
    "        },\n",
    "        'check': total_to_siret_current -\n",
    "        total_rows_no_dup +\n",
    "        total_rows_dup\n",
    "    }\n",
    "\n",
    "    path_log = os.path.join(current_dir,\n",
    "                 #'Data_preprocessed\\programme_matching',\n",
    "                            'data\\\\logs\\\\{0}\\\\{1}_{2}_logs.json'.format(origin,\n",
    "                                                                        key,filename)\n",
    "                           )\n",
    "                            \n",
    "    with open(path_log, 'w') as f:\n",
    "        json.dump(dic_, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruction log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for file in glob.glob(\n",
    "    os.path.join(current_dir,\"data\\logs\\InitialPartielEVTNEW\\*.json\")):\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = pd.json_normalize(data).sort_values(by = 'key')\n",
    "logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs[['total_match_rows_current']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(logs[['perc_total_match_rows_initial']].sum(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(logs[['perc_total_match_siren_initial']].sum(), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "{'ncc', 'Code_Postal', 'Code_Commune', 'INSEE', 'digit_inpi'}, 0\n",
    " {'ncc', 'Code_Postal', 'Code_Commune', 'INSEE'},1\n",
    " {'ncc', 'Code_Postal', 'Code_Commune', 'digit_inpi'}, 2\n",
    " {'ncc', 'Code_Postal', 'Code_Commune'},    3\n",
    " {'ncc', 'Code_Postal'}, 4\n",
    " {'ncc'}, 5\n",
    " {'Code_Postal'}, 6\n",
    " {'Code_Commune'} 7\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs[['key',\n",
    "    'perc_total_match_rows_initial',\n",
    "      'perc_total_match_siren_initial']].set_index('key').plot.bar(stacked=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs[['total_match_rows_current']].plot.bar(stacked=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pas a pas\n",
    "\n",
    "L'algorithme de SIRETISATION fonctionne avec l'aide de trois fonctions:\n",
    "\n",
    "- `step_one`: permet d'écarter les doublons du merge et d'appliquer les premières règles afin de connaitre l'origine de la siretisation\n",
    "- `step_two_assess_test`: détermine l'origine du matching, a savoir la date, adresse, voie, numéro de voie\n",
    "- `step_two_duplication`: permet de récuperer des SIRET sur les doublons émanant du merge avec l'INSEE\n",
    "\n",
    "Dans premier temps, on crée un dictionnaire avec toutes les variables de matching. L'algorithme va utiliser séquentiellement les variables suivantes:\n",
    "\n",
    "```\n",
    " {'ncc', 'Code_Postal', 'Code_Commune', 'INSEE', 'digit_inpi'},\n",
    " {'ncc', 'Code_Postal', 'Code_Commune', 'INSEE'},\n",
    " {'ncc', 'Code_Postal', 'Code_Commune', 'digit_inpi'},\n",
    " {'ncc', 'Code_Postal', 'Code_Commune'},   \n",
    " {'ncc', 'Code_Postal'},\n",
    " {'ncc'},\n",
    " {'Code_Postal'},\n",
    " {'Code_Commune'}\n",
    "```\n",
    "\n",
    "**Vue d'ensemble du programme**\n",
    "\n",
    "![](https://www.lucidchart.com/publicSegments/view/5a8cb28f-dc42-4708-babd-423962514878/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step One\n",
    "\n",
    "La première étape de la séquence est l'ingestion d'un fichier gz contenant les SIREN a trouver. L'ingestion va se faire en convertissant le dataframe en Dask. L'algorithme tout d'abord utiliser la fonction `step_one` et produire deux dataframes selon si le matching avec l'INSEE a débouté sur des doublons ou non.\n",
    "\n",
    "Les doublons sont générés si pour un même nombre de variables de matching, il existe plusieurs possibilités à l'INSEE. Par exemple, pour un siren, ville, adressse donnée, il y a plusieurs possibilités. Cela constitue un doublon et il sera traité ultérieurement, dans la mesure du possible.\n",
    "\n",
    "Les étapes déroulées lors du premier processus est le suivant:\n",
    "\n",
    "```\n",
    "- Test 1: doublon\n",
    "        - non: Save-> `test_1['not_duplication']`\n",
    "        - oui:\n",
    "            - Test 2: Date equal\n",
    "                - oui:\n",
    "                    - Test 2 bis: doublon\n",
    "                        - non: Save-> `test_2_bis['not_duplication']`\n",
    "                        - oui: Save-> `test_2_bis['duplication']`\n",
    "                - non:\n",
    "                    - Test 3: Date sup\n",
    "                        - oui:\n",
    "                            - Test 2 bis: doublon\n",
    "                                - non: Save-> `test_3_oui_bis['not_duplication']`\n",
    "                                - oui: Save-> `test_3_oui_bis['duplication']`\n",
    "                        - non: Save-> `test_3_non`\n",
    "```\n",
    "\n",
    "Deux dataframe sont créés, un ne contenant pas de doublon et un deuxième contenant les doublons. L'algorithme va réaliser les tests sur le premier et faire d'avantage de recherche sur le second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ets = 'data\\\\input\\\\INPI\\\\{0}\\\\{1}_{2}.csv'.format(origin, filename, 0)\n",
    "\n",
    "inpi = al_siret.import_dask(file=os.path.join(current_dir,\n",
    "                                                  #'Data_preprocessed\\programme_matching' ,\n",
    "                                                  df_ets),\n",
    "                                usecols=inpi_col,\n",
    "                                dtype=inpi_dtype,\n",
    "                                parse_dates=False)\n",
    "inpi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation de deux dataframe:\n",
    "\n",
    "1. df_no_duplication: Pure match\n",
    "2. df_duplication: Doublon\n",
    "\n",
    "### Partie  1 fonction: [Merge](https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/inpi_insee/siretisation.py#L240)\n",
    "\n",
    "```\n",
    " temp = df_input.merge(insee,\n",
    "                          how='left',\n",
    "                          left_on=left_on,\n",
    "                          right_on= right_on,\n",
    "                          indicator=True,\n",
    "                          suffixes=['_insee', '_inpi'])\n",
    "\n",
    "#### Recupere les merges\n",
    "to_check = temp[temp['_merge'].isin(['both'])].drop(columns= '_merge')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_possibilities[0]['match']['inpi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_possibilities[0]['match']['insee']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, re\n",
    "from dask.diagnostics import ProgressBar\n",
    "from dask.multiprocessing import get\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "pbar = ProgressBar()\n",
    "pbar.register()\n",
    "\n",
    "insee_col = ['siren',\n",
    "         'siret',\n",
    "         'dateCreationEtablissement',\n",
    "         \"etablissementSiege\",\n",
    "         \"etatAdministratifEtablissement\",\n",
    "         'complementAdresseEtablissement',\n",
    "         'numeroVoieEtablissement',\n",
    "         'indiceRepetitionEtablissement',\n",
    "         'typeVoieEtablissement',\n",
    "         'libelleVoieEtablissement',\n",
    "         'codePostalEtablissement',\n",
    "         'libelleCommuneEtablissement',\n",
    "         'libelleCommuneEtrangerEtablissement',\n",
    "         'distributionSpecialeEtablissement',\n",
    "         'codeCommuneEtablissement',\n",
    "         'codeCedexEtablissement',\n",
    "         'libelleCedexEtablissement',\n",
    "         'codePaysEtrangerEtablissement',\n",
    "         'libellePaysEtrangerEtablissement',\n",
    "         'count_initial_insee','len_digit_address_insee','list_digit_insee']\n",
    "\n",
    "insee_dtype = {\n",
    "             'siren': 'object',\n",
    "             'siret': 'object',\n",
    "             \"etablissementSiege\": \"object\",\n",
    "             \"etatAdministratifEtablissement\": \"object\",\n",
    "             #'dateCreationEtablissement': 'object',\n",
    "             'complementAdresseEtablissement': 'object',\n",
    "             'numeroVoieEtablissement': 'object',\n",
    "             'indiceRepetitionEtablissement': 'object',\n",
    "             'typeVoieEtablissement': 'object',\n",
    "             'libelleVoieEtablissement': 'object',\n",
    "             'codePostalEtablissement': 'object',\n",
    "             'libelleCommuneEtablissement': 'object',\n",
    "             'libelleCommuneEtrangerEtablissement': 'object',\n",
    "             'distributionSpecialeEtablissement': 'object',\n",
    "             'codeCommuneEtablissement': 'object',\n",
    "             'codeCedexEtablissement': 'object',\n",
    "             'libelleCedexEtablissement': 'object',\n",
    "             'codePaysEtrangerEtablissement': 'object',\n",
    "             'libellePaysEtrangerEtablissement': 'object',\n",
    "             'count_initial_insee': 'int',\n",
    "             'len_digit_address_insee':'object'\n",
    "         }\n",
    "\n",
    "\n",
    "insee = dd.read_csv(param['insee'], usecols = insee_col, dtype = insee_dtype,\n",
    "        blocksize=None, low_memory = True,\n",
    "        parse_dates = ['dateCreationEtablissement'])\n",
    "insee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = inpi.merge(insee,\n",
    "                          how='left',\n",
    "                          left_on=list_possibilities[0]['match']['inpi'],\n",
    "                          right_on= list_possibilities[0]['match']['insee'],\n",
    "                          indicator=True,\n",
    "                          suffixes=['_insee', '_inpi'])\n",
    "to_check = temp[temp['_merge'].isin(['both'])].drop(columns= '_merge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "temp.compute().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partie 2 fonction: Doublons vs non doublons\n",
    "\n",
    "On test les doublons via la fonction [`split_duplication`](https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/inpi_insee/siretisation.py#L109)\n",
    "\n",
    "Split un dataframe si l'index (la variable, pas l'index) contient des doublons.\n",
    "\n",
    "L'idée est de distinguer les doublons resultants du merge avec l'INSEE\n",
    "\n",
    "\n",
    "```\n",
    "test_1 = self.split_duplication(df = to_check)\n",
    "        # Test 1: doublon -> non\n",
    "        test_1['not_duplication'] = test_1['not_duplication'].assign(\n",
    "        origin_test = 'test_1_no_duplication'\n",
    "        )\n",
    "\n",
    "```\n",
    "\n",
    "Returns:\n",
    "        - Un Dictionary avec:\n",
    "            - not_duplication: Dataframe ne contenant pas les doublons\n",
    "            - duplication: Dataframe contenant les doublons\n",
    "            - report_dup: Une Serie avec le nombres de doublon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_check.compute().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Solution temporaire\n",
    "to_check[\"date_debut_activite\"] = \\\n",
    "        to_check[\"date_debut_activite\"].map_partitions(\n",
    "        pd.to_datetime,\n",
    "        format='%Y/%m/%d',\n",
    "        errors = 'coerce',\n",
    "        meta = ('datetime64[ns]')\n",
    "        )\n",
    "\n",
    "test_1 = al_siret.split_duplication(df = to_check)\n",
    "        # Test 1: doublon -> non\n",
    "test_1['not_duplication'] = test_1['not_duplication'].assign(\n",
    "        origin_test = 'test_1_no_duplication'\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pas de doublon lors de la première passe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1['not_duplication'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nombre de doublons nécéssitants des tests, avec le nombre de siren concerné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1['duplication'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre siren\n",
    "test_1['report_dup'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partie 3 fonction: premiers tests\n",
    "\n",
    "On applique la première règle qui est de savoir si la date de création est égale à l'INSEE et à l'INPI.\n",
    "\n",
    "Si c'est égale, on applique de nouveau la fonction des doublons.\n",
    "\n",
    "Pour les doublons restants, on test un nouvelle règle, a savoir si la date de l'INSEE est supérieure à L'INPI. En effet, l'INPI est la première informée des dates de créations car c'est elle qui les réalise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test 2: Date equal -> oui\n",
    "test_2_oui = test_1['duplication'][\n",
    "        (test_1['duplication']['date_debut_activite'] ==\n",
    "                     test_1['duplication']['dateCreationEtablissement'])\n",
    "                     ]\n",
    "### Test 2: Date equal -> oui, Test 2 bis: doublon\n",
    "test_2_bis = al_siret.split_duplication(df = test_2_oui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2_bis['not_duplication'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2_bis['duplication'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2_bis['report_dup'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ets = 'data\\\\input\\\\INPI\\\\{0}\\\\{1}_{2}.csv'.format(origin, filename, 0)\n",
    "\n",
    "inpi = al_siret.import_dask(file=os.path.join(current_dir,\n",
    "                                                  #'Data_preprocessed\\programme_matching' ,\n",
    "                                                  df_ets),\n",
    "                                usecols=inpi_col,\n",
    "                                dtype=inpi_dtype,\n",
    "                                parse_dates=False)\n",
    "\n",
    "df_no_duplication, df_duplication = al_siret.step_one(\n",
    "        df_input=inpi,\n",
    "        left_on=list_possibilities[0]['match']['inpi'],\n",
    "        right_on=list_possibilities[0]['match']['insee']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "365 lignes récupérées sur les 1556"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 365 observations récupérées\n",
    "60275-59910"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_duplication.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l'objectif plus tard est de récuperer les informations des 357 observations restantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_duplication.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a des observations qui ne statisfont pas les règles de gestion et sont écarté, a savoir si la date de l'INSEE est supérieure à l'INPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conformité des règles: [step_two_assess_test](https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/inpi_insee/siretisation.py#L331)\n",
    "\n",
    "Le premier dataframe ne contient pas de doublon, il est donc possible de réaliser différents tests afin de mieux déterminer l'origine du matching. Plus précisement, si le matching a pu se faire sur la date, l'adresse, la voie, numéro de voie et le nombre unique d'index. Les règles sont définies ci-dessous.\n",
    "\n",
    "```\n",
    "- Test 1: address libelle\n",
    "            - Si mots dans inpi est contenu dans INSEE, True\n",
    "        - Test 1 bis: address complement\n",
    "            - Si mots dans inpi est contenu dans INSEE, True\n",
    "        - Test 2: Date\n",
    "            - dateCreationEtablissement >= Date_Début_Activité OR\n",
    "            Date_Début_Activité = NaN OR (nombre SIREN a l'INSEE = 1 AND nombre\n",
    "            SIREN des variables de matching = 1), True\n",
    "        - Test 3: siege\n",
    "            - Type = ['SEP', 'SIE'] AND siege = true, True\n",
    "        - Test 4: voie\n",
    "            - Type voie INPI = Type voie INSEE, True\n",
    "        - Test 5: numero voie\n",
    "            - Numero voie INPI = Numero voie INSEE, True\n",
    "```\n",
    "\n",
    "Un premier fichier csv est enregistré contenant les \"pure matches\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Calculer nombre de SIREN\n",
    "\n",
    "On utilise cette variable pour le test de la date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calcul nb siren/siret\n",
    "df_ = (df_no_duplication\n",
    "        .merge(\n",
    "        (df_no_duplication\n",
    "        .groupby(list_possibilities[0]['match']['inpi'])['siren']\n",
    "             .count()\n",
    "             .rename('count_siren_siret')\n",
    "             .reset_index()\n",
    "             ),how = 'left'\n",
    "             )\n",
    "             )\n",
    "df_.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Verification adresse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test 1: address\n",
    "df_ = dd.from_pandas(df_, npartitions=10)\n",
    "df_['test_address_libelle'] = df_.map_partitions(\n",
    "            lambda df:\n",
    "                df.apply(lambda x:\n",
    "                    al_siret.find_regex(\n",
    "                     x['adresse_new_clean_reg'],\n",
    "                     x['libelleVoieEtablissement']), axis=1)\n",
    "                     ).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_['test_address_libelle'].value_counts().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'autres tests sont réalisés. \n",
    "\n",
    "- Test 1 bis: address complement\n",
    "            - Si mots dans inpi est contenu dans INSEE, True\n",
    "        - Test 2: Date\n",
    "            - dateCreationEtablissement >= Date_Début_Activité OR\n",
    "            Date_Début_Activité = NaN OR (nombre SIREN a l'INSEE = 1 AND nombre\n",
    "            SIREN des variables de matching = 1), True\n",
    "        - Test 3: siege\n",
    "            - Type = ['SEP', 'SIE'] AND siege = true, True\n",
    "        - Test 4: voie\n",
    "            - Type voie INPI = Type voie INSEE, True\n",
    "        - Test 5: numero voie\n",
    "            - Numero voie INPI = Numero voie INSEE, True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pure_match = al_siret.step_two_assess_test(df=df_no_duplication,\n",
    "                                               var_group=values['match']['inpi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ = ['test_address_libelle', 'test_address_complement',\n",
    "        'test_join_address', 'test_date', 'test_siege',\n",
    "         'test_voie', 'test_numero']\n",
    "\n",
    "for i in test_:\n",
    "    print('Test: {0}:\\n {1}'.format(i,pure_match[i].value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step_two_duplication\n",
    "\n",
    "Les seconds dataframes contiennent les doublons obtenus après le matching avec l'INSEE. L'algorithme va travailler sur différentes variables de manière séquentielle pour tenter de trouver le bons SIRET. Plus précisément, 3 variables qui ont été récemment créées sont utilisées:\n",
    "\n",
    "- test_join_address -> True si la variable test_address_libelle = True (ie mot INPI trouvé dans INSEE) et test_join_address =  True\n",
    "- test_address_libelle ->  True si la variable test_address_libelle = True (ie mot INPI trouvé dans INSEE)\n",
    "- test_address_complement -> True si la variable test_join_address =  True\n",
    "\n",
    "Pour chaque séquence, on réalise les tests suivants:\n",
    "\n",
    "```\n",
    "- Si test_join_address = True:\n",
    "        - Test 1: doublon:\n",
    "            - Oui: append-> `df_not_duplicate`\n",
    "            - Non: Pass\n",
    "            - Exclue les `index` de df_duplication\n",
    "            - then go next\n",
    "        - Si test_address_libelle = True:\n",
    "            - Test 1: doublon:\n",
    "                - Oui: append-> `df_not_duplicate`\n",
    "                - Non: Pass\n",
    "                - Exclue les `index` de df_duplication\n",
    "                - then go next\n",
    "        - Si test_address_complement = True:\n",
    "            - Test 1: doublon:\n",
    "                - Oui: append-> `df_not_duplicate`\n",
    "                - Non: Pass\n",
    "                - Exclue les `index` de df_duplication\n",
    "```\n",
    "\n",
    "Dernière étape de l'algorithme permettant de récuperer des SIRET sur les\n",
    "        doublons émanant du merge avec l'INSEE. Cette étape va utliser l'étape\n",
    "        précédante, a savoir les variables 'test_join_address',\n",
    "        'test_address_libelle', 'test_address_complement'. Le résultat du test\n",
    "        distingue 2 différents dataframe. Un premier pour les doublons\n",
    "        fraichement siretisés, un deuxième contenant des SIREN qui feront\n",
    "        l'objet d'un traitement spécial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partie 1: Realisation des tests sur les doublons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_duplication.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_ = al_siret.step_two_assess_test(df = df_duplication,\n",
    "        var_group=list_possibilities[0]['match']['inpi'])\n",
    "\n",
    "df_not_duplicate = pd.DataFrame()\n",
    "copy_duplicate = duplicates_.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partie 2: Tests sur 'test_join_address'\n",
    "\n",
    "Le test join adresse:\n",
    "\n",
    "- Si regex trouvé dans libelleVoieEtablissement et complementAdresseEtablissement, alors True (réalisé a l'étape précédente)\n",
    "\n",
    "On va donc récuperer les doublons pour lequel le test join adresse est TRUE et on applique la fonction de séparation des doublons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1 = al_siret.split_duplication(\n",
    "            copy_duplicate[\n",
    "            copy_duplicate['test_join_address'].isin([True])]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1['not_duplication'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1['duplication'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On récupere les siret trouvés (ie sans les doublons), et on les ajoute au dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### append unique\n",
    "df_not_duplicate = (\n",
    "            df_not_duplicate\n",
    "            .append(test_1['not_duplication']\n",
    "            .assign(test = 'test_join_address')\n",
    "            )\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On exclue du dataframe `copy_duplicate` les valeurs trouvées et non trouvées en prenant le soin d'enlever les doublons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_duplicate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([\n",
    "                           test_1['duplication'],\n",
    "                           test_1['not_duplication']\n",
    "                       ], axis = 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_duplicate.shape[0]- (test_1['duplication'].shape[0] +  test_1['not_duplication'].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifier pourquoi on enlève les duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(copy_duplicate\n",
    "                   .loc[~copy_duplicate['index'].isin(\n",
    "                       pd.concat([\n",
    "                           test_1['duplication'],\n",
    "                           test_1['not_duplication']\n",
    "                       ], axis = 0)['index']\n",
    "                       .drop_duplicates())]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_not_duplicate, sp = al_siret.step_two_duplication(df_duplication,\n",
    "                                                        var_group = \n",
    "                                                         list_possibilities[0]['match']['inpi']\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_not_duplicate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.shape"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
