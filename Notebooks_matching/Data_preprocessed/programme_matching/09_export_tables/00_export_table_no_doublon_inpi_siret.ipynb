{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export CSV table INPI siretisee\n",
    "\n",
    "* The ID is kxb88sjjt94211q\n",
    "\n",
    "## Objective(s)\n",
    "\n",
    "*  Dans le but de partager de la travail de siretisation, nous devons exporter la table ets_inpi_insee_no_duplicate \n",
    "* La table contient partiellement des index_id avec des doublons. Cela est du a une mauvaise préparation de la donnée ou bien a des index impossibles en l’état a dédoublonner. Dès lors, il faut retirer ses index de la table a exporter\n",
    "* Nous allons créer une table finale qui contient les informations des données brutes de l’INPI et les données transformées. Pour cela, il faut utiliser la table ets_inpi_sql \n",
    "  * Il faudra ensuite exporter en csv la table complète et une seconde table avec uniquement le siren, siren, ID séquence et variables référentielles de l’établissement au sens de l’INPI.\n",
    "  * Les deux CSV seront disponibles [calfdata/TEMP_PARTAGE_DATA_INPI](https://s3.console.aws.amazon.com/s3/buckets/calfdata/TEMP_PARTAGE_DATA_INPI/?region=eu-west-3&tab=overview)\n",
    "* Please, update the Source URL by clicking on the button after the information have been pasted\n",
    "  * US 01 CSV INPI Modify rows\n",
    "  * Delete tables and Github related to the US: Delete rows\n",
    "  \n",
    "## Metadata\n",
    "\n",
    "* Epic: Epic 5\n",
    "* US: US 1\n",
    "* Date Begin: 9/14/2020\n",
    "* Duration Task: 1\n",
    "* Description: Export de la base INPI siretisee sans les doublons\n",
    "* Status: Active\n",
    "  * Change Status task: Active\n",
    "  * Update table: Modify rows\n",
    "* Source URL: US 01 CSV INPI\n",
    "* Task type: Jupyter Notebook\n",
    "* Users: Thomas Pernet\n",
    "* Watchers: Thomas Pernet\n",
    "* User Account: https://937882855452.signin.aws.amazon.com/console\n",
    "* Estimated Log points: 5\n",
    "* Task tag: #s3,#export-csv,#siretisation,#inpi\n",
    "* Toggl Tag: #share-result\n",
    "\n",
    "## Input Cloud Storage [AWS/GCP]\n",
    "\n",
    "If link from the internet, save it to the cloud first\n",
    "Table/file\n",
    "\n",
    "* Origin: \n",
    "    * Athena\n",
    "* Name: \n",
    "    * ets_insee_inpi_no_duplicate\n",
    "    * ets_inpi_sql\n",
    "* Github: \n",
    "    * https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/08_US_DATUM/11_creation_table_ets_insee_inpi_no_duplicate.md\n",
    "    * https://github.com/thomaspernet/InseeInpi_matching/blob/master/01_Data_preprocessing/Data_preprocessed/programme_matching/01_preparation/05_nettoyage_enseigne_inpi.md\n",
    "\n",
    "## Destination Output/Delivery\n",
    "\n",
    "Table/file\n",
    "\n",
    "* Origin: \n",
    "    * Athena\n",
    "* Name:\n",
    "    * ets_inpi_no_doublon_siret\n",
    "* GitHub:\n",
    " * https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Notebooks_matching/Data_preprocessed/programme_matching/09_export_tables/00_export_table_no_doublon_inpi_siret.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connexion serveur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsPy.aws_authorization import aws_connector\n",
    "from awsPy.aws_athena import service_athena\n",
    "from awsPy.aws_s3 import service_s3\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os, shutil\n",
    "\n",
    "path = os.getcwd()\n",
    "parent_path = str(Path(path).parent)\n",
    "path_cred = r\"{}/credential_AWS.json\".format(parent_path)\n",
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                       region = 'eu-west-3')\n",
    "\n",
    "region = 'eu-west-3'\n",
    "bucket = 'calfdata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'calfdata'\n",
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                       region = region)\n",
    "client= con.client_boto()\n",
    "s3 = service_s3.connect_S3(client = client,\n",
    "                      bucket = bucket, verbose = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_setting = True\n",
    "if pandas_setting:\n",
    "    cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output = 'inpi/sql_output'\n",
    "database = 'siretisation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief analysis\n",
    "\n",
    "Il y a certains `index_id` qui peuvent avoir des doublons après avoir merger avec la table `ets_inpi_sql` car la date de transmission est la même (ie le timestamp) Lors de nos développements, nous n'avons pas envisagé ce cas de figure, toutefois lors de la mise en production, cet aspect a été pris en compte.\n",
    "\n",
    "Dans la query si dessous, nous allons imprimer les lignes ayant des doublons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH merge_inpi AS (\n",
    "  SELECT \n",
    "    ROW_NUMBER() OVER (PARTITION BY ets_insee_inpi_no_duplicate.index_id ORDER BY file_timestamp) AS row_id_group,\n",
    "    ets_insee_inpi_no_duplicate.index_id, \n",
    "    ets_insee_inpi_no_duplicate.siren, \n",
    "    ets_insee_inpi_no_duplicate.siret, \n",
    "    ets_insee_inpi_no_duplicate.sequence_id\n",
    "  FROM \n",
    "    siretisation.ets_insee_inpi_no_duplicate \n",
    "    INNER JOIN siretisation.ets_inpi_sql ON ets_insee_inpi_no_duplicate.index_id = siretisation.ets_inpi_sql.index_id \n",
    "  WHERE \n",
    "    count_index = 1\n",
    ") \n",
    "SELECT \n",
    "\n",
    "  index_id, COUNT(*) AS cnt\n",
    "  \n",
    "FROM \n",
    "  merge_inpi \n",
    "GROUP BY  index_id\n",
    "ORDER BY cnt DESC\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database=database,\n",
    "            s3_output=s3_output,\n",
    "  filename = \"nb_index\", ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation tables\n",
    "\n",
    "Nous avons constaté dans avec la query précédente qu'il y avait 9 lignes ayant des timestamps de transmission identique. Pour ne pas avoir de doublons lors de la création de la table `ets_inpi_no_doublon_siret`, nous décidons de ne récupérer la première ligne. Ce n'est pas optimal comme solution!\n",
    "\n",
    "```\n",
    "{\n",
    "\t\"StorageDescriptor\": {\n",
    "\t\t\"cols\": {\n",
    "\t\t\t\"FieldSchema\": [\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"row_id_group\",\n",
    "\t\t\t\t\t\"type\": \"bigint\",\n",
    "\t\t\t\t\t\"comment\": \"Nombre de lignes par index_id. Normalement que des 1\",\n",
    "                    \"Notebook origin\": \"https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/09_export_tables/00_export_table_no_doublon_inpi_siret.md#steps\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"index_id\",\n",
    "\t\t\t\t\t\"type\": \"bigint\",\n",
    "\t\t\t\t\t\"comment\": \"Identification lignes de l'INPI. Création du numéro de ligne\",\n",
    "                    \"Notebook_origin\":\"https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/01_Athena_concatenate_ETS.md#create-id-and-id-sequence\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"siren\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"siret\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"sequence_id\",\n",
    "\t\t\t\t\t\"type\": \"bigint\",\n",
    "\t\t\t\t\t\"comment\": \"Numéro d'identification du groupe Siren, nom greffe, code greffe, id etablissement\",\n",
    "                     \"Notebook_origin\":\"https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/01_Athena_concatenate_ETS.md#create-id-and-id-sequence\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"code_greffe\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"nom_greffe\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"numero_gestion\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"id_etablissement\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"status\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \" indique si les lignes sont a ignorer ou non car fait référence à une ligne provenant d'un partiel\",\n",
    "                    \"Notebook_origin\":\"https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/01_Athena_concatenate_ETS.md#test-de-v%C3%A9rification-1\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"origin\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"Indique si le csv provient de la branche stock ou la branche événement\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"date_greffe\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"file_timestamp\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"Extraction de la date de transmission inscrite dans le csv\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"libelle_evt\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"last_libele_evt\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"Extraction du dernier libellé de l'événement connu pour une séquence, et appliquer cette information à l'ensemble de la séquence\",\n",
    "                    \"Notebook_origin\":\"https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/03_ETS_add_variables.md#etape-4-cr%C3%A9ation-last_libele_evt-status_admin-status_ets\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"status_admin\",\n",
    "\t\t\t\t\t\"type\": \"varchar(1)\",\n",
    "\t\t\t\t\t\"comment\": \"Indique si l'établissement est ouvert ou fermée. La ligne impacte toute l'historique\",\n",
    "                    \"Notebook_origin\":\"https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/03_ETS_add_variables.md#etapes\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"type\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"status_ets\",\n",
    "\t\t\t\t\t\"type\": \"varchar(5)\",\n",
    "\t\t\t\t\t\"comment\": \"Informe du type d'établissement (SIE/PRI.SEC) concernant une séquence\",\n",
    "                    \"Notebook_origin\":\"https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/03_ETS_add_variables.md#etape-4-cr%C3%A9ation-last_libele_evt-status_admin-status_ets\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"siège_pm\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"rcs_registre\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"adresse_ligne1\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"adresse_ligne2\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"adresse_ligne3\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"adresse_reconstituee_inpi\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"Concatenation des champs de l'adresse et suppression des espace\",\n",
    "                    \"notebook_origin\":\"https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/03_ETS_add_variables.md#adress_reconsitituee_inpi\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"adresse_distance_inpi\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"Concatenation des champs de l'adresse, suppression des espaces et des articles. Utilisé pour calculer le score permettant de distinguer la similarité/dissimilarité entre deux adresses (INPI vs INSEE)\"\n",
    "                    \"notebook_origin\":\"https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/03_ETS_add_variables.md#adresse_distance_inpi\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"list_numero_voie_matching_inpi\",\n",
    "\t\t\t\t\t\"type\": \"array<string>\",\n",
    "\t\t\t\t\t\"comment\": \"Ensemble des numéros compris dans les champs de l'adresse\",\n",
    "    \"notebook_origin\":\"https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/03_ETS_add_variables.md#etapes\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"numero_voie_matching\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"Extraction du premier numéro de voie dans l'adresse. Deprecated\",\n",
    "    \"notebook_origin\":\"https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/03_ETS_add_variables.md#numero_voie_matching\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"voie_clean\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"Extraction du type de voie contenu dans l’adresse. Variable pivot servant à reprendre l’abrevation du type de voie comme à l’INSEE\",\n",
    "    \"notebook_origin\":\"https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/03_ETS_add_variables.md#voie_matching\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"type_voie_matching\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"Extration du type de voie dans l'adresse et match avec abbrévation type de voie de l'INSEE\",\n",
    "    \"notebook_origin\":\"https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/03_ETS_add_variables.md#voie_matching\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"code_postal\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"code_postal_matching\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"Reconstruit le code postal si le champs code postal est nul. La reconstitution se fait si le champs ville contient exactement 5 digit\",\n",
    "    \"notebook_origin\":\"https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/01_Athena_concatenate_ETS.md#filtrer-les-dates-de-greffe\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"ville\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"ville_matching\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"Nettoyage regex de la ville et suppression des espaces. La même logique de nettoyage est appliquée coté INSEE\",\n",
    "    \"notebook_origin\":\"https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/03_ETS_add_variables.md#etape-1-pr%C3%A9paration-ville_matching\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"code_commune\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"pays\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"domiciliataire_nom\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"domiciliataire_siren\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"domiciliataire_greffe\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"domiciliataire_complément\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"siege_domicile_représentant\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"nom_commercial\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"enseigne\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"activité_ambulante\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"activité_saisonnière\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"activité_non_sédentaire\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"date_début_activité\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"activité\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"origine_fonds\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"origine_fonds_info\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"type_exploitation\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"name\": \"csv_source\",\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t}\n",
    "\t\t\t]\n",
    "\t\t},\n",
    "\t\t\"location\": \"s3://calfdata/inpi/sql_output/tables/bf7473f3-4aab-4389-abed-ccc92e1d42ec/\",\n",
    "\t\t\"inputFormat\": \"org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\",\n",
    "\t\t\"outputFormat\": \"org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\",\n",
    "\t\t\"compressed\": \"false\",\n",
    "\t\t\"numBuckets\": \"0\",\n",
    "\t\t\"SerDeInfo\": {\n",
    "\t\t\t\"name\": \"ets_inpi_no_doublon_siret\",\n",
    "\t\t\t\"serializationLib\": \"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\",\n",
    "\t\t\t\"parameters\": {}\n",
    "\t\t},\n",
    "\t\t\"bucketCols\": [],\n",
    "\t\t\"sortCols\": [],\n",
    "\t\t\"parameters\": {},\n",
    "\t\t\"SkewedInfo\": {},\n",
    "\t\t\"storedAsSubDirectories\": \"false\"\n",
    "\t},\n",
    "\t\"parameters\": {\n",
    "\t\t\"EXTERNAL\": \"TRUE\",\n",
    "\t\t\"has_encrypted_data\": \"false\"\n",
    "\t}\n",
    "}\n",
    "```\n",
    "    \n",
    "## Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "DROP TABLE `ets_inpi_no_doublon_siret`;\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database=database,\n",
    "            s3_output=s3_output,\n",
    "  filename = None, ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "CREATE TABLE siretisation.ets_inpi_no_doublon_siret\n",
    "WITH (\n",
    "  format='PARQUET'\n",
    ") AS\n",
    "WITH merge_inpi AS (\n",
    "  SELECT \n",
    "    ROW_NUMBER() OVER (PARTITION BY ets_insee_inpi_no_duplicate.index_id ORDER BY file_timestamp) AS row_id_group,\n",
    "    ets_insee_inpi_no_duplicate.index_id, \n",
    "    ets_insee_inpi_no_duplicate.siren, \n",
    "    ets_insee_inpi_no_duplicate.siret, \n",
    "    ets_insee_inpi_no_duplicate.sequence_id,\n",
    "    code_greffe, \n",
    "    nom_greffe, \n",
    "    numero_gestion, \n",
    "    id_etablissement, \n",
    "    status, \n",
    "    origin, \n",
    "    date_greffe, \n",
    "    file_timestamp, \n",
    "    libelle_evt, \n",
    "    last_libele_evt, \n",
    "    ets_insee_inpi_no_duplicate.status_admin, \n",
    "    type, \n",
    "    ets_insee_inpi_no_duplicate.status_ets, \n",
    "    \"siège_pm\", \n",
    "    rcs_registre, \n",
    "    adresse_ligne1, \n",
    "    adresse_ligne2, \n",
    "    adresse_ligne3, \n",
    "    adresse_reconstituee_inpi, \n",
    "    ets_insee_inpi_no_duplicate.adresse_distance_inpi, \n",
    "    ets_insee_inpi_no_duplicate.list_numero_voie_matching_inpi, \n",
    "    numero_voie_matching, \n",
    "    voie_clean, \n",
    "    type_voie_matching, \n",
    "    code_postal, \n",
    "    code_postal_matching, \n",
    "    ville, \n",
    "    ville_matching, \n",
    "    code_commune, \n",
    "    pays, \n",
    "    domiciliataire_nom, \n",
    "    domiciliataire_siren, \n",
    "    domiciliataire_greffe, \n",
    "    \"domiciliataire_complément\", \n",
    "    \"siege_domicile_représentant\", \n",
    "    nom_commercial, \n",
    "    ets_insee_inpi_no_duplicate.enseigne, \n",
    "    \"activité_ambulante\", \n",
    "    \"activité_saisonnière\", \n",
    "    \"activité_non_sédentaire\", \n",
    "    ets_insee_inpi_no_duplicate.\"date_début_activité\", \n",
    "    \"activité\", \n",
    "    origine_fonds, \n",
    "    origine_fonds_info, \n",
    "    type_exploitation, \n",
    "    csv_source \n",
    "  FROM \n",
    "    siretisation.ets_insee_inpi_no_duplicate \n",
    "    INNER JOIN siretisation.ets_inpi_sql ON ets_insee_inpi_no_duplicate.index_id = siretisation.ets_inpi_sql.index_id \n",
    "  WHERE \n",
    "    count_index = 1\n",
    ") \n",
    "SELECT \n",
    "\n",
    "  *\n",
    "  \n",
    "FROM \n",
    "  merge_inpi \n",
    "WHERE row_id_group = 1    \n",
    "\"\"\"\n",
    "\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database=database,\n",
    "            s3_output=s3_output,\n",
    "  filename = None, ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que la table est créée, nous pouvons la copier dans le dossier [calfdata/TEMP_PARTAGE_DATA_INPI](https://s3.console.aws.amazon.com/s3/buckets/calfdata/TEMP_PARTAGE_DATA_INPI/?region=eu-west-3&tab=overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT index_id, siren, siret, sequence_id, code_greffe, nom_greffe, numero_gestion, id_etablissement\n",
    "FROM ets_inpi_no_doublon_siret \n",
    "\"\"\"\n",
    "output = s3.run_query(\n",
    "            query=query,\n",
    "            database=database,\n",
    "            s3_output=s3_output,\n",
    "  filename = None, ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_key =  '{}/{}.csv'.format(s3_output, output['QueryID'])\n",
    "destination_key_filename = '{}/{}.csv'.format('TEMP_PARTAGE_DATA_INPI', 'inpi_siret')\n",
    "s3.copy_object_s3(source_key = source_key,\n",
    "                              destination_key = destination_key_filename,\n",
    "                              remove = True\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM ets_inpi_no_doublon_siret \n",
    "\"\"\"\n",
    "output = s3.run_query(\n",
    "            query=query,\n",
    "            database=database,\n",
    "            s3_output=s3_output,\n",
    "  filename = None, ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_key =  '{}/{}.csv'.format(s3_output, output['QueryID'])\n",
    "destination_key_filename = '{}/{}.csv'.format('TEMP_PARTAGE_DATA_INPI', 'inpi_siret_full')\n",
    "s3.copy_object_s3(source_key = source_key,\n",
    "                              destination_key = destination_key_filename,\n",
    "                              remove = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nombre de lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT COUNT(*) as cnt\n",
    "FROM ets_inpi_no_doublon_siret\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database=database,\n",
    "            s3_output=s3_output,\n",
    "  filename = \"analyse_1\", ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nombre de siren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT COUNT(DISTINCT(siren)) as CNT\n",
    "FROM ets_inpi_no_doublon_siret\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database=database,\n",
    "            s3_output=s3_output,\n",
    "  filename = \"analyse_2\", ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nombre de siret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT COUNT(DISTINCT(siret)) as CNT\n",
    "FROM ets_inpi_no_doublon_siret\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database=database,\n",
    "            s3_output=s3_output,\n",
    "  filename = \"analyse_3\", ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nombre d'établissements par ville"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT ville_matching, COUNT(DISTINCT(siret)) as CNT\n",
    "FROM ets_inpi_no_doublon_siret\n",
    "GROUP BY ville_matching\n",
    "ORDER BY CNT DESC\n",
    "LIMIT 25\n",
    "\"\"\"\n",
    "(\n",
    "    s3.run_query(\n",
    "            query=query,\n",
    "            database=database,\n",
    "            s3_output=s3_output,\n",
    "  filename = \"analyse_4\", ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )\n",
    "    .set_index('ville_matching')\n",
    "    .style\n",
    "    .format(\"{:,.0f}\")\n",
    "    .bar(subset= ['CNT' ],\n",
    "                       color='#d65f5f')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nombre d\"établissements par Greffe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT nom_greffe, COUNT(DISTINCT(siret)) as CNT\n",
    "FROM ets_inpi_no_doublon_siret\n",
    "GROUP BY nom_greffe\n",
    "ORDER BY CNT DESC\n",
    "LIMIT 25\n",
    "\"\"\"\n",
    "(\n",
    "    s3.run_query(\n",
    "            query=query,\n",
    "            database=database,\n",
    "            s3_output=s3_output,\n",
    "  filename = \"analyse_4\", ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )\n",
    "    .set_index('nom_greffe')\n",
    "    .style\n",
    "    .format(\"{:,.0f}\")\n",
    "    .bar(subset= ['CNT' ],\n",
    "                       color='#d65f5f')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nombre d'établissements créés par année"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT YEAR(\n",
    "Coalesce(\n",
    "      try(\n",
    "        date_parse(\n",
    "          \"date_début_activité\", '%Y-%m-%d'\n",
    "        )\n",
    "      ), \n",
    "      try(\n",
    "        date_parse(\n",
    "          \"date_début_activité\", '%Y-%m-%d %hh:%mm:%ss.SSS'\n",
    "        )\n",
    "      ), \n",
    "      try(\n",
    "        date_parse(\n",
    "          \"date_début_activité\", '%Y-%m-%d %hh:%mm:%ss'\n",
    "        )\n",
    "      ), \n",
    "      try(\n",
    "        cast(\n",
    "          \"date_début_activité\" as timestamp\n",
    "        )\n",
    "      )\n",
    "    ) \n",
    ") as date_debut_activite,\n",
    "\n",
    "\n",
    "COUNT(DISTINCT(siret)) as CNT\n",
    "FROM ets_inpi_no_doublon_siret\n",
    "GROUP BY YEAR(\n",
    "Coalesce(\n",
    "      try(\n",
    "        date_parse(\n",
    "          \"date_début_activité\", '%Y-%m-%d'\n",
    "        )\n",
    "      ), \n",
    "      try(\n",
    "        date_parse(\n",
    "          \"date_début_activité\", '%Y-%m-%d %hh:%mm:%ss.SSS'\n",
    "        )\n",
    "      ), \n",
    "      try(\n",
    "        date_parse(\n",
    "          \"date_début_activité\", '%Y-%m-%d %hh:%mm:%ss'\n",
    "        )\n",
    "      ), \n",
    "      try(\n",
    "        cast(\n",
    "          \"date_début_activité\" as timestamp\n",
    "        )\n",
    "      )\n",
    "    ) \n",
    ")\n",
    "ORDER BY CNT DESC\n",
    "LIMIT 25\n",
    "\"\"\"\n",
    "(\n",
    "    s3.run_query(\n",
    "            query=query,\n",
    "            database=database,\n",
    "            s3_output=s3_output,\n",
    "  filename = \"analyse_4\", ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )\n",
    "    .dropna()\n",
    "    .style\n",
    "    .format(\"{:,.0f}\")\n",
    "    .bar(subset= ['CNT' ],\n",
    "                       color='#d65f5f')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nombre de siret par événements\n",
    "\n",
    "A verifier pourquoi nombre de lignes différents du nombre de siret par ville"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT ville_matching, libelle_evt,\n",
    "COUNT(DISTINCT(siret)) as CNT\n",
    "FROM ets_inpi_no_doublon_siret\n",
    "GROUP BY ville_matching, libelle_evt\n",
    "ORDER BY CNT DESC\n",
    "\"\"\"\n",
    "\n",
    "output = (\n",
    "    s3.run_query(\n",
    "            query=query,\n",
    "            database=database,\n",
    "            s3_output=s3_output,\n",
    "  filename = \"analyse_4\", ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )\n",
    "    #.dropna()\n",
    "    #.style\n",
    "    #.format(\"{:,.0f}\")\n",
    "    #.bar(subset= ['CNT' ],\n",
    "    #                   color='#d65f5f')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(output\n",
    " #.dropna()\n",
    " .set_index(['ville_matching','libelle_evt'])\n",
    " .unstack(-1)\n",
    " .assign(total = lambda x: x.sum(axis = 1))\n",
    " .sort_values(by = 'total', ascending = False)\n",
    " .head(25)\n",
    " .fillna(0)\n",
    " .style\n",
    " .format(\"{:,.0f}\")\n",
    " .bar(subset= ['total' ],\n",
    "                       color='#d65f5f')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, shutil, urllib, ipykernel, json\n",
    "from pathlib import Path\n",
    "from notebook import notebookapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_report(extension = \"html\", keep_code = False):\n",
    "    \"\"\"\n",
    "    Create a report from the current notebook and save it in the \n",
    "    Report folder (Parent-> child directory)\n",
    "    \n",
    "    1. Exctract the current notbook name\n",
    "    2. Convert the Notebook \n",
    "    3. Move the newly created report\n",
    "    \n",
    "    Args:\n",
    "    extension: string. Can be \"html\", \"pdf\", \"md\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ### Get notebook name\n",
    "    connection_file = os.path.basename(ipykernel.get_connection_file())\n",
    "    kernel_id = connection_file.split('-', 1)[0].split('.')[0]\n",
    "\n",
    "    for srv in notebookapp.list_running_servers():\n",
    "        try:\n",
    "            if srv['token']=='' and not srv['password']:  \n",
    "                req = urllib.request.urlopen(srv['url']+'api/sessions')\n",
    "            else:\n",
    "                req = urllib.request.urlopen(srv['url']+ \\\n",
    "                                             'api/sessions?token=' + \\\n",
    "                                             srv['token'])\n",
    "            sessions = json.load(req)\n",
    "            notebookname = sessions[0]['name']\n",
    "        except:\n",
    "            pass  \n",
    "    \n",
    "    sep = '.'\n",
    "    path = os.getcwd()\n",
    "    #parent_path = str(Path(path).parent)\n",
    "    \n",
    "    ### Path report\n",
    "    #path_report = \"{}/Reports\".format(parent_path)\n",
    "    #path_report = \"{}/Reports\".format(path)\n",
    "    \n",
    "    ### Path destination\n",
    "    name_no_extension = notebookname.split(sep, 1)[0]\n",
    "    source_to_move = name_no_extension +'.{}'.format(extension)\n",
    "    dest = os.path.join(path,'Reports', source_to_move)\n",
    "    \n",
    "    ### Generate notebook\n",
    "    if keep_code:\n",
    "        os.system('jupyter nbconvert --to {} {}'.format(\n",
    "    extension,notebookname))\n",
    "    else:\n",
    "        os.system('jupyter nbconvert --no-input --to {} {}'.format(\n",
    "    extension,notebookname))\n",
    "    \n",
    "    ### Move notebook to report folder\n",
    "    #time.sleep(5)\n",
    "    shutil.move(source_to_move, dest)\n",
    "    print(\"Report Available at this adress:\\n {}\".format(dest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_report(extension = \"html\", keep_code = True)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nteract": {
   "version": "0.24.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
