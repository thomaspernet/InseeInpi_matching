{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation méthodologie calcul poids similarité adresse via word2vec\n",
    "\n",
    "Objective(s)\n",
    "\n",
    "- Lors du POC de l’US [US 06 Union et Intersection](https://coda.io/d/CreditAgricole_dCtnoqIftTn/US-06-Union-et-Intersection_sucns), nous avons besoin d’une table contenant les poids indiquant la similarité entre deux mots. Dès lors, il est indispensable de créer un notebook avec de la documentation sur la création de ces poids et de la technique utilisée. \n",
    "\n",
    "  - Une table sera créé dans Athena avec deux colonnes pour chacun des mots et un poids. Les trois nouvelles variables seront appelées:\n",
    "\n",
    "  - Mot_A\n",
    "  - Mot_B\n",
    "  - Index_relation\n",
    "\n",
    "- Pour calculer les poids, il faut utiliser la table suivante XX avec les variables:\n",
    "\n",
    "  -  `adresse_distance_inpi` \n",
    "  -  `adresse_distance_inpi` \n",
    "\n",
    "## Metadata\n",
    "\n",
    "- Metadata parameters are available here: [Ressources_suDYJ#_luZqd](http://Ressources_suDYJ#_luZqd)\n",
    "\n",
    "  - Task type:\n",
    "\n",
    "- Jupyter Notebook\n",
    "\n",
    "- Users: :\n",
    "\n",
    "    - [Thomas Pernet](mailto:t.pernetcoudrier@gmail.com)\n",
    "\n",
    "- Watchers:\n",
    "\n",
    "  - [Thomas Pernet](mailto:t.pernetcoudrier@gmail.com)\n",
    "\n",
    "- Estimated Log points:\n",
    "\n",
    "  - One being a simple task, 15 a very difficult one\n",
    "    -  7\n",
    "\n",
    "- Task tag\n",
    "\n",
    "  - \\#machine-learning,#word2vec,#documentation,#similarite\n",
    "\n",
    "- Toggl Tag\n",
    "\n",
    "  - \\#variable-computation\n",
    " \n",
    "  \n",
    "## Input Cloud Storage [AWS/GCP]\n",
    "\n",
    "If link from the internet, save it to the cloud first\n",
    "\n",
    "### Tables [AWS/BigQuery]\n",
    "\n",
    "1. Batch 1:\n",
    "  * Select Provider: Athena\n",
    "  * Select table(s): ets_insee_inpi\n",
    "    * Select only tables created from the same notebook, else copy/paste selection to add new input tables\n",
    "    * If table(s) does not exist, add them: Add New Table\n",
    "    * Information:\n",
    "      * Region: \n",
    "        * NameEurope (Paris)\n",
    "        * Code: eu-west-3\n",
    "      * Database: inpi\n",
    "      * Notebook construction file: https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/03_ETS_add_variables.md\n",
    "    \n",
    "## Destination Output/Delivery\n",
    "\n",
    "1. Athena: \n",
    "      * Region: Europe (Paris)\n",
    "      * Database: machine_learning\n",
    "      * Tables (Add name new table):\n",
    "          - list_mots_insee_inpi\n",
    "          - list_mots_insee_inpi_word2vec_weights\n",
    "\n",
    "2. S3(Add new filename to Database: Ressources)\n",
    "      * Origin: Jupyter notebook\n",
    "      * Bucket: calfdata\n",
    "      * Key: MACHINE_LEARNING/NLP/WORD2VEC_WEIGHTS\n",
    "      * Filename(s): word2vec_weights_100\n",
    "\n",
    "  \n",
    "## Things to know (Steps, Attention points or new flow of information)\n",
    "\n",
    "### Sources of information  (meeting notes, Documentation, Query, URL)\n",
    "\n",
    "- Query [Athena/BigQuery]\n",
    "\n",
    "  1. Link 1: [Liste ngrams](https://eu-west-3.console.aws.amazon.com/athena/home?region=eu-west-3#query/history/79a481c2-df9c-4785-993b-4c6813947770)\n",
    "\n",
    "    - Description: Query utilisée précédemment pour créer la liste des combinaisons INSEE-INPI\n",
    "    \n",
    "1. GitHub\n",
    "  * Repo: https://github.com/thomaspernet/InseeInpi_matching\n",
    "  * Folder name: Notebooks_matching/Data_preprocessed/programme_matching/02_siretisation\n",
    "  * Source code:  Test_word2Vec.md\n",
    "2. Python Module [Module name](link)\n",
    "  * Library 1: gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connexion serveur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsPy.aws_authorization import aws_connector\n",
    "from awsPy.aws_athena import service_athena\n",
    "from awsPy.aws_s3 import service_s3\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, shutil\n",
    "bucket = 'calfdata'\n",
    "path = os.getcwd()\n",
    "parent_path = str(Path(path).parent)\n",
    "path_cred = r\"{}/credential_AWS.json\".format(parent_path)\n",
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                       region = 'eu-west-3')\n",
    "client= con.client_boto()\n",
    "s3 = service_s3.connect_S3(client = client,\n",
    "                      bucket = 'calfdata', verbose = False) \n",
    "athena = service_athena.connect_athena(client = client,\n",
    "                      bucket = 'calfdata') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"CREATE DATABASE IF NOT EXISTS machine_learning\n",
    "  COMMENT 'DB for machine learning tests'\n",
    "  LOCATION 's3://calfdata/MACHINE_LEARNING/NLP/'\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation Tables\n",
    "\n",
    "Pour cacluler la similarité entre l'adresse de l'INSEE et de l'INPI, nous devons créer une liste de combinaison unique entre les deux adresses. Pour cela, nous utilisons les variables `adresse_distance_inpi`  et `adresse_distance_insee` qui ont été, au préalable, néttoyée, puis nous les concatènons en prenant le soin d'enlever les mots redondants. Dit autrement, si deux mots sont présents dans les deux adresses, alors, nous n'en gardons qu'un seul. \n",
    "\n",
    "La table contient environ 2,836,384 combinaisons possibles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create table = False\n",
    "query_combination = \"\"\"\n",
    "/*Combinaison mots insee inpi*/\n",
    "CREATE TABLE machine_learning.list_mots_insee_inpi\n",
    "WITH (\n",
    "  format='PARQUET'\n",
    ") AS\n",
    "SELECT unique_combinaition,\n",
    "COUNT(*) AS CNT\n",
    "FROM (SELECT\n",
    "array_distinct(\n",
    "    concat(\n",
    "    array_distinct(\n",
    "      split(adresse_distance_inpi, ' ')\n",
    "      ),\n",
    "    array_distinct(\n",
    "      split(adresse_distance_insee, ' ')\n",
    "    )    )\n",
    "  ) unique_combinaition\n",
    "FROM inpi.ets_insee_inpi \n",
    "      )\n",
    "      GROUP BY unique_combinaition\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_table:\n",
    "    output = athena.run_query(\n",
    "            query=query_combination,\n",
    "            database='inpi',\n",
    "            s3_output='INPI/sql_output'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcul model\n",
    "\n",
    "Dès lors que la table d'entrainement est prète, nous allons calculer un vecteur de poid pour chaque mot. Par défaut, nous calculons 100 poids pour chacune des occurences. Les poids sont calculés grace a la technique de Word2Vec\n",
    "\n",
    "## [What Are Word Embeddings for Text?](https://machinelearningmastery.com/what-are-word-embeddings/) \n",
    "\n",
    "- by [[Jason Brownlee]]\n",
    "\n",
    "## What Are Word Embeddings?\n",
    "\n",
    "- A word embedding is a learned representation for text where words that have the same meaning have a similar representation\n",
    "- It is this approach to representing words and documents that may be considered one of the key breakthroughs of deep learning on challenging natural language processing problems.\n",
    "\n",
    "    -> One of the benefits of using dense and low-dimensional vectors is computational\n",
    "    \n",
    "- The main benefit of the dense representations is generalization power\n",
    "- if we believe some features may provide similar clues, it is worthwhile to provide a representation that is able to capture these similarities\n",
    "- Word embeddings are in fact a class of techniques where individual words are represented as real-valued vectors in a predefined vector space\n",
    "- Each word is mapped to one vector and the vector values are learned in a way that resembles a neural network, and hence the technique is often lumped into the field of deep learning\n",
    "- Key to the approach is the idea of using a dense distributed representation for each word\n",
    "- Each word is represented by a real-valued vector, often tens or hundreds of dimensions. This is contrasted to the thousands or millions of dimensions required for sparse word representations, such as a one-hot encoding\n",
    "- The distributed representation is learned based on the usage of words\n",
    "- This allows words that are used in similar ways to result in having similar representations, naturally capturing their meaning\n",
    "- This can be contrasted with the crisp but fragile representation in a bag of words model where, unless explicitly managed, different words have different representations, regardless of how they are used\n",
    "\n",
    "## Word Embedding Algorithms\n",
    "    \n",
    "- Word embedding methods learn a real-valued vector representation for a predefined fixed sized vocabulary from a corpus of text\n",
    "- The learning process is either joint with the neural network model on some task, such as document classification, or is an unsupervised process, using document statistics.\n",
    "\n",
    "\n",
    "### Word2Vec\n",
    "\n",
    "- Word2Vec is a statistical method for efficiently learning a standalone word embedding from a text corpus\n",
    "- It was developed by Tomas Mikolov, et al. at Google in 2013 as a response to make the neural-network-based training of the embedding more efficient and since then has become the de facto standard for developing pre-trained word embedding\n",
    "- Two different learning models were introduced that can be used as part of the word2vec approach to learn the word embedding; they are:\n",
    "    - Continuous Bag-of-Words, or CBOW model \n",
    "    - Continuous Skip-Gram Model. \n",
    "    - The CBOW model learns the embedding by predicting the current word based on its context\n",
    "    - The continuous skip-gram model learns by predicting the surrounding words given a current word\n",
    "    - Both models are focused on learning about words given their local usage context, where the context is defined by a window of neighboring words\n",
    "    - The key benefit of the approach is that high-quality word embeddings can be learned efficiently (low space and time complexity), allowing larger embeddings to be learned (more dimensions) from much larger corpora of text (billions of words).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM machine_learning.list_mots_insee_inpi\n",
    "\"\"\"\n",
    "\n",
    "### run query\n",
    "output = athena.run_query(\n",
    "        query=query,\n",
    "        database='machine_learning',\n",
    "        s3_output='INPI/sql_output'\n",
    "    )\n",
    "\n",
    "results = False\n",
    "filename = 'combinaison_adresses_insee_inpi.csv'\n",
    "    \n",
    "while results != True:\n",
    "    source_key = \"{}/{}.csv\".format(\n",
    "                            'INPI/sql_output',\n",
    "                            output['QueryExecutionId']\n",
    "                                   )\n",
    "    destination_key = \"{}/{}\".format(\n",
    "                                'MACHINE_LEARNING/NLP/LISTE_INSEE_INPI',\n",
    "                                filename\n",
    "                            )\n",
    "        \n",
    "    results = s3.copy_object_s3(\n",
    "                                source_key = source_key,\n",
    "                                destination_key = destination_key,\n",
    "                                remove = True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_insee_inpi = (s3.read_df_from_s3(\n",
    "            key = 'MACHINE_LEARNING/NLP/LISTE_INSEE_INPI/{}'.format(filename), sep = ',')\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "There are many parameters on this constructor; a few noteworthy arguments you may wish to configure are:\n",
    "\n",
    "- size: (default 100) The number of dimensions of the embedding, e.g. the length of the dense vector to represent each token (word).\n",
    "- window: (default 5) The maximum distance between a target word and words around the target word.\n",
    "- min_count: (default 5) The minimum count of words to consider when training the model; words with an occurrence less than this count will be ignored.\n",
    "- workers: (default 3) The number of threads to use while training.\n",
    "- sg: (default 0 or CBOW) The training algorithm, either CBOW (0) or skip gram (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(text):\n",
    "    return re.sub(r'[^\\w\\s]|[|]', '', text).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = list_insee_inpi['unique_combinaition'].apply(lambda x: basic_clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le POC, nous utilisons les paramètres par défault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "model = Word2Vec(df_text.tolist(),\n",
    "                 size = 100,\n",
    "                 window = 5,\n",
    "                 min_count=5,\n",
    "                 sg = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous devons calculer la similarité entre les mots communs dans l'adresse INPI/INSEE, donc nous pouvons utiliser les poids du modèles et ensuite calculer la similarité avec la méthode du cosine. \n",
    "\n",
    "La librarie `gensim` permet d'exporter les poids en `.txt`. Toutefois, il n'est pas concevable de calculer l'ensembles des similarités entre toutes les occurences (environ 90.000), donc lors des traitements dans Athena, nous calculerons le cosines à la demande. \n",
    "\n",
    "Pour cela, nous allons créer un csv avec deux colonnes, `words` et `list_weights`. Attention, cette dernière n'est pas une liste dans le csv, mais le sera dans Athena. Athena permet d'importer un ensemble de valeur dans un array. Si on crée une liste dans le csv, Athena va créer une liste de liste. Ainsi, il est plus simple dans le csv de créer uniquement deux colonnes, les mots et les poids. Le séparateur `|` sera utilisé. Le csv ressemble a ca:\n",
    "\n",
    "```\n",
    "Words | list_of_weights\n",
    "RUE | .1, .4 ......\n",
    "AVENUE | .2, .9 ......\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format('word2vec_weights_100.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_header = ['Words'].extend(list(range(1, 101)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wieghts = pd.read_csv('word2vec_weights_100.txt',\n",
    "                            sep = ' ', skiprows= 1,\n",
    "                           header= list_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_weight = list(\n",
    "    zip(\n",
    "    model_wieghts.set_index(0).index.values.tolist(),\n",
    "    model_wieghts.set_index(0).values.tolist()\n",
    ")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.DataFrame(zipped_weight)\n",
    "  .rename(columns= {0:'words', 1: 'list_weights'})\n",
    "  .assign(list_weights = lambda x:x['list_weights'].apply(lambda x: ','.join(map(str, x))))\n",
    "  .to_csv('word2vec_weights_100.csv', index = False, sep = \"|\")       \n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle se trouve à l'adresse suivante [MACHINE_LEARNING/NLP/WORD2VEC_WEIGHTS](https://s3.console.aws.amazon.com/s3/buckets/calfdata/MACHINE_LEARNING/NLP/WORD2VEC_WEIGHTS/?region=eu-west-3&tab=overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.upload_file('word2vec_weights_100.csv',\n",
    "               'MACHINE_LEARNING/NLP/WORD2VEC_WEIGHTS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tables weights\n",
    "\n",
    "Pour convertir un string en array, il suffit d'utiliser `array<string>`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS machine_learning.list_mots_insee_inpi_word2vec_weights (\n",
    "\n",
    "`Words` string,\n",
    "`list_weights` array<string>\n",
    "  )\n",
    "\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'\n",
    "     WITH SERDEPROPERTIES (\n",
    "      'serialization.format' = ',',\n",
    "      'field.delim' = '|') \n",
    "     LOCATION 's3://calfdata/MACHINE_LEARNING/NLP/WORD2VEC_WEIGHTS'\n",
    "     TBLPROPERTIES ('has_encrypted_data'='false', \n",
    "     'skip.header.line.count'='1')\n",
    "     \n",
    "\"\"\"\n",
    "output = athena.run_query(\n",
    "        query=query,\n",
    "        database='machine_learning',\n",
    "        s3_output='INPI/sql_output'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse du modèle\n",
    "\n",
    "La librairie `gensim` a une fonction integrée pour calculer la similarité. Toutefois, nous devons cacluler le cosine manuelement dans Athena car il n'y a pas de fonction SQL prévue a cet effet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarité - cosine\n",
    "\n",
    "La fonction coseine se calcule de la facon suivante:\n",
    "\n",
    "$$\\frac{u \\cdot v}{\\|u\\|_{2}\\|v\\|_{2}}$$\n",
    "\n",
    "- Source 1: Calcul cosine:\n",
    "  - [Cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity)\n",
    "  - [Magnitude (mathematics)](https://en.wikipedia.org/wiki/Magnitude_(mathematics)#Euclidean_vector_space)\n",
    "  - [Scipy Cosine](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci dessous, un exemple des 10 premiers poids des mots `BOULEVARD`  et `BD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['BOULEVARD'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['BD'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - distance.cosine(model['BOULEVARD'], model['BD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity('BOULEVARD', 'BD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcul à la main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(model['BD'], model['BOULEVARD'])/ \\\n",
    "(np.sqrt(np.sum(np.square(model['BD']))) * np.sqrt(np.sum(np.square(model['BOULEVARD']))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T-SNE plot pour les similarités entre les mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_closestwords_tsnescatterplot(model, word, size):\n",
    "    \n",
    "    fig= plt.figure(figsize=(10,10))\n",
    "    \n",
    "    arr = np.empty((0,size), dtype='f')\n",
    "    word_labels = [word]\n",
    "    close_words = model.similar_by_word(word)\n",
    "    arr = np.append(arr, np.array([model[word]]), axis=0)\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model[wrd_score[0]]\n",
    "        word_labels.append(wrd_score[0])\n",
    "        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(arr)\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "    plt.scatter(x_coords, y_coords)\n",
    "    for label, x, y in zip(word_labels, x_coords, y_coords):\n",
    "            plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n",
    "    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_closestwords_tsnescatterplot(model = model,\n",
    "                                     word = 'BOULEVARD',\n",
    "                                     size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_closestwords_tsnescatterplot(model = model,\n",
    "                                     word = 'AVENUE',\n",
    "                                     size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_closestwords_tsnescatterplot(model = model,\n",
    "                                     word = 'ZI',\n",
    "                                     size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_closestwords_tsnescatterplot(model = model,\n",
    "                                     word = 'APPART',\n",
    "                                     size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_closestwords_tsnescatterplot(model = model,\n",
    "                                     word = 'CDT',\n",
    "                                     size = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, shutil, urllib, ipykernel, json\n",
    "from pathlib import Path\n",
    "from notebook import notebookapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_report(extension = \"html\"):\n",
    "    \"\"\"\n",
    "    Create a report from the current notebook and save it in the \n",
    "    Report folder (Parent-> child directory)\n",
    "    \n",
    "    1. Exctract the current notbook name\n",
    "    2. Convert the Notebook \n",
    "    3. Move the newly created report\n",
    "    \n",
    "    Args:\n",
    "    extension: string. Can be \"html\", \"pdf\", \"md\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ### Get notebook name\n",
    "    connection_file = os.path.basename(ipykernel.get_connection_file())\n",
    "    kernel_id = connection_file.split('-', 1)[0].split('.')[0]\n",
    "\n",
    "    for srv in notebookapp.list_running_servers():\n",
    "        try:\n",
    "            if srv['token']=='' and not srv['password']:  \n",
    "                req = urllib.request.urlopen(srv['url']+'api/sessions')\n",
    "            else:\n",
    "                req = urllib.request.urlopen(srv['url']+ \\\n",
    "                                             'api/sessions?token=' + \\\n",
    "                                             srv['token'])\n",
    "            sessions = json.load(req)\n",
    "            notebookname = sessions[0]['name']\n",
    "        except:\n",
    "            pass  \n",
    "    \n",
    "    sep = '.'\n",
    "    path = os.getcwd()\n",
    "    #parent_path = str(Path(path).parent)\n",
    "    \n",
    "    ### Path report\n",
    "    #path_report = \"{}/Reports\".format(parent_path)\n",
    "    #path_report = \"{}/Reports\".format(path)\n",
    "    \n",
    "    ### Path destination\n",
    "    name_no_extension = notebookname.split(sep, 1)[0]\n",
    "    source_to_move = name_no_extension +'.{}'.format(extension)\n",
    "    dest = os.path.join(path,'Reports', source_to_move)\n",
    "    \n",
    "    ### Generate notebook\n",
    "    os.system('jupyter nbconvert --no-input --to {} {}'.format(\n",
    "    extension,notebookname))\n",
    "    \n",
    "    ### Move notebook to report folder\n",
    "    #time.sleep(5)\n",
    "    shutil.move(source_to_move, dest)\n",
    "    print(\"Report Available at this adress:\\n {}\".format(dest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_report(extension = \"html\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
