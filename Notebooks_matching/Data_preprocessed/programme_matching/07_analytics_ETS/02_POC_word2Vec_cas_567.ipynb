{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test similarite exception list mots INSEE et INPI siretisation\n",
    "\n",
    "Objective(s)\n",
    "\n",
    "*  L’objectif de cette tache est de trouver une solution pour retourner la distance donnée par le Word2Vec entre 2 listes contenant des mots qui ne sont pas communs dans l’adresse INSEE et INPI\n",
    "* Il faut faire le test lorsque la variable status_cas est egal a CAS_5,6 ou 7\n",
    "* Par exemple:\n",
    "    * inpi_except: [A, B]\n",
    "    * insee_except: [A,C]\n",
    "    * Le test: [[A,A], [A,C], [B,A],[B,C]]\n",
    "    * Output: [p1, p2, p3, p4]\n",
    "    * Recupération max list output\n",
    "    * Variables nécéssaire:\n",
    "        * inpi_except \n",
    "        * insee_except \n",
    "        * status_cas\n",
    "\n",
    "## Metadata\n",
    "\n",
    "* Metadata parameters are available here: Ressources_suDYJ#_luZqd\n",
    "* Task type:\n",
    "  * Jupyter Notebook\n",
    "* Users: :\n",
    "  * Thomas Pernet\n",
    "* Watchers:\n",
    "  * Thomas Pernet\n",
    "* Estimated Log points:\n",
    "  * One being a simple task, 15 a very difficult one\n",
    "  *  14\n",
    "* Task tag\n",
    "  *  #sql-query,#matching,#siretisation,#machine-learning,#word2vec\n",
    "* Toggl Tag\n",
    "  * #poc\n",
    "  \n",
    "## Input Cloud Storage [AWS/GCP]\n",
    "\n",
    "If link from the internet, save it to the cloud first\n",
    "\n",
    "### Tables [AWS/BigQuery]\n",
    "\n",
    "1. Batch 1:\n",
    "    * Select Provider: Athena\n",
    "      * Select table(s): ets_inpi_insee_cases\n",
    "        * Select only tables created from the same notebook, else copy/paste selection to add new input tables\n",
    "        * If table(s) does not exist, add them: Add New Table\n",
    "        * Information:\n",
    "          * Region: \n",
    "            * NameEurope (Paris)\n",
    "            * Code: eu-west-3\n",
    "          * Database: inpi\n",
    "          * Notebook construction file: [07_pourcentage_siretisation_v3](https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/02_siretisation/07_pourcentage_siretisation_v3.md)\n",
    "2. Batch 2:\n",
    "  * Select Provider: Athena\n",
    "  * Select table(s): list_mots_insee_inpi_word2vec_weights\n",
    "    * Select only tables created from the same notebook, else copy/paste selection to add new input tables\n",
    "    * If table(s) does not exist, add them: Add New Table\n",
    "    * Information:\n",
    "      * Region: \n",
    "        * NameEurope (Paris)\n",
    "        * Code: eu-west-3\n",
    "      * Database: machine_learning\n",
    "      * Notebook construction file: 03_POC_word2Vec_weights_computation\n",
    "    \n",
    "## Destination Output/Delivery\n",
    "\n",
    "* Athena: \n",
    "    * Region: Europe (Paris)\n",
    "    * Database: inpi\n",
    "    * Tables (Add name new table): ets_inpi_insee_wordvec\n",
    "\n",
    "  \n",
    "## Things to know (Steps, Attention points or new flow of information)\n",
    "\n",
    "### Sources of information  (meeting notes, Documentation, Query, URL)\n",
    "\n",
    "1. Jupyter Notebook (Github Link)\n",
    "  1. md : [Test_word2Vec.md](https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/02_siretisation/Test_word2Vec.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connexion serveur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsPy.aws_authorization import aws_connector\n",
    "from awsPy.aws_athena import service_athena\n",
    "from awsPy.aws_s3 import service_s3\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, shutil\n",
    "bucket = 'calfdata'\n",
    "path = os.getcwd()\n",
    "parent_path = str(Path(path).parent)\n",
    "path_cred = r\"{}/credential_AWS.json\".format(parent_path)\n",
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                       region = 'eu-west-3')\n",
    "client= con.client_boto()\n",
    "s3 = service_s3.connect_S3(client = client,\n",
    "                      bucket = 'calfdata', verbose = False) \n",
    "athena = service_athena.connect_athena(client = client,\n",
    "                      bucket = 'calfdata') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etape de creation\n",
    "\n",
    "- Filtrer les cas 5 à 7 et `test_list_num_voie != 'False'`. Il est inutile d'appliquer le test sur des lignes dont on est sur que les numéros ne sont pas identiques. On garde les `NULL` car ils correspondent aux lignes sans numéro.\n",
    "- créer deux colonnes avec le pseudo-produit cartésien (table INPI vers table INSEE). Autrement dit, on ne souhaite pas comparer les mots au sein de la même liste, mais entre les listes. \n",
    "    - Table `ets_inpi_insee_cases` \n",
    "- Merge la liste des poids dans la table `list_mots_insee_inpi_word2vec_weights` \n",
    "- Calcul de la Cosine distance (dot product sur la magnitude)\n",
    "- Calcul de la Cosine distance maximum par group `index_id` et `unzip_inpi`\n",
    "- Recupération de la combinaison maximum par group\n",
    "- Calculer le nombre de Cosine distance supérieure à .6.\n",
    "    - Sert pour determiner combien de mots ont été retrouvé\n",
    "- Création de la table `ets_inpi_insee_word2vec` pour analyse\n",
    "\n",
    "On travaille sur une table de `1250134` lignes.  [Query](https://eu-west-3.console.aws.amazon.com/athena/home?region=eu-west-3#query/history/a4450f46-1f00-4e73-bb5d-8b41bf6a7125)\n",
    "\n",
    "Attention, si il y a des valeurs nulles dans le calcul de la Cosine distance, c'est que le modèle n'a pas calculer de poid car pas suffisament d'occurences.\n",
    "\n",
    "## Detail\n",
    "\n",
    "La table `ets_inpi_insee_word2vec` est créé via une seule query, toutefois, ci dessous se trouve le detail de chaque étape\n",
    "\n",
    "### Filtre et pseudo produit cartesien + merge\n",
    "\n",
    "```\n",
    "WITH\n",
    "dataset AS (\n",
    "SELECT index_id,inpi_except, insee_except, \n",
    "  transform(sequence(1, CARDINALITY(insee_except)), x-> insee_except),\n",
    "  ZIP(inpi_except,\n",
    "      \n",
    "        transform(sequence(1, CARDINALITY(inpi_except)), x-> insee_except)\n",
    "     ) as test\n",
    "FROM inpi.ets_inpi_insee_cases\n",
    "where status_cas = 'CAS_6'\n",
    "LIMIT 10\n",
    "  )\n",
    "  SELECT *\n",
    "  FROM (\n",
    "    WITH weight_inpi AS (\n",
    "  SELECT index_id, inpi_except, insee_except, unzip_inpi, unzip_insee, list_weights as list_weights_inpi\n",
    "  FROM (\n",
    "  SELECT\n",
    "  index_id, inpi_except, insee_except,\n",
    "    unzip.field0 as unzip_inpi,\n",
    "    unzip.field1 as insee,\n",
    "    test\n",
    "  FROM dataset\n",
    "  CROSS JOIN UNNEST(test) AS new (unzip)\n",
    "    )\n",
    "   CROSS JOIN UNNEST(insee) as test (unzip_insee)\n",
    "   LEFT JOIN list_mots_insee_inpi_word2vec_weights \n",
    "   ON unzip_inpi = list_mots_insee_inpi_word2vec_weights.words\n",
    "  )\n",
    "SELECT\n",
    "    \n",
    "    index_id, inpi_except, insee_except, unzip_inpi, unzip_insee, list_weights_inpi,\n",
    "    list_weights as list_weights_insee\n",
    "    FROM weight_inpi\n",
    "    LEFT JOIN list_mots_insee_inpi_word2vec_weights \n",
    "    ON unzip_insee = list_mots_insee_inpi_word2vec_weights.words\n",
    "    \n",
    ")   \n",
    "``` \n",
    "\n",
    "### Calcul dot\n",
    "\n",
    "```\n",
    "REDUCE(\n",
    "zip_with(list_weights_inpi, list_weights_insee, \n",
    "         (x, y) -> x * y\n",
    "         ),\n",
    "  CAST(ROW(0.0) AS ROW(sum DOUBLE)),\n",
    "  (s, x) -> CAST(ROW(x + s.sum)  AS ROW(sum DOUBLE)),\n",
    "  s -> s.sum\n",
    "  ) AS dot_product,\n",
    "```\n",
    "\n",
    "### Calcul magnitude\n",
    "\n",
    "```\n",
    "SQRT(\n",
    "REDUCE(\n",
    "transform(list_weights_inpi,\n",
    "         (x) -> POW(x, 2)\n",
    "         )\n",
    "         ,\n",
    "  CAST(ROW(0.0) AS ROW(sum DOUBLE)),\n",
    "  (s, x) -> CAST(ROW(x + s.sum)  AS ROW(sum DOUBLE)),\n",
    "  s -> s.sum\n",
    "  )\n",
    "  ) *\n",
    "SQRT(\n",
    "REDUCE(\n",
    "transform(list_weights_insee,\n",
    "         (x) -> POW(x, 2)\n",
    "         )\n",
    "         ,\n",
    "  CAST(ROW(0.0) AS ROW(sum DOUBLE)),\n",
    "  (s, x) -> CAST(ROW(x + s.sum)  AS ROW(sum DOUBLE)),\n",
    "  s -> s.sum\n",
    "  )\n",
    "  )  \n",
    "AS denominator,\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation table analyse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "CREATE TABLE inpi.ets_inpi_insee_word2vec\n",
    "WITH (\n",
    "  format='PARQUET'\n",
    ") AS\n",
    "WITH dataset AS (\n",
    "  SELECT \n",
    "    row_id, \n",
    "    index_id, \n",
    "    status_cas, \n",
    "    inpi_except, \n",
    "    insee_except, \n",
    "    pct_intersection, \n",
    "    len_inpi_except,\n",
    "    len_insee_except, \n",
    "    transform(\n",
    "      sequence(\n",
    "        1, \n",
    "        CARDINALITY(insee_except)\n",
    "      ), \n",
    "      x -> insee_except\n",
    "    ), \n",
    "    ZIP(\n",
    "      inpi_except, \n",
    "      transform(\n",
    "        sequence(\n",
    "          1, \n",
    "          CARDINALITY(inpi_except)\n",
    "        ), \n",
    "        x -> insee_except\n",
    "      )\n",
    "    ) as test \n",
    "  FROM \n",
    "    inpi.ets_inpi_insee_cases \n",
    "  where \n",
    "    (\n",
    "      status_cas = 'CAS_5' \n",
    "      OR status_cas = 'CAS_6' \n",
    "      OR status_cas = 'CAS_7'\n",
    "    ) \n",
    "    AND test_list_num_voie != 'False'\n",
    "     -- AND index_id = 3980536\n",
    ") \n",
    "SELECT \n",
    "  * \n",
    "FROM \n",
    "  (\n",
    "    WITH test AS (\n",
    "      SELECT \n",
    "        * \n",
    "      FROM \n",
    "        (\n",
    "          WITH max_data AS (\n",
    "            SELECT \n",
    "              row_id, \n",
    "              dataset.index_id, \n",
    "              status_cas, \n",
    "              inpi_except, \n",
    "              insee_except, \n",
    "              pct_intersection,\n",
    "              len_inpi_except,\n",
    "              len_insee_except,\n",
    "              test, \n",
    "              max_pct_intersection \n",
    "            FROM \n",
    "              dataset \n",
    "              INNER JOIN (\n",
    "                SELECT \n",
    "                  index_id, \n",
    "                  MAX(pct_intersection) as max_pct_intersection \n",
    "                FROM \n",
    "                  dataset \n",
    "                GROUP BY \n",
    "                  index_id\n",
    "              ) AS max_pct ON dataset.index_id = max_pct.index_id \n",
    "              AND dataset.pct_intersection = max_pct.max_pct_intersection\n",
    "          ) \n",
    "          SELECT \n",
    "            * \n",
    "          FROM \n",
    "            (\n",
    "              WITH weight_inpi AS (\n",
    "                SELECT \n",
    "                  row_id,\n",
    "                  index_id, \n",
    "                  status_cas, \n",
    "                  inpi_except, \n",
    "                  insee_except,\n",
    "                  len_inpi_except,\n",
    "                  len_insee_except,\n",
    "                  unzip_inpi, \n",
    "                  unzip_insee, \n",
    "                  list_weights as list_weights_inpi \n",
    "                FROM \n",
    "                  (\n",
    "                    SELECT \n",
    "                    row_id,\n",
    "                      index_id, \n",
    "                      status_cas, \n",
    "                      inpi_except, \n",
    "                      insee_except,\n",
    "                      len_inpi_except,\n",
    "                      len_insee_except,\n",
    "                      unzip.field0 as unzip_inpi, \n",
    "                      unzip.field1 as insee, \n",
    "                      test \n",
    "                    FROM \n",
    "                      max_data CROSS \n",
    "                      JOIN UNNEST(test) AS new (unzip)\n",
    "                  ) CROSS \n",
    "                  JOIN UNNEST(insee) as test (unzip_insee) \n",
    "                  LEFT JOIN machine_learning.list_mots_insee_inpi_word2vec_weights ON unzip_inpi = list_mots_insee_inpi_word2vec_weights.words\n",
    "              ) \n",
    "              SELECT \n",
    "              row_id,\n",
    "                index_id, \n",
    "                status_cas, \n",
    "                inpi_except, \n",
    "                insee_except, \n",
    "                unzip_inpi, \n",
    "                unzip_insee,\n",
    "                len_inpi_except,\n",
    "                len_insee_except,\n",
    "                REDUCE(\n",
    "                  zip_with(\n",
    "                    list_weights_inpi, \n",
    "                    list_weights_insee, \n",
    "                    (x, y) -> x * y\n",
    "                  ), \n",
    "                  CAST(\n",
    "                    ROW(0.0) AS ROW(sum DOUBLE)\n",
    "                  ), \n",
    "                  (s, x) -> CAST(\n",
    "                    ROW(x + s.sum) AS ROW(sum DOUBLE)\n",
    "                  ), \n",
    "                  s -> s.sum\n",
    "                ) / (\n",
    "                  SQRT(\n",
    "                    REDUCE(\n",
    "                      transform(\n",
    "                        list_weights_inpi, \n",
    "                        (x) -> POW(x, 2)\n",
    "                      ), \n",
    "                      CAST(\n",
    "                        ROW(0.0) AS ROW(sum DOUBLE)\n",
    "                      ), \n",
    "                      (s, x) -> CAST(\n",
    "                        ROW(x + s.sum) AS ROW(sum DOUBLE)\n",
    "                      ), \n",
    "                      s -> s.sum\n",
    "                    )\n",
    "                  ) * SQRT(\n",
    "                    REDUCE(\n",
    "                      transform(\n",
    "                        list_weights_insee, \n",
    "                        (x) -> POW(x, 2)\n",
    "                      ), \n",
    "                      CAST(\n",
    "                        ROW(0.0) AS ROW(sum DOUBLE)\n",
    "                      ), \n",
    "                      (s, x) -> CAST(\n",
    "                        ROW(x + s.sum) AS ROW(sum DOUBLE)\n",
    "                      ), \n",
    "                      s -> s.sum\n",
    "                    )\n",
    "                  )\n",
    "                ) AS cosine_distance \n",
    "              FROM \n",
    "                (\n",
    "                  SELECT \n",
    "                  row_id,\n",
    "                    index_id, \n",
    "                    status_cas, \n",
    "                    inpi_except, \n",
    "                    insee_except, \n",
    "                    unzip_inpi, \n",
    "                    unzip_insee,\n",
    "                    len_inpi_except,\n",
    "                    len_insee_except,\n",
    "                    list_weights_inpi, \n",
    "                    list_weights as list_weights_insee \n",
    "                  FROM \n",
    "                    weight_inpi \n",
    "                    LEFT JOIN machine_learning.list_mots_insee_inpi_word2vec_weights ON unzip_insee = list_mots_insee_inpi_word2vec_weights.words\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    ) \n",
    "    SELECT \n",
    "      row_id,\n",
    "      test.index_id, \n",
    "      test.status_cas, \n",
    "      test.unzip_inpi, \n",
    "      unzip_insee,\n",
    "      len_inpi_except,\n",
    "       len_insee_except,\n",
    "      max_cosine_distance,\n",
    "    count_found\n",
    "    FROM \n",
    "      test \n",
    "      INNER JOIN (\n",
    "        SELECT \n",
    "          index_id, \n",
    "          status_cas, \n",
    "          unzip_inpi, \n",
    "          MAX(cosine_distance) as max_cosine_distance \n",
    "        FROM \n",
    "          test \n",
    "        GROUP BY \n",
    "          index_id, \n",
    "          status_cas, \n",
    "          unzip_inpi\n",
    "      ) AS max_cosine_by_group ON test.index_id = max_cosine_by_group.index_id \n",
    "      AND test.status_cas = max_cosine_by_group.status_cas \n",
    "      AND test.unzip_inpi = max_cosine_by_group.unzip_inpi \n",
    "      AND test.cosine_distance = max_cosine_by_group.max_cosine_distance\n",
    "      LEFT JOIN (\n",
    "        SELECT \n",
    "          index_id, \n",
    "          status_cas, \n",
    "          COUNT(*) as count_found \n",
    "        FROM \n",
    "          (\n",
    "            SELECT \n",
    "          index_id, \n",
    "          status_cas, \n",
    "          unzip_inpi, \n",
    "          MAX(cosine_distance) as max_cosine_distance \n",
    "        FROM \n",
    "          test \n",
    "        GROUP BY \n",
    "          index_id, \n",
    "          status_cas, \n",
    "          unzip_inpi\n",
    "      ) \n",
    "        WHERE max_cosine_distance >= .6\n",
    "        GROUP BY index_id, status_cas\n",
    "      ) AS above_threshold \n",
    "      ON test.index_id = above_threshold.index_id \n",
    "      AND test.status_cas = above_threshold.status_cas \n",
    "  )\n",
    "\"\"\"\n",
    "output = athena.run_query(\n",
    "        query=query,\n",
    "        database='inpi',\n",
    "        s3_output='INPI/sql_output'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul decile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT approx_percentile(\n",
    "  max_cosine_distance, ARRAY[\n",
    "    0.25,\n",
    "    0.50,\n",
    "    0.60,\n",
    "    0.70,\n",
    "    0.75,\n",
    "    0.80,\n",
    "    0.85,\n",
    "    0.95,\n",
    "    0.99]\n",
    "  )\n",
    "  FROM ets_inpi_insee_word2vec \n",
    "\"\"\"\n",
    "#[0.15966796874999997, 0.4188717631017009, 0.5195312499999999,\n",
    "#0.6388182005831299, 0.6618696474706767, 0.725784156601784, 0.769106062989077, 0.8180236405537482, 0.8996922673858876]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribtution combinaison \n",
    "\n",
    "Lorsqu'il y a des mots qui n'ont aucun rapport, mais avec à la fois un cosine élevée et un count unique, c'est du aux entreprises avec beaucoup de doublons dans la base, engendrant ainsi une augmentation artificielle du count lors de l'entrainement du modèle. Il faudrait tuner le modèle différement et voir l'impact sur ce genre de situation, voir modifier le jeu d'entrainement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_combinaison(from_ = .6, to_ = .7):\n",
    "    query = \"\"\"\n",
    "    SELECT combinaison, COUNT(combinaison) as count_combinaison, AVG(max_cosine_distance) avg_cosine\n",
    "    FROM (\n",
    "    SELECT CONCAT(ARRAY[unzip_inpi],ARRAY[unzip_insee]) as combinaison, max_cosine_distance  \n",
    "    FROM ets_inpi_insee_word2vec \n",
    "    WHERE max_cosine_distance >= {0} AND max_cosine_distance < {1}\n",
    "      )\n",
    "      GROUP BY combinaison\n",
    "      ORDER BY count_combinaison DESC\n",
    "\n",
    "    \"\"\".format(from_, to_)\n",
    "\n",
    "    output = athena.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output='INPI/sql_output'\n",
    "        )\n",
    "\n",
    "    results = False\n",
    "    filename = 'combinaison_word2vec_{0}_{1}.csv'.format(from_, to_)\n",
    "\n",
    "    while results != True:\n",
    "        source_key = \"{}/{}.csv\".format(\n",
    "                                'INPI/sql_output',\n",
    "                                output['QueryExecutionId']\n",
    "                                       )\n",
    "        destination_key = \"{}/{}\".format(\n",
    "                                    'MACHINE_LEARNING/NLP/RESULTAT_WORD2VEC',\n",
    "                                    filename\n",
    "                                )\n",
    "\n",
    "        results = s3.copy_object_s3(\n",
    "                                    source_key = source_key,\n",
    "                                    destination_key = destination_key,\n",
    "                                    remove = True\n",
    "                                )\n",
    "        \n",
    "    test_1 = (s3.read_df_from_s3(\n",
    "            key = 'MACHINE_LEARNING/NLP/RESULTAT_WORD2VEC/{}'.format(filename), sep = ',')\n",
    "              .assign(\n",
    "                  pct_total = lambda x: x['count_combinaison']/x['count_combinaison'].sum(),\n",
    "                  pct_total_cum = lambda x: x['pct_total'].cumsum(),\n",
    "              \n",
    "              )\n",
    "             )\n",
    "    \n",
    "    return test_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine de .6 a .95\n",
    "\n",
    "- 70 pairs regroupe environ 75% des lignes avec test lignes et cas 5 à 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = count_combinaison(from_ = .6, to_ = .95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.loc[lambda x: x['pct_total']>.00001]#.to_excel('test.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine De .6 a .7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_combinaison(from_ = .6, to_ = .65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine De .65 a .7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_combinaison(from_ = .65, to_ = .7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine De .7 a .75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_combinaison(from_ = .7, to_ = .75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine De .75 a .8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_combinaison(from_ = .75, to_ = .8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine De .8 a .85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_combinaison(from_ = .8, to_ = .85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine De .85 a .9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_combinaison(from_ = .85, to_ = .9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_combinaison(from_ = .9, to_ = .95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse duplicate\n",
    "\n",
    "Pour trouver une méthode de séparation lorsque l'`index_id` a de nombreuses possibilités, nous pouvons regarder les duplicates. \n",
    "\n",
    "Dans un premier temps, on va compter le nombre d'index par nombre de combinaison possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compte index par combinaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT count_id AS combinaison, COUNT(count_id) as count_duplicate\n",
    "FROM (\n",
    "SELECT index_id, COUNT(*) as count_id\n",
    "FROM ets_inpi_insee_word2vec \n",
    "GROUP BY index_id\n",
    ")\n",
    "GROUP BY count_id\n",
    "ORDER BY count_duplicate DESC, combinaison\n",
    "\"\"\"\n",
    "output = athena.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output='INPI/sql_output'\n",
    "        )\n",
    "\n",
    "results = False\n",
    "filename = 'duplicate_index_combinaison_word2vec.csv'\n",
    "\n",
    "while results != True:\n",
    "    source_key = \"{}/{}.csv\".format(\n",
    "                                'INPI/sql_output',\n",
    "                                output['QueryExecutionId']\n",
    "                                       )\n",
    "    destination_key = \"{}/{}\".format(\n",
    "                                    'MACHINE_LEARNING/NLP/RESULTAT_WORD2VEC',\n",
    "                                    filename\n",
    "                                )\n",
    "\n",
    "    results = s3.copy_object_s3(\n",
    "                                    source_key = source_key,\n",
    "                                    destination_key = destination_key,\n",
    "                                    remove = True\n",
    "                                )\n",
    "        \n",
    "test_1 = (s3.read_df_from_s3(\n",
    "            key = 'MACHINE_LEARNING/NLP/RESULTAT_WORD2VEC/{}'.format(filename), sep = ',')\n",
    "              .assign(\n",
    "                  pct_total = lambda x: x['count_duplicate']/x['count_duplicate'].sum(),\n",
    "                  pct_total_cum = lambda x: x['pct_total'].cumsum(),\n",
    "              \n",
    "              )\n",
    "             )\n",
    "test_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse des cas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT status_cas, approx_percentile(\n",
    "  pct_found, ARRAY[\n",
    "    0.25,\n",
    "    0.50,\n",
    "    0.60,\n",
    "    0.70,\n",
    "    0.75,\n",
    "    0.80,\n",
    "    0.85,\n",
    "    0.95,\n",
    "    0.99]\n",
    "  ) as avg_pct_found_cas\n",
    "FROM (\n",
    "SELECT row_id, index_id, status_cas, AVG(pct_found) as pct_found\n",
    "FROM(\n",
    "SELECT *, \n",
    "CASE WHEN count_found IS NULL THEN 0 ELSE\n",
    "cast(count_found as decimal(7,2))/ cast(len_insee_except as decimal(7,2)) END AS pct_found \n",
    "FROM \"inpi\".\"ets_inpi_insee_word2vec\"\n",
    ")\n",
    "GROUP BY row_id, index_id, status_cas\n",
    ")\n",
    "GROUP BY status_cas\n",
    "\"\"\"\n",
    "\n",
    "output = athena.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output='INPI/sql_output'\n",
    "        )\n",
    "\n",
    "results = False\n",
    "filename = 'distribution_pct_combinaison_word2vec.csv'\n",
    "\n",
    "while results != True:\n",
    "    source_key = \"{}/{}.csv\".format(\n",
    "                                'INPI/sql_output',\n",
    "                                output['QueryExecutionId']\n",
    "                                       )\n",
    "    destination_key = \"{}/{}\".format(\n",
    "                                    'MACHINE_LEARNING/NLP/RESULTAT_WORD2VEC',\n",
    "                                    filename\n",
    "                                )\n",
    "\n",
    "    results = s3.copy_object_s3(\n",
    "                                    source_key = source_key,\n",
    "                                    destination_key = destination_key,\n",
    "                                    remove = True\n",
    "                                )\n",
    "    \n",
    "test_1 = (s3.read_df_from_s3(\n",
    "            key = 'MACHINE_LEARNING/NLP/RESULTAT_WORD2VEC/{}'.format(filename), sep = ',')\n",
    "         )\n",
    "test_1\n",
    "#[0.25,0.50,0.60,0.70,0.75,0.80,0.85,0.95,0.99]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple de relation, overfitting? Dans cette adresse, il y a deux mots, ALLUES et MUSILLON. Le machine a trouvé une similarité de .78 (très élevé). Ca m'a paru bizare. J'ai regardé sur Google, en fait, c'est un chalet à Méribel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM ets_inpi_insee_word2vec  \n",
    "WHERE index_id = 423494\n",
    "\"\"\"\n",
    "\n",
    "output = athena.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output='INPI/sql_output'\n",
    "        )\n",
    "\n",
    "results = False\n",
    "filename = 'index_423494_combinaison_word2vec.csv'\n",
    "\n",
    "while results != True:\n",
    "    source_key = \"{}/{}.csv\".format(\n",
    "                                'INPI/sql_output',\n",
    "                                output['QueryExecutionId']\n",
    "                                       )\n",
    "    destination_key = \"{}/{}\".format(\n",
    "                                    'MACHINE_LEARNING/NLP/RESULTAT_WORD2VEC',\n",
    "                                    filename\n",
    "                                )\n",
    "\n",
    "    results = s3.copy_object_s3(\n",
    "                                    source_key = source_key,\n",
    "                                    destination_key = destination_key,\n",
    "                                    remove = True\n",
    "                                )\n",
    "    \n",
    "test_1 = (s3.read_df_from_s3(\n",
    "            key = 'MACHINE_LEARNING/NLP/RESULTAT_WORD2VEC/{}'.format(filename), sep = ',')\n",
    "         )\n",
    "test_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count nombre de test si pct_found supérieur à .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT status_cas, test_exception, COUNT(*) as count_test_exception\n",
    "FROM(\n",
    "SELECT row_id, index_id, status_cas, pct_found, CASE WHEN pct_found >=.5 THEN 'True' ELSE 'False' END as test_exception\n",
    "FROM (\n",
    "SELECT row_id, index_id, status_cas, AVG(pct_found) as pct_found\n",
    "FROM(\n",
    "SELECT *, \n",
    "CASE WHEN count_found IS NULL THEN 0 ELSE\n",
    "cast(count_found as decimal(7,2))/ cast(len_insee_except as decimal(7,2)) END AS pct_found \n",
    "FROM \"inpi\".\"ets_inpi_insee_word2vec\"\n",
    ")\n",
    "GROUP BY row_id, index_id, status_cas\n",
    ")\n",
    "  )\n",
    "  GROUP BY status_cas, test_exception\n",
    "  ORDER BY status_cas, test_exception\n",
    "\n",
    "\"\"\"\n",
    "output = athena.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output='INPI/sql_output'\n",
    "        )\n",
    "\n",
    "results = False\n",
    "filename = 'resultat_test_combinaison_word2vec.csv'\n",
    "\n",
    "while results != True:\n",
    "    source_key = \"{}/{}.csv\".format(\n",
    "                                'INPI/sql_output',\n",
    "                                output['QueryExecutionId']\n",
    "                                       )\n",
    "    destination_key = \"{}/{}\".format(\n",
    "                                    'MACHINE_LEARNING/NLP/RESULTAT_WORD2VEC',\n",
    "                                    filename\n",
    "                                )\n",
    "\n",
    "    results = s3.copy_object_s3(\n",
    "                                    source_key = source_key,\n",
    "                                    destination_key = destination_key,\n",
    "                                    remove = True\n",
    "                                )\n",
    "    \n",
    "test_1 = (s3.read_df_from_s3(\n",
    "            key = 'MACHINE_LEARNING/NLP/RESULTAT_WORD2VEC/{}'.format(filename), sep = ',')\n",
    "         )\n",
    "(\n",
    "    test_1.set_index(['status_cas', 'test_exception']).unstack(-1)\n",
    "    .assign(cumsum_true = lambda x: x[('count_test_exception',True)].cumsum())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, shutil, urllib, ipykernel, json\n",
    "from pathlib import Path\n",
    "from notebook import notebookapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_report(extension = \"html\"):\n",
    "    \"\"\"\n",
    "    Create a report from the current notebook and save it in the \n",
    "    Report folder (Parent-> child directory)\n",
    "    \n",
    "    1. Exctract the current notbook name\n",
    "    2. Convert the Notebook \n",
    "    3. Move the newly created report\n",
    "    \n",
    "    Args:\n",
    "    extension: string. Can be \"html\", \"pdf\", \"md\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ### Get notebook name\n",
    "    connection_file = os.path.basename(ipykernel.get_connection_file())\n",
    "    kernel_id = connection_file.split('-', 1)[0].split('.')[0]\n",
    "\n",
    "    for srv in notebookapp.list_running_servers():\n",
    "        try:\n",
    "            if srv['token']=='' and not srv['password']:  \n",
    "                req = urllib.request.urlopen(srv['url']+'api/sessions')\n",
    "            else:\n",
    "                req = urllib.request.urlopen(srv['url']+ \\\n",
    "                                             'api/sessions?token=' + \\\n",
    "                                             srv['token'])\n",
    "            sessions = json.load(req)\n",
    "            notebookname = sessions[0]['name']\n",
    "        except:\n",
    "            pass  \n",
    "    \n",
    "    sep = '.'\n",
    "    path = os.getcwd()\n",
    "    #parent_path = str(Path(path).parent)\n",
    "    \n",
    "    ### Path report\n",
    "    #path_report = \"{}/Reports\".format(parent_path)\n",
    "    #path_report = \"{}/Reports\".format(path)\n",
    "    \n",
    "    ### Path destination\n",
    "    name_no_extension = notebookname.split(sep, 1)[0]\n",
    "    source_to_move = name_no_extension +'.{}'.format(extension)\n",
    "    dest = os.path.join(path,'Reports', source_to_move)\n",
    "    \n",
    "    ### Generate notebook\n",
    "    os.system('jupyter nbconvert --no-input --to {} {}'.format(\n",
    "    extension,notebookname))\n",
    "    \n",
    "    ### Move notebook to report folder\n",
    "    #time.sleep(5)\n",
    "    shutil.move(source_to_move, dest)\n",
    "    print(\"Report Available at this adress:\\n {}\".format(dest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_report(extension = \"html\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
