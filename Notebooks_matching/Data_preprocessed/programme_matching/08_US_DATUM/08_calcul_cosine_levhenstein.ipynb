{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcul de la distance de Cosine et Levhenstein\n",
    "\n",
    "Copy paste from Coda to fill the information\n",
    "\n",
    "## Objective(s)\n",
    "\n",
    "- Dans l'US, Création table poids obtenus via le Word2Vec, nous avons préparé une table avec la liste des mots les plus récurants dans la base d'entrainement avec les poids rattachées. Dans cette nouvelle étape, nous devons calculer la similarité entre les mots qui ne sont pas identiques dans l'adresse de l'INSEE et de l'INPI.\n",
    "- Lors de l'US, Creation table inpi insee contenant le test `status_cas` a effectuer pour dedoublonner les lignes, nous avons créé deux variables, `list_excep_insee` et `list_except_inpi` qui représentent les mots qui ne sont pas identiques.\n",
    "- Lors de l'US, Creation table merge INSEE INPI filtree, nous avons créé la variable `row_id`, qui va nous permettre de rajouter les variables suivantes a la table des cas\n",
    "\n",
    "La siretisation repose sur une matrice de règles de gestion classée de manière ordonnée. Pour créer la matrice, il faut au préalable créer les variables nécéssaires à la création des tests. \n",
    "\n",
    "Le tableau ci dessous indique l'ensemble des tests a réaliser ainsi que leur dépendence.\n",
    "\n",
    "| Rang | Nom_variable                              | Dependence                                    | Notebook                           | Difficulte | Table_input                                                                                                                                                            | Variables_crees_US                                                                 | Possibilites                  |\n",
    "|------|-------------------------------------------|-----------------------------------------------|------------------------------------|------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------|-------------------------------|\n",
    "| 1    | status_cas                                |                                               | 02_cas_de_figure                   | Moyen      | ets_insee_inpi_status_cas                                                                                                                                              | status_cas,intersection,pct_intersection,union_,inpi_except,insee_except           | CAS_1,CAS_2,CAS_3,CAS_4,CAS_5 |\n",
    "| 2    | test_list_num_voie                        | intersection_numero_voie,union_numero_voie    | 03_test_list_num_voie              | Moyen      | ets_insee_inpi_list_num_voie                                                                                                                                           | intersection_numero_voie,union_numero_voie                                         | FALSE,NULL,TRUE,PARTIAL       |\n",
    "| 3    | test_enseigne                             | list_enseigne,enseigne                        | 04_test_enseigne                   | Moyen      | ets_insee_inpi_list_enseigne                                                                                                                                           | list_enseigne_contain                                                              | FALSE,NULL,TRUE               |\n",
    "| 4    | test_pct_intersection                     | pct_intersection,index_id_max_intersection    | 06_creation_nb_siret_siren_max_pct | Facile     | ets_insee_inpi_var_group_max                                                                                                                                           | count_inpi_index_id_siret,count_inpi_siren_siret,index_id_max_intersection         | FALSE,TRUE                    |\n",
    "| 4    | test_index_id_duplicate                   | count_inpi_index_id_siret                     | 06_creation_nb_siret_siren_max_pct | Facile     | ets_insee_inpi_var_group_max                                                                                                                                           | count_inpi_index_id_siret,count_inpi_siren_siret,index_id_max_intersection         | FALSE,TRUE                    |\n",
    "| 4    | test_siren_insee_siren_inpi               | count_initial_insee,count_inpi_siren_siret    | 06_creation_nb_siret_siren_max_pct | Facile     | ets_insee_inpi_var_group_max                                                                                                                                           | count_inpi_index_id_siret,count_inpi_siren_siret,index_id_max_intersection         | FALSE,TRUE                    |\n",
    "| 5    | test_similarite_exception_words           | max_cosine_distance                           | 08_calcul_cosine_levhenstein       | Difficile  | ets_insee_inpi_similarite_max_word2vec                                                                                                                                 | unzip_inpi,unzip_insee,max_cosine_distance,levenshtein_distance,key_except_to_test | FALSE,NULL,TRUE               |\n",
    "| 5    | test_distance_levhenstein_exception_words | levenshtein_distance                          | 08_calcul_cosine_levhenstein       | Difficile  | ets_insee_inpi_similarite_max_word2vec                                                                                                                                 | unzip_inpi,unzip_insee,max_cosine_distance,levenshtein_distance,key_except_to_test | FALSE,NULL,TRUE               |\n",
    "| 6    | test_date                                 | datecreationetablissement,date_debut_activite | 10_match_et_creation_regles.md     | Facile     | ets_insee_inpi_list_num_voie,ets_insee_inpi_list_enseigne,ets_insee_inpi_similarite_max_word2vec,ets_insee_inpi_status_cas,ets_insee_inpi_var_group_max,ets_insee_inpi |                                                                                    | FALSE,TRUE                    |\n",
    "| 6    | test_siege                                | status_ets,etablissementsiege                 | 10_match_et_creation_regles.md     | Facile     | ets_insee_inpi_list_num_voie,ets_insee_inpi_list_enseigne,ets_insee_inpi_similarite_max_word2vec,ets_insee_inpi_status_cas,ets_insee_inpi_var_group_max,ets_insee_inpi |                                                                                    | FALSE,TRUE,NULL               |\n",
    "| 6    | test_status_admin                         | etatadministratifetablissement,status_admin   | 10_match_et_creation_regles.md     | Facile     | ets_insee_inpi_list_num_voie,ets_insee_inpi_list_enseigne,ets_insee_inpi_similarite_max_word2vec,ets_insee_inpi_status_cas,ets_insee_inpi_var_group_max,ets_insee_inpi |                                                                                    | FALSE,NULL,TRUE               |\n",
    "\n",
    "Lors de cette US, nous allons créer 6 variables qui vont permettre a la réalisation des tests `test_similarite_exception_words` et `test_distance_levhenstein_exception_words`. Les six variables sont les suivantes:\n",
    "\n",
    "- `unzip_inpi`: Mot comparé coté inpi\n",
    "- `unzip_insee`: Mot comparé coté insee\n",
    "- `max_cosine_distance`: Score de similarité entre le mot compaté coté inpi et coté insee\n",
    "- `levenshtein_distance`: Nombre d'édition qu'il faut réaliser pour arriver à reproduire les deux mots\n",
    "- `key_except_to_test`: Champs clé-valeur pour toutes les possibiltés des mots qui ne sont pas en communs entre l'insee et l'inpi\n",
    "* Il faut penser a garder la variable `row_id` \n",
    "- La similarité doit etre calculée sur l'ensemble des éléments non communs, puis il faut récupérer la distance la plus élevée.\n",
    "\n",
    "## Metadata \n",
    "\n",
    "* Metadata parameters are available here: \n",
    "* US Title: Calcul de la distance de Cosine et Levhenstein\n",
    "* Epic: Epic 8\n",
    "* US: US 8\n",
    "* Date Begin: 9/8/2020\n",
    "* Duration Task: 0\n",
    "* Status:  \n",
    "* Source URL: [US 08 PreparationWord2Vec](https://coda.io/d/_dCtnoqIftTn/US-08-PreparationWord2Vec_su_Xz)\n",
    "* Task type:\n",
    "  * Jupyter Notebook\n",
    "* Users: :\n",
    "  * Thomas Pernet\n",
    "* Watchers:\n",
    "  * Thomas Pernet\n",
    "* Estimated Log points:\n",
    "  * One being a simple task, 15 a very difficult one\n",
    "  *  5\n",
    "* Task tag\n",
    "  *  #computation,#sql-query,#machine-learning,#word2vec,#similarite,#preparation-similarite\n",
    "* Toggl Tag\n",
    "  * #data-preparation\n",
    "  \n",
    "## Input Cloud Storage [AWS]\n",
    "\n",
    "If link from the internet, save it to the cloud first\n",
    "\n",
    "### Tables [AWS]\n",
    "\n",
    "1. Batch 1:\n",
    "  * Select Provider: Athena\n",
    "  * Select table(s): ets_insee_inpi_cases,list_weight_mots_insee_inpi_word2vec,ets_insee_inpi_status_cas\n",
    "    * Select only tables created from the same notebook, else copy/paste selection to add new input tables\n",
    "    * If table(s) does not exist, add them: Add New Table\n",
    "    * Information:\n",
    "      * Region: \n",
    "        * NameEurope (Paris)\n",
    "        * Code: eu-west-3\n",
    "      * Database: siretisation\n",
    "      * Notebook construction file: \n",
    "        * 05_creation_table_cases\n",
    "        * 07_creation_table_poids_Word2Vec\n",
    "        * 02_cas_de_figure\n",
    "    \n",
    "## Destination Output/Delivery\n",
    "\n",
    "1. AWS\n",
    "    1. Athena: \n",
    "      * Region: Europe (Paris)\n",
    "      * Database: siretisation\n",
    "      * Tables (Add name new table): ets_inpi_similarite_max_word2vec\n",
    "      * List new tables\n",
    "      * ets_inpi_similarite_max_word2vec\n",
    "\n",
    "## Things to know (Steps, Attention points or new flow of information)\n",
    "\n",
    "### Sources of information  (meeting notes, Documentation, Query, URL)\n",
    "\n",
    "Sources of information  (meeting notes, Documentation, Query, URL)\n",
    "1. Jupyter Notebook (Github Link)\n",
    "  1. md : [07_creation_table_poids_Word2Vec.md](https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/08_US_DATUM/07_creation_table_poids_Word2Vec.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connexion serveur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsPy.aws_authorization import aws_connector\n",
    "from awsPy.aws_athena import service_athena\n",
    "from awsPy.aws_s3 import service_s3\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os, shutil\n",
    "\n",
    "path = os.getcwd()\n",
    "parent_path = str(Path(path).parent)\n",
    "path_cred = r\"{}/credential_AWS.json\".format(parent_path)\n",
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                       region = 'eu-west-3')\n",
    "\n",
    "region = 'eu-west-3'\n",
    "bucket = 'calfdata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                       region = region)\n",
    "client= con.client_boto()\n",
    "s3 = service_s3.connect_S3(client = client,\n",
    "                      bucket = bucket, verbose = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_setting = True\n",
    "if pandas_setting:\n",
    "    cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output = 'inpi/sql_output'\n",
    "database = 'siretisation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "DROP TABLE siretisation.ets_inpi_similarite_max_word2vec;\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database=database,\n",
    "            s3_output=s3_output,\n",
    "  filename = None, ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "CREATE TABLE siretisation.ets_inpi_similarite_max_word2vec\n",
    "WITH (\n",
    "  format='PARQUET'\n",
    ") AS\n",
    "WITH dataset AS (\n",
    "  SELECT \n",
    "    siretisation.ets_insee_inpi.row_id, \n",
    "    index_id, \n",
    "    status_cas, \n",
    "    inpi_except, \n",
    "    insee_except, \n",
    "    transform(\n",
    "      sequence(\n",
    "        1, \n",
    "        CARDINALITY(insee_except)\n",
    "      ), \n",
    "      x -> insee_except\n",
    "    ), \n",
    "    ZIP(\n",
    "      inpi_except, \n",
    "      transform(\n",
    "        sequence(\n",
    "          1, \n",
    "          CARDINALITY(inpi_except)\n",
    "        ), \n",
    "        x -> insee_except\n",
    "      )\n",
    "    ) as test \n",
    "  FROM \n",
    "    siretisation.ets_insee_inpi  \n",
    "    \n",
    "  LEFT JOIN siretisation.ets_insee_inpi_status_cas \n",
    "  ON siretisation.ets_insee_inpi.row_id = siretisation.ets_insee_inpi_status_cas.row_id\n",
    "  where \n",
    "    (status_cas != 'CAS_2' AND CARDINALITY(inpi_except)  > 0 AND CARDINALITY(insee_except) > 0)\n",
    "  )\n",
    " SELECT \n",
    "  * \n",
    "FROM \n",
    "  (\n",
    "    WITH distance AS (\n",
    "      SELECT \n",
    "        * \n",
    "      FROM \n",
    "        (\n",
    "          WITH list_weights_insee_inpi AS (\n",
    "            SELECT \n",
    "              row_id, \n",
    "              index_id, \n",
    "              status_cas, \n",
    "              inpi_except, \n",
    "              insee_except, \n",
    "              unzip_inpi, \n",
    "              unzip_insee, \n",
    "              list_weights_inpi, \n",
    "              list_weights_insee \n",
    "            FROM \n",
    "              (\n",
    "                SELECT \n",
    "                  row_id, \n",
    "                  index_id, \n",
    "                  status_cas, \n",
    "                  inpi_except, \n",
    "                  insee_except, \n",
    "                  unzip.field0 as unzip_inpi, \n",
    "                  unzip.field1 as insee, \n",
    "                  test \n",
    "                FROM \n",
    "                  dataset CROSS \n",
    "                  JOIN UNNEST(test) AS new (unzip)\n",
    "              ) CROSS \n",
    "              JOIN UNNEST(insee) as test (unzip_insee) \n",
    "              LEFT JOIN (\n",
    "                SELECT \n",
    "                  words, \n",
    "                  list_weights as list_weights_inpi \n",
    "                FROM \n",
    "                  siretisation.list_weight_mots_insee_inpi_word2vec \n",
    "              ) tb_weight_inpi ON unzip_inpi = tb_weight_inpi.words \n",
    "              LEFT JOIN (\n",
    "                SELECT \n",
    "                  words, \n",
    "                  list_weights as list_weights_insee \n",
    "                FROM \n",
    "                  siretisation.list_weight_mots_insee_inpi_word2vec \n",
    "              ) tb_weight_insee ON unzip_insee = tb_weight_insee.words \n",
    "          ) \n",
    "          SELECT \n",
    "            row_id, \n",
    "            index_id, \n",
    "            status_cas, \n",
    "            inpi_except, \n",
    "            insee_except, \n",
    "            unzip_inpi, \n",
    "            unzip_insee, \n",
    "            REDUCE(\n",
    "              zip_with(\n",
    "                list_weights_inpi, \n",
    "                list_weights_insee, \n",
    "                (x, y) -> x * y\n",
    "              ), \n",
    "              CAST(\n",
    "                ROW(0.0) AS ROW(sum DOUBLE)\n",
    "              ), \n",
    "              (s, x) -> CAST(\n",
    "                ROW(x + s.sum) AS ROW(sum DOUBLE)\n",
    "              ), \n",
    "              s -> s.sum\n",
    "            ) / (\n",
    "              SQRT(\n",
    "                REDUCE(\n",
    "                  transform(\n",
    "                    list_weights_inpi, \n",
    "                    (x) -> POW(x, 2)\n",
    "                  ), \n",
    "                  CAST(\n",
    "                    ROW(0.0) AS ROW(sum DOUBLE)\n",
    "                  ), \n",
    "                  (s, x) -> CAST(\n",
    "                    ROW(x + s.sum) AS ROW(sum DOUBLE)\n",
    "                  ), \n",
    "                  s -> s.sum\n",
    "                )\n",
    "              ) * SQRT(\n",
    "                REDUCE(\n",
    "                  transform(\n",
    "                    list_weights_insee, \n",
    "                    (x) -> POW(x, 2)\n",
    "                  ), \n",
    "                  CAST(\n",
    "                    ROW(0.0) AS ROW(sum DOUBLE)\n",
    "                  ), \n",
    "                  (s, x) -> CAST(\n",
    "                    ROW(x + s.sum) AS ROW(sum DOUBLE)\n",
    "                  ), \n",
    "                  s -> s.sum\n",
    "                )\n",
    "              )\n",
    "            ) AS cosine_distance \n",
    "          FROM \n",
    "            list_weights_insee_inpi\n",
    "        )\n",
    "    ) \n",
    "    SELECT \n",
    "      row_id, \n",
    "      dataset.index_id, \n",
    "      inpi_except, \n",
    "      insee_except, \n",
    "      unzip_inpi, \n",
    "      unzip_insee, \n",
    "      max_cosine_distance,\n",
    "      -- CASE WHEN max_cosine_distance >= .6 THEN 'TRUE' ELSE 'FALSE' END AS test_distance_cosine,\n",
    "      test as key_except_to_test,\n",
    "      levenshtein_distance(unzip_inpi, unzip_insee) AS levenshtein_distance\n",
    "      -- CASE WHEN levenshtein_distance(unzip_inpi, unzip_insee) <=1  THEN 'TRUE' ELSE 'FALSE' END AS test_distance_levhenstein\n",
    "    \n",
    "    FROM \n",
    "      dataset \n",
    "      LEFT JOIN (\n",
    "        SELECT \n",
    "          distance.index_id, \n",
    "          unzip_inpi, \n",
    "          unzip_insee, \n",
    "          max_cosine_distance \n",
    "        FROM \n",
    "          distance \n",
    "          RIGHT JOIN (\n",
    "            SELECT \n",
    "              index_id, \n",
    "              MAX(cosine_distance) as max_cosine_distance \n",
    "            FROM \n",
    "              distance \n",
    "            GROUP BY \n",
    "              index_id\n",
    "          ) as tb_max_distance ON distance.index_id = tb_max_distance.index_id \n",
    "          AND distance.cosine_distance = tb_max_distance.max_cosine_distance\n",
    "      ) as tb_max_distance_lookup ON dataset.index_id = tb_max_distance_lookup.index_id\n",
    "  )\n",
    "\"\"\"\n",
    "\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database=database,\n",
    "            s3_output=s3_output,\n",
    "  filename = None, ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pas a pas\n",
    "\n",
    "Pour récupérer la similarité la plus élevée entre les mots qui ne sont pas communs entre l'adresse de l'INSEE et de l'INPI, il faut suivre plusieurs étapes. Les étapes sont les suivantes:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. filtre et creation ensemble des similarités a calculer\n",
    "\n",
    "- Filtrer les lignes qui ne correspondent pas au cas 2 et qui ont une cardinalité des mots `except` supérieure à 0. Effectivement, il n'est pas nécéssaire de calculer une similarité si l'une des listes, insee ou inpi, est vide.\n",
    "- Création d'un champ clé valeur qui indique l'ensemble des similarités a calculer\n",
    "\n",
    "On utilise les fonctions:\n",
    "\n",
    "- `transform`\n",
    "- `ZIP`\n",
    "- `sequence`\n",
    "\n",
    "La difficulté dans cette étape était de trouver un moyen de dupliquer la liste de l'INSEE pour chacune des clés de la liste de l'INPI. Le trick est d'utiliser `sequence` afin de répeter autant de fois la liste de l'INSEE qu'il y a de clé à l'INPI. Plus précisément, si la liste de l'INPI a deux valeurs, ie deux clés, et que la liste de l'INSEE a trois éléments. La taille de l'INSEE n'a pas d'impact, ce qui est important c'est de connaitre la taille de l'INPI. Dans notre exemple, le code va répéter la liste de l'INSEE 2 fois, car il y a deux clés à l'INPI.\n",
    "\n",
    "Exemple concret:\n",
    "\n",
    "- INPI -> [FRERES, AMADEO]\n",
    "- INSEE -> [MARTYRS, RESISTANCE]\n",
    "- Il faut comparer: \n",
    "    - FRERES -> [MARTYRS, RESISTANCE] \n",
    "    - AMADEO -> [MARTYRS, RESISTANCE]\n",
    "- Clé valeur finale -> [\n",
    "{field0=FRERES, field1=[MARTYRS, RESISTANCE]},\n",
    "{field0=AMADEO, field1=[MARTYRS, RESISTANCE]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH dataset AS (\n",
    "  SELECT \n",
    "    siretisation.ets_insee_inpi.row_id, \n",
    "    index_id, \n",
    "    status_cas, \n",
    "    inpi_except, \n",
    "    insee_except, \n",
    "    transform(\n",
    "      sequence(\n",
    "        1, \n",
    "        CARDINALITY(insee_except)\n",
    "      ), \n",
    "      x -> insee_except\n",
    "    ), \n",
    "    ZIP(\n",
    "      inpi_except, \n",
    "      transform(\n",
    "        sequence(\n",
    "          1, \n",
    "          CARDINALITY(inpi_except)\n",
    "        ), \n",
    "        x -> insee_except\n",
    "      )\n",
    "    ) as test \n",
    "  FROM \n",
    "    siretisation.ets_insee_inpi  \n",
    "    \n",
    "  LEFT JOIN siretisation.ets_insee_inpi_status_cas \n",
    "  ON siretisation.ets_insee_inpi.row_id = siretisation.ets_insee_inpi_status_cas.row_id\n",
    "  where \n",
    "    (status_cas != 'CAS_2' AND CARDINALITY(inpi_except)  > 0 AND CARDINALITY(insee_except) > 0 OR index_id = 4664896)\n",
    "  LIMIT 10\n",
    "  )\n",
    "  \n",
    " SELECT \n",
    "  * \n",
    "  FROM dataset\n",
    "\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database=database,\n",
    "            s3_output=s3_output,\n",
    "  filename = 'repeat', ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Produit cartesien possibilité et liste poids\n",
    "\n",
    "Dans la seconde étape, nous allons \"exploser\" la clé-valeur afin de pouvoir attribuler la liste des poids aux mots de l'INPI et de l'INSEE.\n",
    "\n",
    "Nous allons poursuivre le reste du pas à pas avec l'index `4664896`, qui fait référence à l'exemple ci dessus.\n",
    "\n",
    "L'explosion du champs `test` se fait avec la fonction `CROSS JOIN`. La variable `unzip_inpi` correspond aux clés du champs `test` alors que la variable `unzip_insee` correspond aux valeurs. Le `CROSS JOIN` implique 4 lignes au total. \n",
    "\n",
    "Nous avons donc deux colonnes avec les pairs de mots qu'il faut calculer la similarité, et deux colonnes avec les poids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH dataset AS (\n",
    "  SELECT \n",
    "    siretisation.ets_insee_inpi.row_id, \n",
    "    index_id, \n",
    "    status_cas, \n",
    "    inpi_except, \n",
    "    insee_except, \n",
    "    transform(\n",
    "      sequence(\n",
    "        1, \n",
    "        CARDINALITY(insee_except)\n",
    "      ), \n",
    "      x -> insee_except\n",
    "    ), \n",
    "    ZIP(\n",
    "      inpi_except, \n",
    "      transform(\n",
    "        sequence(\n",
    "          1, \n",
    "          CARDINALITY(inpi_except)\n",
    "        ), \n",
    "        x -> insee_except\n",
    "      )\n",
    "    ) as test \n",
    "  FROM \n",
    "    siretisation.ets_insee_inpi  \n",
    "    \n",
    "  LEFT JOIN siretisation.ets_insee_inpi_status_cas \n",
    "  ON siretisation.ets_insee_inpi.row_id = siretisation.ets_insee_inpi_status_cas.row_id\n",
    "  where \n",
    "    (status_cas != 'CAS_2' AND CARDINALITY(inpi_except)  > 0 AND CARDINALITY(insee_except) > 0 AND index_id = 4664896)\n",
    "  \n",
    "  )\n",
    "SELECT \n",
    "              row_id, \n",
    "              index_id, \n",
    "              status_cas, \n",
    "              inpi_except, \n",
    "              insee_except, \n",
    "              unzip_inpi, \n",
    "              unzip_insee, \n",
    "              list_weights_inpi, \n",
    "              list_weights_insee \n",
    "            FROM \n",
    "              (\n",
    "                SELECT \n",
    "                  row_id, \n",
    "                  index_id, \n",
    "                  status_cas, \n",
    "                  inpi_except, \n",
    "                  insee_except, \n",
    "                  unzip.field0 as unzip_inpi, \n",
    "                  unzip.field1 as insee, \n",
    "                  test \n",
    "                FROM \n",
    "                  dataset CROSS \n",
    "                  JOIN UNNEST(test) AS new (unzip)\n",
    "              ) CROSS \n",
    "              JOIN UNNEST(insee) as test (unzip_insee) \n",
    "              LEFT JOIN (\n",
    "                SELECT \n",
    "                  words, \n",
    "                  list_weights as list_weights_inpi \n",
    "                FROM \n",
    "                  siretisation.list_weight_mots_insee_inpi_word2vec \n",
    "              ) tb_weight_inpi ON unzip_inpi = tb_weight_inpi.words \n",
    "              LEFT JOIN (\n",
    "                SELECT \n",
    "                  words, \n",
    "                  list_weights as list_weights_insee \n",
    "                FROM \n",
    "                  siretisation.list_weight_mots_insee_inpi_word2vec \n",
    "              ) tb_weight_insee ON unzip_insee = tb_weight_insee.words \n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database=database,\n",
    "            s3_output=s3_output,\n",
    "  filename = 'explosion', ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Calcul de la similarité\n",
    "\n",
    "Le calcul de la similarité s'effectue avec la distance de cosine. Pour connaitre le pas a pas, veuillez vous référer au notebook [07_creation_table_poids_Word2Vec.md#test-acceptanceanalyse-du-modele](https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/08_US_DATUM/07_creation_table_poids_Word2Vec.md#test-acceptanceanalyse-du-mod%C3%A8le) pour comprendre les étapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query =  \"\"\"\n",
    "WITH dataset AS (\n",
    "  SELECT \n",
    "    siretisation.ets_insee_inpi.row_id, \n",
    "    index_id, \n",
    "    status_cas, \n",
    "    inpi_except, \n",
    "    insee_except, \n",
    "    transform(\n",
    "      sequence(\n",
    "        1, \n",
    "        CARDINALITY(insee_except)\n",
    "      ), \n",
    "      x -> insee_except\n",
    "    ), \n",
    "    ZIP(\n",
    "      inpi_except, \n",
    "      transform(\n",
    "        sequence(\n",
    "          1, \n",
    "          CARDINALITY(inpi_except)\n",
    "        ), \n",
    "        x -> insee_except\n",
    "      )\n",
    "    ) as test \n",
    "  FROM \n",
    "    siretisation.ets_insee_inpi  \n",
    "    \n",
    "  LEFT JOIN siretisation.ets_insee_inpi_status_cas \n",
    "  ON siretisation.ets_insee_inpi.row_id = siretisation.ets_insee_inpi_status_cas.row_id\n",
    "  where \n",
    "    (status_cas != 'CAS_2' AND CARDINALITY(inpi_except)  > 0 AND CARDINALITY(insee_except) > 0 AND index_id = 4664896)\n",
    "  \n",
    "  )\n",
    "  SELECT \n",
    "  * \n",
    "FROM \n",
    "  (\n",
    "    WITH distance AS (\n",
    "SELECT \n",
    "              row_id, \n",
    "              index_id, \n",
    "              status_cas, \n",
    "              inpi_except, \n",
    "              insee_except, \n",
    "              unzip_inpi, \n",
    "              unzip_insee, \n",
    "              list_weights_inpi, \n",
    "              list_weights_insee \n",
    "            FROM \n",
    "              (\n",
    "                SELECT \n",
    "                  row_id, \n",
    "                  index_id, \n",
    "                  status_cas, \n",
    "                  inpi_except, \n",
    "                  insee_except, \n",
    "                  unzip.field0 as unzip_inpi, \n",
    "                  unzip.field1 as insee, \n",
    "                  test \n",
    "                FROM \n",
    "                  dataset CROSS \n",
    "                  JOIN UNNEST(test) AS new (unzip)\n",
    "              ) CROSS \n",
    "              JOIN UNNEST(insee) as test (unzip_insee) \n",
    "              LEFT JOIN (\n",
    "                SELECT \n",
    "                  words, \n",
    "                  list_weights as list_weights_inpi \n",
    "                FROM \n",
    "                  siretisation.list_weight_mots_insee_inpi_word2vec \n",
    "              ) tb_weight_inpi ON unzip_inpi = tb_weight_inpi.words \n",
    "              LEFT JOIN (\n",
    "                SELECT \n",
    "                  words, \n",
    "                  list_weights as list_weights_insee \n",
    "                FROM \n",
    "                  siretisation.list_weight_mots_insee_inpi_word2vec \n",
    "              ) tb_weight_insee ON unzip_insee = tb_weight_insee.words \n",
    "      )\n",
    "    SELECT row_id, \n",
    "            index_id, \n",
    "            status_cas, \n",
    "            inpi_except, \n",
    "            insee_except, \n",
    "            unzip_inpi, \n",
    "            unzip_insee, \n",
    "            REDUCE(\n",
    "              zip_with(\n",
    "                list_weights_inpi, \n",
    "                list_weights_insee, \n",
    "                (x, y) -> x * y\n",
    "              ), \n",
    "              CAST(\n",
    "                ROW(0.0) AS ROW(sum DOUBLE)\n",
    "              ), \n",
    "              (s, x) -> CAST(\n",
    "                ROW(x + s.sum) AS ROW(sum DOUBLE)\n",
    "              ), \n",
    "              s -> s.sum\n",
    "            ) / (\n",
    "              SQRT(\n",
    "                REDUCE(\n",
    "                  transform(\n",
    "                    list_weights_inpi, \n",
    "                    (x) -> POW(x, 2)\n",
    "                  ), \n",
    "                  CAST(\n",
    "                    ROW(0.0) AS ROW(sum DOUBLE)\n",
    "                  ), \n",
    "                  (s, x) -> CAST(\n",
    "                    ROW(x + s.sum) AS ROW(sum DOUBLE)\n",
    "                  ), \n",
    "                  s -> s.sum\n",
    "                )\n",
    "              ) * SQRT(\n",
    "                REDUCE(\n",
    "                  transform(\n",
    "                    list_weights_insee, \n",
    "                    (x) -> POW(x, 2)\n",
    "                  ), \n",
    "                  CAST(\n",
    "                    ROW(0.0) AS ROW(sum DOUBLE)\n",
    "                  ), \n",
    "                  (s, x) -> CAST(\n",
    "                    ROW(x + s.sum) AS ROW(sum DOUBLE)\n",
    "                  ), \n",
    "                  s -> s.sum\n",
    "                )\n",
    "              )\n",
    "            ) AS cosine_distance  \n",
    "    FROM distance\n",
    "    )\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database=database,\n",
    "            s3_output=s3_output,\n",
    "  filename = 'cosine', ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Recupération de la similarité maximum par `row_id` et calcul Levensthein\n",
    "\n",
    "La dernière étape consiste a récupérer la similarité maximum sur les doublons provenant du `row_id` de sorte à n'avoir qu'une ligne par pair "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH dataset AS (\n",
    "  SELECT \n",
    "    siretisation.ets_insee_inpi.row_id, \n",
    "    index_id, \n",
    "    status_cas, \n",
    "    inpi_except, \n",
    "    insee_except, \n",
    "    transform(\n",
    "      sequence(\n",
    "        1, \n",
    "        CARDINALITY(insee_except)\n",
    "      ), \n",
    "      x -> insee_except\n",
    "    ), \n",
    "    ZIP(\n",
    "      inpi_except, \n",
    "      transform(\n",
    "        sequence(\n",
    "          1, \n",
    "          CARDINALITY(inpi_except)\n",
    "        ), \n",
    "        x -> insee_except\n",
    "      )\n",
    "    ) as test \n",
    "  FROM \n",
    "    siretisation.ets_insee_inpi  \n",
    "    \n",
    "  LEFT JOIN siretisation.ets_insee_inpi_status_cas \n",
    "  ON siretisation.ets_insee_inpi.row_id = siretisation.ets_insee_inpi_status_cas.row_id\n",
    "  where \n",
    "    (status_cas != 'CAS_2' AND CARDINALITY(inpi_except)  > 0 AND CARDINALITY(insee_except) > 0 AND index_id = 4664896)\n",
    "  )\n",
    " SELECT \n",
    "  * \n",
    "FROM \n",
    "  (\n",
    "    WITH distance AS (\n",
    "      SELECT \n",
    "        * \n",
    "      FROM \n",
    "        (\n",
    "          WITH list_weights_insee_inpi AS (\n",
    "            SELECT \n",
    "              row_id, \n",
    "              index_id, \n",
    "              status_cas, \n",
    "              inpi_except, \n",
    "              insee_except, \n",
    "              unzip_inpi, \n",
    "              unzip_insee, \n",
    "              list_weights_inpi, \n",
    "              list_weights_insee \n",
    "            FROM \n",
    "              (\n",
    "                SELECT \n",
    "                  row_id, \n",
    "                  index_id, \n",
    "                  status_cas, \n",
    "                  inpi_except, \n",
    "                  insee_except, \n",
    "                  unzip.field0 as unzip_inpi, \n",
    "                  unzip.field1 as insee, \n",
    "                  test \n",
    "                FROM \n",
    "                  dataset CROSS \n",
    "                  JOIN UNNEST(test) AS new (unzip)\n",
    "              ) CROSS \n",
    "              JOIN UNNEST(insee) as test (unzip_insee) \n",
    "              LEFT JOIN (\n",
    "                SELECT \n",
    "                  words, \n",
    "                  list_weights as list_weights_inpi \n",
    "                FROM \n",
    "                  siretisation.list_weight_mots_insee_inpi_word2vec \n",
    "              ) tb_weight_inpi ON unzip_inpi = tb_weight_inpi.words \n",
    "              LEFT JOIN (\n",
    "                SELECT \n",
    "                  words, \n",
    "                  list_weights as list_weights_insee \n",
    "                FROM \n",
    "                  siretisation.list_weight_mots_insee_inpi_word2vec \n",
    "              ) tb_weight_insee ON unzip_insee = tb_weight_insee.words \n",
    "          ) \n",
    "          SELECT \n",
    "            row_id, \n",
    "            index_id, \n",
    "            status_cas, \n",
    "            inpi_except, \n",
    "            insee_except, \n",
    "            unzip_inpi, \n",
    "            unzip_insee, \n",
    "            REDUCE(\n",
    "              zip_with(\n",
    "                list_weights_inpi, \n",
    "                list_weights_insee, \n",
    "                (x, y) -> x * y\n",
    "              ), \n",
    "              CAST(\n",
    "                ROW(0.0) AS ROW(sum DOUBLE)\n",
    "              ), \n",
    "              (s, x) -> CAST(\n",
    "                ROW(x + s.sum) AS ROW(sum DOUBLE)\n",
    "              ), \n",
    "              s -> s.sum\n",
    "            ) / (\n",
    "              SQRT(\n",
    "                REDUCE(\n",
    "                  transform(\n",
    "                    list_weights_inpi, \n",
    "                    (x) -> POW(x, 2)\n",
    "                  ), \n",
    "                  CAST(\n",
    "                    ROW(0.0) AS ROW(sum DOUBLE)\n",
    "                  ), \n",
    "                  (s, x) -> CAST(\n",
    "                    ROW(x + s.sum) AS ROW(sum DOUBLE)\n",
    "                  ), \n",
    "                  s -> s.sum\n",
    "                )\n",
    "              ) * SQRT(\n",
    "                REDUCE(\n",
    "                  transform(\n",
    "                    list_weights_insee, \n",
    "                    (x) -> POW(x, 2)\n",
    "                  ), \n",
    "                  CAST(\n",
    "                    ROW(0.0) AS ROW(sum DOUBLE)\n",
    "                  ), \n",
    "                  (s, x) -> CAST(\n",
    "                    ROW(x + s.sum) AS ROW(sum DOUBLE)\n",
    "                  ), \n",
    "                  s -> s.sum\n",
    "                )\n",
    "              )\n",
    "            ) AS cosine_distance \n",
    "          FROM \n",
    "            list_weights_insee_inpi\n",
    "        )\n",
    "    ) \n",
    "    SELECT \n",
    "      row_id, \n",
    "      dataset.index_id, \n",
    "      inpi_except, \n",
    "      insee_except, \n",
    "      unzip_inpi, \n",
    "      unzip_insee, \n",
    "      max_cosine_distance,\n",
    "      test as key_except_to_test,\n",
    "      levenshtein_distance(unzip_inpi, unzip_insee) AS levenshtein_distance\n",
    "    \n",
    "    FROM \n",
    "      dataset \n",
    "      LEFT JOIN (\n",
    "        SELECT \n",
    "          distance.index_id, \n",
    "          unzip_inpi, \n",
    "          unzip_insee, \n",
    "          max_cosine_distance \n",
    "        FROM \n",
    "          distance \n",
    "          RIGHT JOIN (\n",
    "            SELECT \n",
    "              index_id, \n",
    "              MAX(cosine_distance) as max_cosine_distance \n",
    "            FROM \n",
    "              distance \n",
    "            GROUP BY \n",
    "              index_id\n",
    "          ) as tb_max_distance ON distance.index_id = tb_max_distance.index_id \n",
    "          AND distance.cosine_distance = tb_max_distance.max_cosine_distance\n",
    "      ) as tb_max_distance_lookup ON dataset.index_id = tb_max_distance_lookup.index_id\n",
    "  )\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database=database,\n",
    "            s3_output=s3_output,\n",
    "  filename = 'max_cosine', ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, shutil, urllib, ipykernel, json\n",
    "from pathlib import Path\n",
    "from notebook import notebookapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_report(extension = \"html\", keep_code = False):\n",
    "    \"\"\"\n",
    "    Create a report from the current notebook and save it in the \n",
    "    Report folder (Parent-> child directory)\n",
    "    \n",
    "    1. Exctract the current notbook name\n",
    "    2. Convert the Notebook \n",
    "    3. Move the newly created report\n",
    "    \n",
    "    Args:\n",
    "    extension: string. Can be \"html\", \"pdf\", \"md\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ### Get notebook name\n",
    "    connection_file = os.path.basename(ipykernel.get_connection_file())\n",
    "    kernel_id = connection_file.split('-', 1)[0].split('.')[0]\n",
    "\n",
    "    for srv in notebookapp.list_running_servers():\n",
    "        try:\n",
    "            if srv['token']=='' and not srv['password']:  \n",
    "                req = urllib.request.urlopen(srv['url']+'api/sessions')\n",
    "            else:\n",
    "                req = urllib.request.urlopen(srv['url']+ \\\n",
    "                                             'api/sessions?token=' + \\\n",
    "                                             srv['token'])\n",
    "            sessions = json.load(req)\n",
    "            notebookname = sessions[0]['name']\n",
    "        except:\n",
    "            pass  \n",
    "    \n",
    "    sep = '.'\n",
    "    path = os.getcwd()\n",
    "    #parent_path = str(Path(path).parent)\n",
    "    \n",
    "    ### Path report\n",
    "    #path_report = \"{}/Reports\".format(parent_path)\n",
    "    #path_report = \"{}/Reports\".format(path)\n",
    "    \n",
    "    ### Path destination\n",
    "    name_no_extension = notebookname.split(sep, 1)[0]\n",
    "    source_to_move = name_no_extension +'.{}'.format(extension)\n",
    "    dest = os.path.join(path,'Reports', source_to_move)\n",
    "    \n",
    "    ### Generate notebook\n",
    "    if keep_code:\n",
    "        os.system('jupyter nbconvert --to {} {}'.format(\n",
    "    extension,notebookname))\n",
    "    else:\n",
    "        os.system('jupyter nbconvert --no-input --to {} {}'.format(\n",
    "    extension,notebookname))\n",
    "    \n",
    "    ### Move notebook to report folder\n",
    "    #time.sleep(5)\n",
    "    shutil.move(source_to_move, dest)\n",
    "    print(\"Report Available at this adress:\\n {}\".format(dest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_report(extension = \"html\", keep_code = True)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nteract": {
   "version": "0.24.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
