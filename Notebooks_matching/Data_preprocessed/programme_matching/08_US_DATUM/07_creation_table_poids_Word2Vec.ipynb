{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création table poids obtenus via le Word2Vec\n",
    "\n",
    "Copy paste from Coda to fill the information\n",
    "\n",
    "## Objective(s)\n",
    "\n",
    "La siretisation repose sur une matrice de règles de gestion classée de manière ordonnée. Lors des US précédents, nous avons calculé 10 variables tests sur les 12. Voici les 10 tests calculés au préalable, avec leur dépendence:\n",
    "\n",
    "| Rang | Nom_variable                              | Dependence                                    | Notebook                           | Difficulte | Table_input                                                                                                                                                            | Variables_crees_US                                                                 | Possibilites                  |\n",
    "|------|-------------------------------------------|-----------------------------------------------|------------------------------------|------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------|-------------------------------|\n",
    "| 1    | status_cas                                |                                               | 02_cas_de_figure                   | Moyen      | ets_insee_inpi_status_cas                                                                                                                                              | status_cas,intersection,pct_intersection,union_,inpi_except,insee_except           | CAS_1,CAS_2,CAS_3,CAS_4,CAS_5 |\n",
    "| 2    | test_list_num_voie                        | intersection_numero_voie,union_numero_voie    | 03_test_list_num_voie              | Moyen      | ets_insee_inpi_list_num_voie                                                                                                                                           | intersection_numero_voie,union_numero_voie                                         | FALSE,NULL,TRUE,PARTIAL       |\n",
    "| 3    | test_enseigne                             | list_enseigne,enseigne                        | 04_test_enseigne                   | Moyen      | ets_insee_inpi_list_enseigne                                                                                                                                           | list_enseigne_contain                                                              | FALSE,NULL,TRUE               |\n",
    "| 4    | test_pct_intersection                     | pct_intersection,index_id_max_intersection    | 06_creation_nb_siret_siren_max_pct | Facile     | ets_insee_inpi_var_group_max                                                                                                                                           | count_inpi_index_id_siret,count_inpi_siren_siret,index_id_max_intersection         | FALSE,TRUE                    |\n",
    "| 4    | test_index_id_duplicate                   | count_inpi_index_id_siret                     | 06_creation_nb_siret_siren_max_pct | Facile     | ets_insee_inpi_var_group_max                                                                                                                                           | count_inpi_index_id_siret,count_inpi_siren_siret,index_id_max_intersection         | FALSE,TRUE                    |\n",
    "| 4    | test_siren_insee_siren_inpi               | count_initial_insee,count_inpi_siren_siret    | 06_creation_nb_siret_siren_max_pct | Facile     | ets_insee_inpi_var_group_max                                                                                                                                           | count_inpi_index_id_siret,count_inpi_siren_siret,index_id_max_intersection         | FALSE,TRUE                    |\n",
    "| 5    | test_similarite_exception_words           | max_cosine_distance                           | 08_calcul_cosine_levhenstein       | Difficile  | ets_insee_inpi_similarite_max_word2vec                                                                                                                                 | unzip_inpi,unzip_insee,max_cosine_distance,levenshtein_distance,key_except_to_test | FALSE,NULL,TRUE               |\n",
    "| 5    | test_distance_levhenstein_exception_words | levenshtein_distance                          | 08_calcul_cosine_levhenstein       | Difficile  | ets_insee_inpi_similarite_max_word2vec                                                                                                                                 | unzip_inpi,unzip_insee,max_cosine_distance,levenshtein_distance,key_except_to_test | FALSE,NULL,TRUE               |\n",
    "| 6    | test_date                                 | datecreationetablissement,date_debut_activite | 10_match_et_creation_regles.md     | Facile     | ets_insee_inpi_list_num_voie,ets_insee_inpi_list_enseigne,ets_insee_inpi_similarite_max_word2vec,ets_insee_inpi_status_cas,ets_insee_inpi_var_group_max,ets_insee_inpi |                                                                                    | FALSE,TRUE                    |\n",
    "| 6    | test_siege                                | status_ets,etablissementsiege                 | 10_match_et_creation_regles.md     | Facile     | ets_insee_inpi_list_num_voie,ets_insee_inpi_list_enseigne,ets_insee_inpi_similarite_max_word2vec,ets_insee_inpi_status_cas,ets_insee_inpi_var_group_max,ets_insee_inpi |                                                                                    | FALSE,TRUE,NULL               |\n",
    "| 6    | test_status_admin                         | etatadministratifetablissement,status_admin   | 10_match_et_creation_regles.md     | Facile     | ets_insee_inpi_list_num_voie,ets_insee_inpi_list_enseigne,ets_insee_inpi_similarite_max_word2vec,ets_insee_inpi_status_cas,ets_insee_inpi_var_group_max,ets_insee_inpi |                                                                                    | FALSE,NULL,TRUE               |\n",
    "\n",
    "Il reste encore deux tests a calculer afin de pouvoir matcher avec la matrice des règles. Les deux règles manquantes sont `test_distance_cosine` et `test_distance_levhenstein`. Néanmoins, ses deux tests requièrent le calcul de deux variables, dont l'une d'elle `max_cosine_distance` dépend des résultats de l'algorithme de Word2Vec. C'est l'object de cette US.\n",
    "\n",
    "Dans cette US, nous allons calculer les poids permettants de mettre en avant la similarité entre deux mots. La similarité est calculé via le Word2Vec embedding.\n",
    "\n",
    "## Metadata \n",
    "\n",
    "* Metadata parameters are available here: \n",
    "* US Title: Création table poids obtenus via le Word2Vec\n",
    "* Epic: Epic 5\n",
    "* US: US 7\n",
    "* Date Begin: 9/7/2020\n",
    "* Duration Task: 1\n",
    "* Status:  \n",
    "* Source URL:US 07 Preparation tables et variables tests\n",
    "* Task type:\n",
    "  * Jupyter Notebook\n",
    "* Users: :\n",
    "  * Thomas Pernet\n",
    "* Watchers:\n",
    "  * Thomas Pernet\n",
    "* Estimated Log points:\n",
    "  * One being a simple task, 15 a very difficult one\n",
    "  *  7\n",
    "* Task tag\n",
    "  *  #machine-learning,#word2vec,#siretisation,#sql-query,#similarite,#preparation-tableword3vec\n",
    "* Toggl Tag\n",
    "  * #data-preparation\n",
    "  \n",
    "## Input Cloud Storage [AWS]\n",
    "\n",
    "If link from the internet, save it to the cloud first\n",
    "\n",
    "### Tables [AWS]\n",
    "\n",
    "1. Batch 1:\n",
    "  * Select Provider: Athena\n",
    "  * Select table(s): ets_insee_inpi\n",
    "    * Select only tables created from the same notebook, else copy/paste selection to add new input tables\n",
    "    * If table(s) does not exist, add them: Add New Table\n",
    "    * Information:\n",
    "      * Region: \n",
    "        * NameEurope (Paris)\n",
    "        * Code: eu-west-3\n",
    "      * Database: inpi\n",
    "      * Notebook construction file: \n",
    "        * https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/03_ETS_add_variables.md\n",
    "    \n",
    "## Destination Output/Delivery\n",
    "\n",
    "* AWS\n",
    "    1. Athena: \n",
    "      * Region: Europe (Paris)\n",
    "      * Database: siretisation\n",
    "      * Tables (Add name new table): list_pairs_mots_insee_inpi,weights_mots_insee_inpi_word2vec\n",
    "      * List new tables\n",
    "          * list_pairs_mots_insee_inpi\n",
    "          * weights_mots_insee_inpi_word2vec\n",
    "\n",
    "## Things to know (Steps, Attention points or new flow of information)\n",
    "\n",
    "### Sources of information  (meeting notes, Documentation, Query, URL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connexion serveur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsPy.aws_authorization import aws_connector\n",
    "from awsPy.aws_athena import service_athena\n",
    "from awsPy.aws_s3 import service_s3\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os, shutil\n",
    "\n",
    "path = os.getcwd()\n",
    "parent_path = str(Path(path).parent)\n",
    "path_cred = r\"{}/credential_AWS.json\".format(parent_path)\n",
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                       region = 'eu-west-3')\n",
    "\n",
    "region = 'eu-west-3'\n",
    "bucket = 'calfdata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                       region = region)\n",
    "client= con.client_boto()\n",
    "s3 = service_s3.connect_S3(client = client,\n",
    "                      bucket = bucket, verbose = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_setting = True\n",
    "if pandas_setting:\n",
    "    cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input/output\n",
    "\n",
    "Pour cacluler la similarité entre l'adresse de l'INSEE et de l'INPI, nous devons créer une liste de combinaison unique entre les deux adresses. Pour cela, nous utilisons les variables `adresse_distance_inpi` et `adresse_distance_insee` qui ont été, au préalable, néttoyée, puis nous les concatènons en prenant le soin d'enlever les mots redondants. Dit autrement, si deux mots sont présents dans les deux adresses, alors, nous n'en gardons qu'un seul.\n",
    "\n",
    "La table contient environ 2,836,384 combinaisons possibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output = 'inpi/sql_output'\n",
    "database = 'siretisation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "/*Combinaison mots insee inpi*/\n",
    "CREATE TABLE siretisation.list_pairs_mots_insee_inpi\n",
    "WITH (\n",
    "  format='PARQUET'\n",
    ") AS\n",
    "SELECT unique_combinaition,\n",
    "COUNT(*) AS CNT\n",
    "FROM (SELECT\n",
    "array_distinct(\n",
    "    concat(\n",
    "    array_distinct(\n",
    "      split(adresse_distance_inpi, ' ')\n",
    "      ),\n",
    "    array_distinct(\n",
    "      split(adresse_distance_insee, ' ')\n",
    "    )    )\n",
    "  ) unique_combinaition\n",
    "FROM siretisation.ets_insee_inpi \n",
    "      )\n",
    "      GROUP BY unique_combinaition\n",
    "\"\"\"\n",
    "\n",
    "output = s3.run_query(\n",
    "            query=query,\n",
    "            database=database,\n",
    "            s3_output=s3_output,\n",
    "  filename = None, ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul model\n",
    "\n",
    "### [What Are Word Embeddings for Text?](https://machinelearningmastery.com/what-are-word-embeddings/)\n",
    "\n",
    "- by [[Jason Brownlee]]\n",
    "\n",
    "- A word embedding is a learned representation for text where words that have the same meaning have a similar representation\n",
    "- It is this approach to representing words and documents that may be considered one of the key breakthroughs of deep learning on challenging natural language processing problems.\n",
    "\n",
    "    -> One of the benefits of using dense and low-dimensional vectors is computational\n",
    "    \n",
    "- The main benefit of the dense representations is generalization power\n",
    "- if we believe some features may provide similar clues, it is worthwhile to provide a representation that is able to capture these similarities\n",
    "- Word embeddings are in fact a class of techniques where individual words are represented as real-valued vectors in a predefined vector space\n",
    "- Each word is mapped to one vector and the vector values are learned in a way that resembles a neural network, and hence the technique is often lumped into the field of deep learning\n",
    "- Key to the approach is the idea of using a dense distributed representation for each word\n",
    "- Each word is represented by a real-valued vector, often tens or hundreds of dimensions. This is contrasted to the thousands or millions of dimensions required for sparse word representations, such as a one-hot encoding\n",
    "- The distributed representation is learned based on the usage of words\n",
    "- This allows words that are used in similar ways to result in having similar representations, naturally capturing their meaning\n",
    "- This can be contrasted with the crisp but fragile representation in a bag of words model where, unless explicitly managed, different words have different representations, regardless of how they are used\n",
    "\n",
    "## Word Embedding Algorithms\n",
    "    \n",
    "- Word embedding methods learn a real-valued vector representation for a predefined fixed sized vocabulary from a corpus of text\n",
    "- The learning process is either joint with the neural network model on some task, such as document classification, or is an unsupervised process, using document statistics.\n",
    "\n",
    "\n",
    "### Word2Vec\n",
    "\n",
    "- Word2Vec is a statistical method for efficiently learning a standalone word embedding from a text corpus\n",
    "- It was developed by Tomas Mikolov, et al. at Google in 2013 as a response to make the neural-network-based training of the embedding more efficient and since then has become the de facto standard for developing pre-trained word embedding\n",
    "- Two different learning models were introduced that can be used as part of the word2vec approach to learn the word embedding; they are:\n",
    "    - Continuous Bag-of-Words, or CBOW model \n",
    "    - Continuous Skip-Gram Model. \n",
    "    - The CBOW model learns the embedding by predicting the current word based on its context\n",
    "    - The continuous skip-gram model learns by predicting the surrounding words given a current word\n",
    "    - Both models are focused on learning about words given their local usage context, where the context is defined by a window of neighboring words\n",
    "    - The key benefit of the approach is that high-quality word embeddings can be learned efficiently (low space and time complexity), allowing larger embeddings to be learned (more dimensions) from much larger corpora of text (billions of words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM siretisation.list_pairs_mots_insee_inpi\n",
    "\"\"\"\n",
    "### run query\n",
    "output = s3.run_query(\n",
    "        query=query,\n",
    "        database='siretisation',\n",
    "        s3_output='INPI/sql_output',\n",
    "    filename = 'list_pairs', ## Add filename to print dataframe\n",
    "  #destination_key = 'INPI/list_pairs_mots_insee_inpi' ### Add destination key if need to copy output\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.sort_values(by = ['cnt']).tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "Il y a plusieurs paramètres dans la librairie que nous allons utilisé, voici la configuration que nous allons prendre:\n",
    "\n",
    "- size: (default 100) The number of dimensions of the embedding, e.g. the length of the dense vector to represent each token (word).\n",
    "- window: (default 5) The maximum distance between a target word and words around the target word.\n",
    "- min_count: (default 5) The minimum count of words to consider when training the model; words with an occurrence less than this count will be ignored.\n",
    "- workers: (default 3) The number of threads to use while training.\n",
    "- sg: (default 0 or CBOW) The training algorithm, either CBOW (0) or skip gram (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(text):\n",
    "    return re.sub(r'[^\\w\\s]|[|]', '', text).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = output['unique_combinaition'].apply(lambda x: basic_clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le POC, nous utilisons les paramètres par défault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "model = Word2Vec(df_text.tolist(),\n",
    "                 size = 100,\n",
    "                 window = 5,\n",
    "                 min_count=5,\n",
    "                 sg = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous devons calculer la similarité entre les mots communs dans l'adresse INPI/INSEE, donc nous pouvons utiliser les poids du modèles et ensuite calculer la similarité avec la méthode du cosine. \n",
    "\n",
    "La librarie `gensim` permet d'exporter les poids en `.txt`. Toutefois, il n'est pas concevable de calculer l'ensembles des similarités entre toutes les occurences (environ 90.000), donc lors des traitements dans Athena, nous calculerons le cosines à la demande. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegarde l'ensemble des poids dans des colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format('word2vec_weights_100.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_header = ['Words'].extend(list(range(1, 101)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights = pd.read_csv('word2vec_weights_100.txt',\n",
    "                            sep = ' ', skiprows= 1,\n",
    "                           header= list_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights.to_csv('word2vec_weights_100_v2.csv', index = False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle se trouve à l'adresse suivante [MACHINE_LEARNING/NLP/WORD2VEC_WEIGHTS/V2](https://s3.console.aws.amazon.com/s3/buckets/calfdata/MACHINE_LEARNING/NLP/WORD2VEC_WEIGHTS/V2?region=eu-west-3&tab=overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.upload_file('word2vec_weights_100_v2.csv',\n",
    "               'MACHINE_LEARNING/NLP/WORD2VEC_WEIGHTS/V2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tables weights\n",
    "\n",
    "Athena ne peut pas créer des array float a partir de fichier csv, du coup on utilise la fonction concat. C'est une solution pour le poc.\n",
    "\n",
    "On créer une table temporaire qui contient l'ensemble des poids en colonnes list_mots_insee_inpi_word2vec_weights_temp puis on canct les colonnes dans la table list_mots_insee_inpi_word2vec_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = \"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS siretisation.weights_mots_insee_inpi_word2vec (\n",
    "\n",
    "`Words` string,\n",
    "\"\"\"\n",
    "middle = \"\"\n",
    "\n",
    "for i in range(0,100):\n",
    "    if i == 99:\n",
    "        middle += \"vec_{} float )\".format(i)\n",
    "    else:\n",
    "        middle += \"vec_{} float,\".format(i)\n",
    "bottom = \"\"\"\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
    "    WITH SERDEPROPERTIES (\n",
    "   'separatorChar' = ',',\n",
    "   'quoteChar' = '\"'\n",
    "   ) \n",
    "     LOCATION 's3://calfdata/MACHINE_LEARNING/NLP/WORD2VEC_WEIGHTS/V2'\n",
    "     TBLPROPERTIES ('has_encrypted_data'='false',\n",
    "              'skip.header.line.count'='1');\n",
    "\"\"\" \n",
    "query = top + middle +bottom\n",
    "output = s3.run_query(\n",
    "        query=query,\n",
    "        database='siretisation',\n",
    "        s3_output='INPI/sql_output'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT * FROM \"siretisation\".\"weights_mots_insee_inpi_word2vec\" limit 10;\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "        query=query,\n",
    "        database='siretisation',\n",
    "        s3_output='INPI/sql_output',\n",
    "    filename = 'weights'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "\n",
    "CREATE TABLE siretisation.list_weight_mots_insee_inpi_word2vec\n",
    "WITH (\n",
    "  format='PARQUET'\n",
    ") AS\n",
    "SELECT words,\n",
    "CONCAT(\n",
    "\n",
    "\"\"\"\n",
    "middle = \"\"\n",
    "for i in range(0, 100):\n",
    "    if i ==99:\n",
    "        middle  = \"ARRAY[vec_{}]) as list_weights\".format(i)\n",
    "    else:\n",
    "        middle  = \"ARRAY[vec_{}],\".format(i)\n",
    "    query += middle\n",
    "bottom = \"\"\"\n",
    "FROM \"siretisation\".\"weights_mots_insee_inpi_word2vec\"\n",
    "\"\"\"\n",
    "query += bottom\n",
    "output = s3.run_query(\n",
    "        query=query,\n",
    "        database='machine_learning',\n",
    "        s3_output='INPI/sql_output'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT * FROM \"siretisation\".\"list_weight_mots_insee_inpi_word2vec\" limit 10;\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "        query=query,\n",
    "        database='siretisation',\n",
    "        s3_output='INPI/sql_output',\n",
    "    filename = 'list_weights'\n",
    "    ).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test acceptance/Analyse du modèle\n",
    "\n",
    "La librairie `gensim` a une fonction integrée pour calculer la similarité. Toutefois, nous devons cacluler le cosine manuelement dans Athena car il n'y a pas de fonction SQL prévue a cet effet. \n",
    "\n",
    "\n",
    "## Similarité - cosine\n",
    "\n",
    "La fonction cosine se calcule de la facon suivante:\n",
    "\n",
    "$$\\frac{u \\cdot v}{\\|u\\|_{2}\\|v\\|_{2}}$$\n",
    "\n",
    "- Source 1: Calcul cosine:\n",
    "  - [Cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity)\n",
    "  - [Magnitude (mathematics)](https://en.wikipedia.org/wiki/Magnitude_(mathematics)#Euclidean_vector_space)\n",
    "  - [Scipy Cosine](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce notebook, nous allons expliquer comment calculer la distance de cosine en SQL, via les fonctions Presto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Creation table exemple\n",
    "\n",
    "On va récupérer deux mots, RTE et ROUTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "CREATE TABLE siretisation.exemple_cosine_sql\n",
    "WITH (\n",
    "  format='PARQUET'\n",
    ") AS\n",
    "WITH db AS (\n",
    "SELECT 'PAIR' AS pairs,words as words_1, list_weights as list_weights_1\n",
    "FROM list_weight_mots_insee_inpi_word2vec \n",
    "WHERE words = 'RTE'\n",
    ")\n",
    "SELECT db.pairs, words_1,words_2, list_weights_1,list_weights_2\n",
    "FROM db\n",
    "LEFT JOIN (\n",
    "  \n",
    "  SELECT 'PAIR' AS pairs,words as words_2, list_weights as list_weights_2\n",
    "FROM list_weight_mots_insee_inpi_word2vec \n",
    "WHERE words = 'ROUTE'\n",
    "  \n",
    "  ) AS tb_list2\n",
    "  ON db.pairs = tb_list2.pairs\n",
    "\n",
    "\"\"\"\n",
    "output = s3.run_query(\n",
    "        query=query,\n",
    "        database='siretisation',\n",
    "        s3_output='INPI/sql_output'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT * FROM \"siretisation\".\"exemple_cosine_sql\"\n",
    "\"\"\"\n",
    "tb_rte_route = s3.run_query(\n",
    "            query=query,\n",
    "            database=database,\n",
    "            s3_output=s3_output,\n",
    "  filename = 'exemple_rte_route', ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )\n",
    "tb_rte_route"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Calcul du nominateur \n",
    "\n",
    "Le nominateur est simplement le dot product entre la liste 1 et la liste 2:\n",
    "\n",
    "$$u \\cdot v$$\n",
    "\n",
    "Pour rappel, le dot product s'effectue de la manière suivante: $a · b = ax × bx + ay × by$\n",
    "\n",
    "Le dot product doit, par définition, retourner un scalaire!\n",
    "\n",
    "Fonction presto:\n",
    "\n",
    "- `REDUCE`: https://prestodb.io/docs/0.172/functions/lambda.html#reduce\n",
    "    - Va permettre de sommer le produit x*y\n",
    "- `zip_with`: https://prestodb.io/docs/0.172/functions/lambda.html#zip_with\n",
    "    - Va permettre de calculer produit x*y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "pairs, words_1, words_2, \n",
    "REDUCE(\n",
    "              zip_with(\n",
    "                list_weights_1, \n",
    "                list_weights_2, \n",
    "                (x, y) -> x * y\n",
    "              ), \n",
    "              CAST(\n",
    "                ROW(0.0) AS ROW(sum DOUBLE)\n",
    "              ), \n",
    "              (s, x) -> CAST(\n",
    "                ROW(x + s.sum) AS ROW(sum DOUBLE)\n",
    "              ), \n",
    "              s -> s.sum\n",
    "            ) AS dot_product,\n",
    "            list_weights_1, list_weights_2\n",
    "\n",
    "FROM \"siretisation\".\"exemple_cosine_sql\"\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database=database,\n",
    "            s3_output=s3_output,\n",
    "  filename = 'exemple_dotproruct', ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va vérifier notre résultat avec numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_array_to_array(x):\n",
    "    \"\"\"\"\"\"\n",
    "    return (tb_rte_route[x]\n",
    " .str\n",
    " .replace('\\[|\\]|,', '', regex=True)\n",
    " .str\n",
    " .split(expand = True)\n",
    " .loc[0]\n",
    " .astype(float)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.dot(sql_array_to_array('list_weights_1'),sql_array_to_array('list_weights_2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Calcul du dénominateur\n",
    "\n",
    "Le denominateur est $\\|u\\|_{2}\\|v\\|_{2}$, ce qui correspond à la norme. La norme est une extension de la valeur absolue des nombres aux vecteurs. Elle permet de mesurer la longueur commune à toutes les représentations d'un vecteur dans un espace affine, mais définit aussi une distance entre deux vecteurs invariante par translation et compatible avec la multiplication externe.\n",
    "\n",
    "La norme usuelle (euclidienne) d'un vecteur peut se calculer à l'aide de ses coordonnées dans un repère orthonormé à l'aide du théorème de Pythagore.\n",
    "Dans le plan, si le vecteur ${\\displaystyle {\\vec {u}}}{\\vec {u}}$ a pour coordonnées ${\\displaystyle (x,y)}(x,y)$, sa norme s'écrire:\n",
    "\n",
    "$$\\|{\\vec  u}\\|={\\sqrt  {x^{2}+y^{2}}}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "pairs, words_1, \n",
    "SQRT(\n",
    "                REDUCE(\n",
    "                  transform(\n",
    "                    list_weights_1, \n",
    "                    (x) -> POW(x, 2)\n",
    "                  ), \n",
    "                  CAST(\n",
    "                    ROW(0.0) AS ROW(sum DOUBLE)\n",
    "                  ), \n",
    "                  (s, x) -> CAST(\n",
    "                    ROW(x + s.sum) AS ROW(sum DOUBLE)\n",
    "                  ), \n",
    "                  s -> s.sum\n",
    "                )\n",
    "              ) as norme_w1 \n",
    "\n",
    "FROM \"siretisation\".\"exemple_cosine_sql\"\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database=database,\n",
    "            s3_output=s3_output,\n",
    "  filename = 'exemple_cosine_sql', ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Résultat via numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(sql_array_to_array('list_weights_1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On calcule du cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query =\"\"\" \n",
    "SELECT \n",
    "pairs, words_1, words_2, \n",
    " REDUCE(\n",
    "              zip_with(\n",
    "                list_weights_1, \n",
    "                list_weights_2, \n",
    "                (x, y) -> x * y\n",
    "              ), \n",
    "              CAST(\n",
    "                ROW(0.0) AS ROW(sum DOUBLE)\n",
    "              ), \n",
    "              (s, x) -> CAST(\n",
    "                ROW(x + s.sum) AS ROW(sum DOUBLE)\n",
    "              ), \n",
    "              s -> s.sum\n",
    "            ) / (\n",
    "              SQRT(\n",
    "                REDUCE(\n",
    "                  transform(\n",
    "                    list_weights_1, \n",
    "                    (x) -> POW(x, 2)\n",
    "                  ), \n",
    "                  CAST(\n",
    "                    ROW(0.0) AS ROW(sum DOUBLE)\n",
    "                  ), \n",
    "                  (s, x) -> CAST(\n",
    "                    ROW(x + s.sum) AS ROW(sum DOUBLE)\n",
    "                  ), \n",
    "                  s -> s.sum\n",
    "                )\n",
    "              ) * SQRT(\n",
    "                REDUCE(\n",
    "                  transform(\n",
    "                    list_weights_2, \n",
    "                    (x) -> POW(x, 2)\n",
    "                  ), \n",
    "                  CAST(\n",
    "                    ROW(0.0) AS ROW(sum DOUBLE)\n",
    "                  ), \n",
    "                  (s, x) -> CAST(\n",
    "                    ROW(x + s.sum) AS ROW(sum DOUBLE)\n",
    "                  ), \n",
    "                  s -> s.sum\n",
    "                )\n",
    "              )\n",
    "            ) AS cosine_distance\n",
    "\n",
    "FROM \"siretisation\".\"exemple_cosine_sql\"\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database=database,\n",
    "            s3_output=s3_output,\n",
    "  filename = 'exemple_cosine', ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "1 - distance.cosine(\n",
    "     sql_array_to_array('list_weights_1'),\n",
    "     sql_array_to_array('list_weights_2')\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, shutil, urllib, ipykernel, json\n",
    "from pathlib import Path\n",
    "from notebook import notebookapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_report(extension = \"html\", keep_code = False):\n",
    "    \"\"\"\n",
    "    Create a report from the current notebook and save it in the \n",
    "    Report folder (Parent-> child directory)\n",
    "    \n",
    "    1. Exctract the current notbook name\n",
    "    2. Convert the Notebook \n",
    "    3. Move the newly created report\n",
    "    \n",
    "    Args:\n",
    "    extension: string. Can be \"html\", \"pdf\", \"md\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ### Get notebook name\n",
    "    connection_file = os.path.basename(ipykernel.get_connection_file())\n",
    "    kernel_id = connection_file.split('-', 1)[0].split('.')[0]\n",
    "\n",
    "    for srv in notebookapp.list_running_servers():\n",
    "        try:\n",
    "            if srv['token']=='' and not srv['password']:  \n",
    "                req = urllib.request.urlopen(srv['url']+'api/sessions')\n",
    "            else:\n",
    "                req = urllib.request.urlopen(srv['url']+ \\\n",
    "                                             'api/sessions?token=' + \\\n",
    "                                             srv['token'])\n",
    "            sessions = json.load(req)\n",
    "            notebookname = sessions[0]['name']\n",
    "        except:\n",
    "            pass  \n",
    "    \n",
    "    sep = '.'\n",
    "    path = os.getcwd()\n",
    "    #parent_path = str(Path(path).parent)\n",
    "    \n",
    "    ### Path report\n",
    "    #path_report = \"{}/Reports\".format(parent_path)\n",
    "    #path_report = \"{}/Reports\".format(path)\n",
    "    \n",
    "    ### Path destination\n",
    "    name_no_extension = notebookname.split(sep, 1)[0]\n",
    "    source_to_move = name_no_extension +'.{}'.format(extension)\n",
    "    dest = os.path.join(path,'Reports', source_to_move)\n",
    "    \n",
    "    ### Generate notebook\n",
    "    if keep_code:\n",
    "        os.system('jupyter nbconvert --to {} {}'.format(\n",
    "    extension,notebookname))\n",
    "    else:\n",
    "        os.system('jupyter nbconvert --no-input --to {} {}'.format(\n",
    "    extension,notebookname))\n",
    "    \n",
    "    ### Move notebook to report folder\n",
    "    #time.sleep(5)\n",
    "    shutil.move(source_to_move, dest)\n",
    "    print(\"Report Available at this adress:\\n {}\".format(dest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_report(extension = \"html\", keep_code = True)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nteract": {
   "version": "0.24.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
