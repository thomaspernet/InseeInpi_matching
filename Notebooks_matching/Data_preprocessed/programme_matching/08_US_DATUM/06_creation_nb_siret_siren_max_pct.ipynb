{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création variables  count index, count siren et maximum pct intersection\n",
    "\n",
    "La réalisation des tests de dedoublage demande la création de trois variables supplémentaires, qui sont créé a partir de groupe, a savoir l'index ou le siren.\n",
    "\n",
    "Les trois variables a créée sont les suivantes:\n",
    "\n",
    "- count_inpi_index_id_siret: nombre de siret possible par index\n",
    "- count_inpi_siren_siret: nombre de siret unique par siren\n",
    "- index_id_max_intersection: Pourcentage maximum de pct_intersection par index\n",
    "    - Creation de `pct_intersection` lors de l'US [3275](https://tree.taiga.io/project/olivierlubet-air/us/3275) -> `intersection / union_ as pct_intersection`\n",
    "\n",
    "## Objective(s)\n",
    "\n",
    "\n",
    "\n",
    "## Metadata \n",
    "\n",
    "* Metadata parameters are available here: \n",
    "* US Title: Création variables  count index, count siren et maximum pct intersection\n",
    "* Epic: Epic 5\n",
    "* US: US 7\n",
    "* Date Begin: 9/1/2020\n",
    "* Duration Task: 6\n",
    "* Status: Active\n",
    "* Source URL: [US 07 Preparation tables et variables tests](https://coda.io/d/_dCtnoqIftTn/US-07-Preparation-tables-et-variables-tests_suFb9)\n",
    "* Task type:\n",
    "  * Jupyter Notebook\n",
    "* Users: :\n",
    "  * Thomas Pernet\n",
    "* Watchers:\n",
    "  * Thomas Pernet\n",
    "* Estimated Log points:\n",
    "  * One being a simple task, 15 a very difficult one\n",
    "  *  5\n",
    "* Task tag\n",
    "  *  #sql-query,#regle-de-gestion,#preparation-nb-index,#preparation-nb-siren,\"preparation-pct-max\n",
    "* Toggl Tag\n",
    "  * #data-preparation\n",
    "  \n",
    "## Input Cloud Storage [AWS]\n",
    "\n",
    "If link from the internet, save it to the cloud first\n",
    "\n",
    "### Tables [AWS]\n",
    "\n",
    "1. Batch 1:\n",
    "  * Select Provider: \n",
    "  * Select table(s): \n",
    "    * Select only tables created from the same notebook, else copy/paste selection to add new input tables\n",
    "    * If table(s) does not exist, add them: Add New Table\n",
    "    * Information:\n",
    "      * Region: \n",
    "        * Name: \n",
    "        * Code: \n",
    "      * Database: \n",
    "      * Notebook construction file: \n",
    "    \n",
    "## Destination Output/Delivery\n",
    "\n",
    "* AWS\n",
    "  1. Athena: \n",
    "      * Region: \n",
    "      * Database: \n",
    "      * Tables (Add name new table): \n",
    "\n",
    "## Things to know (Steps, Attention points or new flow of information)\n",
    "\n",
    "### Sources of information  (meeting notes, Documentation, Query, URL)\n",
    "\n",
    "Sources of information  (meeting notes, Documentation, Query, URL)\n",
    "1. Jupyter Notebook (Github Link)\n",
    "  1. md : https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/08_US_DATUM/05_creation_table_cases.md#create-table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connexion serveur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsPy.aws_authorization import aws_connector\n",
    "from awsPy.aws_athena import service_athena\n",
    "from awsPy.aws_s3 import service_s3\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os, shutil\n",
    "\n",
    "path = os.getcwd()\n",
    "parent_path = str(Path(path).parent)\n",
    "path_cred = r\"{}/credential_AWS.json\".format(parent_path)\n",
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                       region = 'eu-west-3')\n",
    "\n",
    "region = 'eu-west-3'\n",
    "bucket = 'calfdata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                       region = region)\n",
    "client= con.client_boto()\n",
    "s3 = service_s3.connect_S3(client = client,\n",
    "                      bucket = bucket, verbose = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_setting = True\n",
    "if pandas_setting:\n",
    "    cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output = 'inpi/sql_output'\n",
    "database = 'siretisation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "CREATE TABLE siretisation.temp_US_max\n",
    "WITH (\n",
    "  format='PARQUET'\n",
    ") AS\n",
    "WITH create_var AS (\n",
    "SELECT \n",
    "siren,\n",
    "siret,\n",
    "index_id,\n",
    "CAST(\n",
    "      cardinality(\n",
    "        array_distinct(\n",
    "          array_intersect(\n",
    "            split(adresse_distance_inpi, ' '), \n",
    "            split(adresse_distance_insee, ' ')\n",
    "          )\n",
    "        )\n",
    "      ) AS DECIMAL(10, 2)\n",
    "    ) as intersection, \n",
    "    CAST(\n",
    "      cardinality(\n",
    "        array_distinct(\n",
    "          array_union(\n",
    "            split(adresse_distance_inpi, ' '), \n",
    "            split(adresse_distance_insee, ' ')\n",
    "          )\n",
    "        )\n",
    "      ) AS DECIMAL(10, 2)\n",
    "    ) as union_, \n",
    "    CAST(\n",
    "      cardinality(\n",
    "        array_distinct(\n",
    "          array_intersect(\n",
    "            split(adresse_distance_inpi, ' '), \n",
    "            split(adresse_distance_insee, ' ')\n",
    "          )\n",
    "        )\n",
    "      ) AS DECIMAL(10, 2)\n",
    "    )/ CAST(\n",
    "      cardinality(\n",
    "        array_distinct(\n",
    "          array_union(\n",
    "            split(adresse_distance_inpi, ' '), \n",
    "            split(adresse_distance_insee, ' ')\n",
    "          )\n",
    "        )\n",
    "      ) AS DECIMAL(10, 2)\n",
    "    ) as pct_intersection\n",
    "    FROM siretisation.ets_insee_inpi\n",
    ")\n",
    "SELECT \n",
    "create_var.siren,\n",
    "create_var.siret,\n",
    "create_var.index_id,\n",
    "count_inpi_index_id_siret,\n",
    "count_inpi_siren_siret,\n",
    "create_var.pct_intersection,\n",
    "index_id_max_intersection\n",
    "FROM \n",
    "    create_var\n",
    "LEFT JOIN (\n",
    "          SELECT \n",
    "            index_id, \n",
    "            COUNT(\n",
    "              DISTINCT(siret)\n",
    "            ) AS count_inpi_index_id_siret \n",
    "          FROM \n",
    "            create_var \n",
    "          GROUP BY \n",
    "            index_id\n",
    "        ) AS count_rows_index_id_siret ON create_var.index_id = count_rows_index_id_siret.index_id \n",
    "        LEFT JOIN (\n",
    "          SELECT \n",
    "            siren, \n",
    "            COUNT(\n",
    "              DISTINCT(siret)\n",
    "            ) AS count_inpi_siren_siret \n",
    "          FROM \n",
    "            create_var \n",
    "          GROUP BY \n",
    "            siren\n",
    "        ) AS count_rows_sequence ON create_var.siren = count_rows_sequence.siren \n",
    "        LEFT JOIN (\n",
    "          SELECT \n",
    "            index_id, \n",
    "            MAX(pct_intersection) AS index_id_max_intersection \n",
    "          FROM \n",
    "            create_var \n",
    "          GROUP BY \n",
    "            index_id\n",
    "        ) AS is_index_id_index_id_max_intersection ON create_var.index_id = is_index_id_index_id_max_intersection.index_id\n",
    "\"\"\"\n",
    "\n",
    "output = s3.run_query(\n",
    "            query=query,\n",
    "            database=database,\n",
    "            s3_output=s3_output,\n",
    "  filename = None, ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM siretisation.temp_US_max\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "tb = s3.run_query(\n",
    "            query=query,\n",
    "            database=database,\n",
    "            s3_output=s3_output,\n",
    "  filename = 'exemple_US_max', ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )\n",
    "tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tb.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test acceptance\n",
    "\n",
    "1. Vérifier que le nombre de lignes est indentique avant et après la création des variables\n",
    "2. Compter le nombre de lignes par possibilité pour `count_inpi_index_id_siret` et `count_inpi_siren_siret` -> TOP 10\n",
    "3. Donner la distribution de  `pct_intersection` et `index_id_max_intersection`\n",
    "    - distribution -> 0.1,0.25,0.5,0.75,0.8,0.95\n",
    "4. Donner la distribution de `pct_intersection` par possibilité pour `count_inpi_index_id_siret` et `count_inpi_siren_siret`\n",
    "    - distribution -> 0.1,0.25,0.5,0.75,0.8,0.95\n",
    "    - TOP 10\n",
    "    - BOTTOM 10\n",
    "5. Donner la distribution de `index_id_max_intersection` par possibilité pour `count_inpi_index_id_siret` et `count_inpi_siren_siret`\n",
    "    - distribution -> 0.1,0.25,0.5,0.75,0.8,0.95\n",
    "    - TOP 10\n",
    "    - BOTTOM 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vérifier que le nombre de lignes est indentique avant et après la création des variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM siretisation.temp_US_max\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database='siretisation',\n",
    "            s3_output=s3_output,\n",
    "  filename = 'count_ets_insee_inpi', ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM siretisation.temp_US_max\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database='siretisation',\n",
    "            s3_output=s3_output,\n",
    "  filename = 'count_ets_insee_inpi', ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compter le nombre de lignes par possibilité pour `count_inpi_index_id_siret` et `count_inpi_siren_siret`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`count_inpi_index_id_siret`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT count_inpi_index_id_siret, COUNT(*) as cnt_count_inpi_index_id_siret\n",
    "FROM siretisation.temp_us_max  \n",
    "GROUP BY count_inpi_index_id_siret\n",
    "ORDER BY cnt_count_inpi_index_id_siret DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "tb = s3.run_query(\n",
    "            query=query,\n",
    "            database='siretisation',\n",
    "            s3_output=s3_output,\n",
    "  filename = 'cnt_count_inpi_index_id_siret', ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )\n",
    "tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tb.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`count_inpi_siren_siret`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT count_inpi_siren_siret, COUNT(*) as cnt_count_inpi_siren_siret\n",
    "FROM siretisation.temp_us_max  \n",
    "GROUP BY count_inpi_siren_siret\n",
    "ORDER BY cnt_count_inpi_siren_siret DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "tb = s3.run_query(\n",
    "            query=query,\n",
    "            database='siretisation',\n",
    "            s3_output=s3_output,\n",
    "  filename = 'cnt_count_inpi_siren_siret', ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )\n",
    "tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tb.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Donner la distribution de  `pct_intersection` et `index_id_max_intersection`\n",
    "    \n",
    "- distribution -> 0.1,0.25,0.5,0.75,0.8,0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " `pct_intersection`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT approx_percentile(\n",
    "  pct_intersection,\n",
    "  ARRAY[0.1,0.25,0.5,0.75,0.8,0.95]) AS nest\n",
    "FROM temp_us_max  \n",
    "    \"\"\"\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database='siretisation',\n",
    "            s3_output=s3_output,\n",
    "  filename = 'dist_pct_intersection', ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`index_id_max_intersection`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT approx_percentile(\n",
    "  index_id_max_intersection,\n",
    "  ARRAY[0.1,0.25,0.5,0.75,0.8,0.95]) AS nest\n",
    "FROM temp_us_max  \n",
    "    \"\"\"\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database='siretisation',\n",
    "            s3_output=s3_output,\n",
    "  filename = 'dist_max_pct_intersection', ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Donner la distribution de `pct_intersection` par possibilité pour `count_inpi_index_id_siret` et `count_inpi_siren_siret`\n",
    "\n",
    "\n",
    "- distribution -> 0.1,0.25,0.5,0.75,0.8,0.95\n",
    "- TOP 10\n",
    "- BOTTOM 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`count_inpi_index_id_siret`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH dataset AS (\n",
    "  \n",
    "  SELECT \n",
    "  count_inpi_index_id_siret,\n",
    "  MAP(\n",
    "    ARRAY[0.1,0.25,0.5,0.75,0.8,0.95],\n",
    "    approx_percentile(\n",
    "      pct_intersection,\n",
    "    ARRAY[0.1,0.25,0.5,0.75,0.8,0.95])\n",
    "    ) AS nest\n",
    "    FROM temp_us_max \n",
    "    GROUP BY count_inpi_index_id_siret\n",
    "    ) \n",
    "    \n",
    "    SELECT \n",
    "    count_inpi_index_id_siret,\n",
    "    pct, \n",
    "    value AS  pct_intersection\n",
    "    FROM dataset\n",
    "    CROSS JOIN UNNEST(nest) as t(pct, value)\n",
    "\"\"\"\n",
    "output = s3.run_query(\n",
    "            query=query,\n",
    "            database='siretisation',\n",
    "            s3_output=s3_output,\n",
    "  filename = 'pct_count_inpi_index_id_siret', ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb= output.set_index(['count_inpi_index_id_siret', 'pct']).unstack(-1).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tb.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = output.set_index(['count_inpi_index_id_siret', 'pct']).unstack(-1).tail(10)\n",
    "tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tb.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`count_inpi_siren_siret`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH dataset AS (\n",
    "  \n",
    "  SELECT \n",
    "  count_inpi_siren_siret,\n",
    "  MAP(\n",
    "    ARRAY[0.1,0.25,0.5,0.75,0.8,0.95],\n",
    "    approx_percentile(\n",
    "      pct_intersection,\n",
    "    ARRAY[0.1,0.25,0.5,0.75,0.8,0.95])\n",
    "    ) AS nest\n",
    "    FROM temp_us_max \n",
    "    GROUP BY count_inpi_siren_siret\n",
    "    ) \n",
    "    \n",
    "    SELECT \n",
    "    count_inpi_siren_siret,\n",
    "    pct, \n",
    "    value AS  pct_intersection\n",
    "    FROM dataset\n",
    "    CROSS JOIN UNNEST(nest) as t(pct, value)\n",
    "\"\"\"\n",
    "output = s3.run_query(\n",
    "            query=query,\n",
    "            database='siretisation',\n",
    "            s3_output=s3_output,\n",
    "  filename = 'pct_count_inpi_siren_siret', ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb= output.set_index(['count_inpi_siren_siret', 'pct']).unstack(-1).head(10)\n",
    "tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tb.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = output.set_index(['count_inpi_siren_siret', 'pct']).unstack(-1).tail(10)\n",
    "tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tb.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Donner la distribution de `index_id_max_intersection` par possibilité pour `count_inpi_index_id_siret` et `count_inpi_siren_siret`\n",
    "\n",
    "- distribution -> 0.1,0.25,0.5,0.75,0.8,0.95\n",
    "- TOP 10\n",
    "- BOTTOM 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`count_inpi_index_id_siret`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH dataset AS (\n",
    "  \n",
    "  SELECT \n",
    "  count_inpi_index_id_siret,\n",
    "  MAP(\n",
    "    ARRAY[0.1,0.25,0.5,0.75,0.8,0.95],\n",
    "    approx_percentile(\n",
    "      index_id_max_intersection,\n",
    "    ARRAY[0.1,0.25,0.5,0.75,0.8,0.95])\n",
    "    ) AS nest\n",
    "    FROM temp_us_max \n",
    "    GROUP BY count_inpi_index_id_siret\n",
    "    ) \n",
    "    \n",
    "    SELECT \n",
    "    count_inpi_index_id_siret,\n",
    "    pct, \n",
    "    value AS  pct_intersection\n",
    "    FROM dataset\n",
    "    CROSS JOIN UNNEST(nest) as t(pct, value)\n",
    "\"\"\"\n",
    "output = s3.run_query(\n",
    "            query=query,\n",
    "            database='siretisation',\n",
    "            s3_output=s3_output,\n",
    "  filename = 'pct_count_inpi_index_id_siret', ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = output.set_index(['count_inpi_index_id_siret', 'pct']).unstack(-1).head(10)\n",
    "tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tb.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = output.set_index(['count_inpi_index_id_siret', 'pct']).unstack(-1).tail(10)\n",
    "tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tb.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`count_inpi_siren_siret`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH dataset AS (\n",
    "  \n",
    "  SELECT \n",
    "  count_inpi_siren_siret,\n",
    "  MAP(\n",
    "    ARRAY[0.1,0.25,0.5,0.75,0.8,0.95],\n",
    "    approx_percentile(\n",
    "      index_id_max_intersection,\n",
    "    ARRAY[0.1,0.25,0.5,0.75,0.8,0.95])\n",
    "    ) AS nest\n",
    "    FROM temp_us_max \n",
    "    GROUP BY count_inpi_siren_siret\n",
    "    ) \n",
    "    \n",
    "    SELECT \n",
    "    count_inpi_siren_siret,\n",
    "    pct, \n",
    "    value AS pct_intersection\n",
    "    FROM dataset\n",
    "    CROSS JOIN UNNEST(nest) as t(pct, value)\n",
    "\"\"\"\n",
    "output = s3.run_query(\n",
    "            query=query,\n",
    "            database='siretisation',\n",
    "            s3_output=s3_output,\n",
    "  filename = 'pct_count_inpi_siren_siret', ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = output.set_index(['count_inpi_siren_siret', 'pct']).unstack(-1).head(10)\n",
    "tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tb.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = output.set_index(['count_inpi_siren_siret', 'pct']).unstack(-1).tail(10)\n",
    "tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tb.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, shutil, urllib, ipykernel, json\n",
    "from pathlib import Path\n",
    "from notebook import notebookapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_report(extension = \"html\", keep_code = False):\n",
    "    \"\"\"\n",
    "    Create a report from the current notebook and save it in the \n",
    "    Report folder (Parent-> child directory)\n",
    "    \n",
    "    1. Exctract the current notbook name\n",
    "    2. Convert the Notebook \n",
    "    3. Move the newly created report\n",
    "    \n",
    "    Args:\n",
    "    extension: string. Can be \"html\", \"pdf\", \"md\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ### Get notebook name\n",
    "    connection_file = os.path.basename(ipykernel.get_connection_file())\n",
    "    kernel_id = connection_file.split('-', 1)[0].split('.')[0]\n",
    "\n",
    "    for srv in notebookapp.list_running_servers():\n",
    "        try:\n",
    "            if srv['token']=='' and not srv['password']:  \n",
    "                req = urllib.request.urlopen(srv['url']+'api/sessions')\n",
    "            else:\n",
    "                req = urllib.request.urlopen(srv['url']+ \\\n",
    "                                             'api/sessions?token=' + \\\n",
    "                                             srv['token'])\n",
    "            sessions = json.load(req)\n",
    "            notebookname = sessions[0]['name']\n",
    "        except:\n",
    "            pass  \n",
    "    \n",
    "    sep = '.'\n",
    "    path = os.getcwd()\n",
    "    #parent_path = str(Path(path).parent)\n",
    "    \n",
    "    ### Path report\n",
    "    #path_report = \"{}/Reports\".format(parent_path)\n",
    "    #path_report = \"{}/Reports\".format(path)\n",
    "    \n",
    "    ### Path destination\n",
    "    name_no_extension = notebookname.split(sep, 1)[0]\n",
    "    source_to_move = name_no_extension +'.{}'.format(extension)\n",
    "    dest = os.path.join(path,'Reports', source_to_move)\n",
    "    \n",
    "    ### Generate notebook\n",
    "    if keep_code:\n",
    "        os.system('jupyter nbconvert --to {} {}'.format(\n",
    "    extension,notebookname))\n",
    "    else:\n",
    "        os.system('jupyter nbconvert --no-input --to {} {}'.format(\n",
    "    extension,notebookname))\n",
    "    \n",
    "    ### Move notebook to report folder\n",
    "    #time.sleep(5)\n",
    "    shutil.move(source_to_move, dest)\n",
    "    print(\"Report Available at this adress:\\n {}\".format(dest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_report(extension = \"html\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nteract": {
   "version": "0.24.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
