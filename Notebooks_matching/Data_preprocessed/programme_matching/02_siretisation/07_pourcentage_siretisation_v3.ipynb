{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test nombre lignes siretise\n",
    "\n",
    "Objective(s)\n",
    "\n",
    "*   Lors de l’US 6 Union et intersection, nous avons compté séparément le nombre de lignes siretisés avec les tests unitaires, indépendamment des autres tests. Le niveau de matching avec les tests unitaires va de 39% a 70%. Dans cette US, nous allons faire le test avec une prise en compte de l’ensemble des tests, autrement dit, compter le nombre de lignes sirétisé lorsque les tests sont co-dépendant.\n",
    "  * L’objectif principal étant de fournir un pourcentage avec le nombre de lignes siretisé.\n",
    "  * Les tests vont suivre l’arborescence suivante:\n",
    "    * Calcul des cas 1 à 7\n",
    "      * Calcul des tests \n",
    "      * Calcul des duplicates sur index & séquence\n",
    "        * Filtrer les lignes sans duplicate et tests (a definir les règles sur les tests)\n",
    "   * ATTENTION, on exclut lorsque le test list_num_voie est faux, et que le status_cas est egal à 2. En effet, il n’est pas nécéssaire d’analyser lorsqu’au des numéros ne se trouve dans l’une des deux listes ou bien qu’aucun des mots de l’adresse n’est commun.\n",
    "     * La création de la table ets_inpi_insee_cases doit contenir les variables suivantes:\n",
    "    * row_id\n",
    "    * count_initial_insee\n",
    "    * index_id, \n",
    "    * sequence_id, \n",
    "    * siren, \n",
    "    * siret,\n",
    "    * count_initial_insee, \n",
    "    *  count_inpi_siren_siret, \n",
    "      * Pour un siren donné, combien de siret possible. Par exemple, si → 7, cela signifie que pour un même siren, il y a 7 siret (etb). \n",
    "      * Si cette variable est égale à count_initial_insee, potentiellement, tous les etb on été trouvé\n",
    "    *  count_inpi_siren_sequence,\n",
    "      * Pour un siren donné, combien de séquence (etb au sens de l’INPI) possible. Si → 3, cela signifie que pour un même siren, il y a 3 établissements au sens de l’INPI\n",
    "    *  count_inpi_sequence_siret, \n",
    "      * Pour une séquence donnée, combien de siret possible. Cette variable indique le nombre de duplicate par séquence. Si → 3, cela signifie que pour la meme séquence, il y a 3 siret possible\n",
    "    * count_inpi_sequence_stat_cas_siret:\n",
    "      * Pour une séquence-status cas donnée, combien de siret possible. Cette variable indique le nombre de siret possible pour chaque cas. En effet, un duplicate peut se retrouver dans plusieurs cas; C’est le cas lorsque les doublons n’ont pas de relation avec l’adresse de l’INPI. Si 3 → cela signifie que pour la même séquence, appartenant au même cas, il y a trois siret possible.\n",
    "      * Si la variable est différente de count_inpi_sequence_siret, cela signifie que l’adresse de l’INSEE appartient a plusieurs cas.\n",
    "    * count_inpi_index_id_siret,\n",
    "      * Pour un index id, combien de siret possible. Cette variable indique le nombre de ligne dupliqué \n",
    "      * Si 3 → Il y a 3 siret pour un index, donc 3 lignes dupliquées\n",
    "    * count_inpi_index_id_stat_cas_siret\n",
    "      * Pour une pair index id cas, combien de siret possible. Cette variable indique le nombre de ligne dupliqué pour chacun des cas\n",
    "      * Si 3 → Il y a 3 siret pour un index-cas, donc 3 lignes dupliquées pour le même cas\n",
    "    * count_inpi_index_id_stat_cas:\n",
    "      * Nombre de cas par index.\n",
    "      * Si 3 → Il y a trois cas possible pour le meme index\n",
    "    * index_id_duplicate: \n",
    "    * count_inpi_index_id_siret > 1 THEN 'True' ELSE 'False'\n",
    "    * Indique si la ligne est doublée\n",
    "    * test_siren_insee_siren_inpi\n",
    "      * count_initial_insee = count_inpi_siren_siret THEN 'True' ELSE 'False'\n",
    "      * Si la variable est ‘True’ alors tous les établissements ont été trouvé\n",
    "    * test_sequence_siret\n",
    "      * count_inpi_sequence_siret = 1 THEN ‘True’ ELSE ‘False\n",
    "      * Si la variable est ‘True’ alors, il n’y a pas de duplicate pour une séquence, candidat très probable\n",
    "    * test_index_siret\n",
    "      * count_inpi_index_id_stat_cas_siret = 1 THEN 'True' ELSE 'False'\n",
    "      * Si la variable est true, alors, il n’y a qu”un seul cas de figure par index \n",
    "    * test_sequence_siret_many_cas:\n",
    "      * count_inpi_sequence_siret = count_inpi_sequence_stat_cas_siret THEN 'True' ELSE 'False\n",
    "      * test si la séquence appartient a plusieurs cas\n",
    "    * list_numero_voie_matching_inpi,\n",
    "    *  list_numero_voie_matching_insee,\n",
    "    * intersection_numero_voie\n",
    "    * union_numero_voie\n",
    "    * test_list_num_voie: Numéros contenus dans list_numero_voie_matching_inpi sont dans list_numero_voie_matching_insee alors True, sinon False. Si liste Null alors Null\n",
    "    *  datecreationetablissement, \n",
    "    * date_debut_activite,\n",
    "    * test_date\n",
    "    * etatadministratifetablissement, \n",
    "    * status_admin,\n",
    "    * test_status_admin\n",
    "    * etablissementsiege, \n",
    "    * status_ets,\n",
    "    * test_siege\n",
    "    * codecommuneetablissement, \n",
    "    * code_commune, \n",
    "    * test_code_commune\n",
    "    * codepostaletablissement, \n",
    "    * code_postal_matching, \n",
    "    * numerovoieetablissement, \n",
    "    * numero_voie_matching,\n",
    "    * test_numero_voie\n",
    "    * typevoieetablissement, \n",
    "    *  type_voie_matching,\n",
    "    * test_type_voie\n",
    "    * list_inpi,  \n",
    "    *   lenght_list_inpi,\n",
    "    *   list_insee,\n",
    "    *   lenght_list_insee,\n",
    "    *   inpi_except,\n",
    "    *   insee_except,\n",
    "    *   intersection,\n",
    "    *   union_\n",
    "    * status_cas:\n",
    "      * Cas 1, 2,3,4,5,6,7\n",
    "    * index_id_dup_has_cas_1_3_4\n",
    "      * MAX(test_adresse_cas_1_3_4) by index_id\n",
    "      * Informe si l’un des doublons de l’index peut etre trouvé via les cas 1,3 ou 4\n",
    "    * test_duplicates_is_in_cas_1_3_4\n",
    "      * test_adresse_cas_1_3_4 = 'True' AND index_id_dup_has_cas_1_3_4 = 'True' AND count_inpi_index_id_siret > 1 THEN 'TO_KEEP' WHEN test_adresse_cas_1_3_4 = 'False' AND index_id_dup_has_cas_1_3_4 = 'True' AND count_inpi_index_id_siret > 1 THEN 'TO_REMOVE' WHEN count_inpi_index_id_siret = 1 THEN 'NULL' ELSE 'TO_FIND' END AS test_duplicates_is_in_cas_1_3_4\n",
    "      * Indique si la ligne peut etre supprimée car possibilité d’être trouvé via cas 1,3,4\n",
    "    * test_adresse_cas_1_3_4:\n",
    "      * test.status_cas = 'CAS_1' OR test.status_cas = 'CAS_3' OR  test.status_cas = 'CAS_4\n",
    "      * indique si la ligne appartient a un full match ou pas\n",
    "    * enseigne, → enelver les accents\n",
    "    *  enseigne1etablissement\n",
    "    * enseigne2etablissement,\n",
    "    *  enseigne3etablissement, \n",
    "    * test_enseigne:  Si une des enseignes de l’INSEE est contenue dans l’INPI alors True else False, If une des enseignes INPI ou INSEE est null alors null\n",
    "  * Pour fournir le taux de matching, il faut créer un ensemble de test pour chacun des cas (1 à 7).\n",
    "  * Pour fournir le taux de matching, il faut créer un ensemble de test pour chacun des cas (1 à 7).\n",
    "\n",
    "## Metadata\n",
    "\n",
    "* Metadata parameters are available here: Ressources_suDYJ#_luZqd\n",
    "* Task type:\n",
    "  * Jupyter Notebook\n",
    "* Users: :\n",
    "  * Thomas Pernet\n",
    "* Watchers:\n",
    "  * Thomas Pernet\n",
    "* Estimated Log points:\n",
    "  * One being a simple task, 15 a very difficult one\n",
    "  *  9\n",
    "* Task tag\n",
    "  *  #sql-query,#matching,#test-codependance,#siretisation\n",
    "* Toggl Tag\n",
    "  * #data-analysis\n",
    "* Instance [AWS/GCP]\n",
    "  *   \n",
    "  \n",
    "## Input Cloud Storage [AWS/GCP]\n",
    "\n",
    "If link from the internet, save it to the cloud first\n",
    "\n",
    "### Tables [AWS/BigQuery]\n",
    "\n",
    "1. Batch 1:\n",
    "  * Select Provider: Athena\n",
    "  * Select table(s): ets_insee_inpi\n",
    "    * Select only tables created from the same notebook, else copy/paste selection to add new input tables\n",
    "    * If table(s) does not exist, add them: Add New Table\n",
    "    * Information:\n",
    "      * Region: \n",
    "        * Name: Europe (Paris)\n",
    "        * Code: eu-west-3\n",
    "      * Database: inpi\n",
    "      * Notebook construction file: https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/03_ETS_add_variables.md\n",
    "    \n",
    "## Destination Output/Delivery\n",
    "\n",
    "* AWS\n",
    "  1. Athena: \n",
    "      * Region: Europe (Paris)\n",
    "      * Database: inpi\n",
    "      * Tables (Add name new table): ets_inpi_insee_cases\n",
    "      * List new tables\n",
    "      * ets_inpi_insee_cases\n",
    "  \n",
    "## Things to know (Steps, Attention points or new flow of information)\n",
    "\n",
    "## Similarité entre deux adresses\n",
    "\n",
    "Le rapprochement entre les deux tables, à savoir l’INSEE et l’INPI, va amener à la création de deux vecteurs d’adresse. Un vecteur avec des mots contenus spécifiquement à l’INSEE, et un second vecteur avec les mots de l’adresse de l’INPI. Notre objectif est de comparé ses deux vecteurs pour définir si ils sont identiques ou non. Nous avons distingué 7 cas de figures possibles entre les deux vecteurs (figure 1).\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1Qj_HooHrhFYSuTsoqFbl4Vxy9tN3V5Bu)\n",
    "\n",
    "\n",
    "### Sources of information  (meeting notes, Documentation, Query, URL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connexion serveur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsPy.aws_authorization import aws_connector\n",
    "from awsPy.aws_athena import service_athena\n",
    "from awsPy.aws_s3 import service_s3\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, shutil\n",
    "bucket = 'calfdata'\n",
    "path = os.getcwd()\n",
    "parent_path = str(Path(path).parent)\n",
    "path_cred = r\"{}/credential_AWS.json\".format(parent_path)\n",
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                       region = 'eu-west-3')\n",
    "client= con.client_boto()\n",
    "s3 = service_s3.connect_S3(client = client,\n",
    "                      bucket = 'calfdata', verbose = False) \n",
    "athena = service_athena.connect_athena(client = client,\n",
    "                      bucket = 'calfdata') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation table analyse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_table = False\n",
    "if drop_table:\n",
    "    output = athena.run_query(\n",
    "        query=\"DROP TABLE `ets_inpi_insee_cases`;\",\n",
    "        database='inpi',\n",
    "        s3_output='INPI/sql_output'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table = \"\"\"\n",
    "/*match insee inpi 7 cas de figs*/\n",
    "CREATE TABLE inpi.ets_inpi_insee_cases WITH (format = 'PARQUET') AS WITH test_proba AS (\n",
    "  SELECT \n",
    "    count_initial_insee, \n",
    "    index_id, \n",
    "    sequence_id, \n",
    "    siren, \n",
    "    siret, \n",
    "    Coalesce(\n",
    "      try(\n",
    "        date_parse(\n",
    "          datecreationetablissement, '%Y-%m-%d'\n",
    "        )\n",
    "      ), \n",
    "      try(\n",
    "        date_parse(\n",
    "          datecreationetablissement, '%Y-%m-%d %hh:%mm:%ss.SSS'\n",
    "        )\n",
    "      ), \n",
    "      try(\n",
    "        date_parse(\n",
    "          datecreationetablissement, '%Y-%m-%d %hh:%mm:%ss'\n",
    "        )\n",
    "      ), \n",
    "      try(\n",
    "        cast(\n",
    "          datecreationetablissement as timestamp\n",
    "        )\n",
    "      )\n",
    "    ) as datecreationetablissement, \n",
    "    Coalesce(\n",
    "      try(\n",
    "        date_parse(\n",
    "          \"date_début_activité\", '%Y-%m-%d'\n",
    "        )\n",
    "      ), \n",
    "      try(\n",
    "        date_parse(\n",
    "          \"date_début_activité\", '%Y-%m-%d %hh:%mm:%ss.SSS'\n",
    "        )\n",
    "      ), \n",
    "      try(\n",
    "        date_parse(\n",
    "          \"date_début_activité\", '%Y-%m-%d %hh:%mm:%ss'\n",
    "        )\n",
    "      ), \n",
    "      try(\n",
    "        cast(\n",
    "          \"date_début_activité\" as timestamp\n",
    "        )\n",
    "      )\n",
    "    ) as date_debut_activite, \n",
    "    etatadministratifetablissement, \n",
    "    status_admin, \n",
    "    etablissementsiege, \n",
    "    status_ets, \n",
    "    codecommuneetablissement, \n",
    "    code_commune, \n",
    "    codepostaletablissement, \n",
    "    code_postal_matching, \n",
    "    numerovoieetablissement, \n",
    "    numero_voie_matching, \n",
    "    typevoieetablissement, \n",
    "    type_voie_matching, \n",
    "    adresse_distance_inpi, \n",
    "    adresse_distance_insee, \n",
    "    list_numero_voie_matching_inpi, \n",
    "    list_numero_voie_matching_insee, \n",
    "    array_distinct(\n",
    "      split(adresse_distance_inpi, ' ')\n",
    "    ) as list_inpi, \n",
    "    cardinality(\n",
    "      array_distinct(\n",
    "        split(adresse_distance_inpi, ' ')\n",
    "      )\n",
    "    ) as lenght_list_inpi, \n",
    "    array_distinct(\n",
    "      split(adresse_distance_insee, ' ')\n",
    "    ) as list_insee, \n",
    "    cardinality(\n",
    "      array_distinct(\n",
    "        split(adresse_distance_insee, ' ')\n",
    "      )\n",
    "    ) as lenght_list_insee, \n",
    "    array_distinct(\n",
    "      array_except(\n",
    "        split(adresse_distance_insee, ' '), \n",
    "        split(adresse_distance_inpi, ' ')\n",
    "      )\n",
    "    ) as insee_except, \n",
    "    array_distinct(\n",
    "      array_except(\n",
    "        split(adresse_distance_inpi, ' '), \n",
    "        split(adresse_distance_insee, ' ')\n",
    "      )\n",
    "    ) as inpi_except, \n",
    "    CAST(\n",
    "      cardinality(\n",
    "        array_distinct(\n",
    "          array_intersect(\n",
    "            split(adresse_distance_inpi, ' '), \n",
    "            split(adresse_distance_insee, ' ')\n",
    "          )\n",
    "        )\n",
    "      ) AS DECIMAL(10, 2)\n",
    "    ) as intersection, \n",
    "    CAST(\n",
    "      cardinality(\n",
    "        array_distinct(\n",
    "          array_union(\n",
    "            split(adresse_distance_inpi, ' '), \n",
    "            split(adresse_distance_insee, ' ')\n",
    "          )\n",
    "        )\n",
    "      ) AS DECIMAL(10, 2)\n",
    "    ) as union_, \n",
    "    CAST(\n",
    "      cardinality(\n",
    "        array_distinct(\n",
    "          array_intersect(\n",
    "            list_numero_voie_matching_inpi, \n",
    "            list_numero_voie_matching_insee\n",
    "          )\n",
    "        )\n",
    "      ) AS DECIMAL(10, 2)\n",
    "    ) as intersection_numero_voie, \n",
    "    CAST(\n",
    "      cardinality(\n",
    "        array_distinct(\n",
    "          array_union(\n",
    "            list_numero_voie_matching_inpi, \n",
    "            list_numero_voie_matching_insee\n",
    "          )\n",
    "        )\n",
    "      ) AS DECIMAL(10, 2)\n",
    "    ) as union_numero_voie, \n",
    "    REGEXP_REPLACE(\n",
    "      NORMALIZE(enseigne, NFD), \n",
    "      '\\pM', \n",
    "      ''\n",
    "    ) AS enseigne, \n",
    "    enseigne1etablissement, \n",
    "    enseigne2etablissement, \n",
    "    enseigne3etablissement, \n",
    "    array_remove(\n",
    "      array_distinct(\n",
    "        SPLIT(\n",
    "          concat(\n",
    "            enseigne1etablissement, ',', enseigne2etablissement, \n",
    "            ',', enseigne3etablissement\n",
    "          ), \n",
    "          ','\n",
    "        )\n",
    "      ), \n",
    "      ''\n",
    "    ) as test, \n",
    "    contains(\n",
    "      array_remove(\n",
    "        array_distinct(\n",
    "          SPLIT(\n",
    "            concat(\n",
    "              enseigne1etablissement, ',', enseigne2etablissement, \n",
    "              ',', enseigne3etablissement\n",
    "            ), \n",
    "            ','\n",
    "          )\n",
    "        ), \n",
    "        ''\n",
    "      ), \n",
    "      REGEXP_REPLACE(\n",
    "        NORMALIZE(enseigne, NFD), \n",
    "        '\\pM', \n",
    "        ''\n",
    "      )\n",
    "    ) AS temp_test_enseigne \n",
    "  FROM \n",
    "    \"inpi\".\"ets_insee_inpi\" -- limit 10\n",
    "    ) \n",
    "SELECT \n",
    "  * \n",
    "FROM \n",
    "  (\n",
    "    WITH test_rules AS (\n",
    "      SELECT \n",
    "        ROW_NUMBER() OVER () AS row_id, \n",
    "        count_initial_insee, \n",
    "        index_id, \n",
    "        sequence_id, \n",
    "        siren, \n",
    "        siret, \n",
    "        CASE WHEN cardinality(list_numero_voie_matching_inpi) = 0 THEN NULL ELSE list_numero_voie_matching_inpi END as list_numero_voie_matching_inpi, \n",
    "        CASE WHEN cardinality(\n",
    "          list_numero_voie_matching_insee\n",
    "        ) = 0 THEN NULL ELSE list_numero_voie_matching_insee END as list_numero_voie_matching_insee, \n",
    "        intersection_numero_voie, \n",
    "        union_numero_voie, \n",
    "        CASE WHEN intersection_numero_voie = union_numero_voie \n",
    "        AND (\n",
    "          intersection_numero_voie IS NOT NULL \n",
    "          OR union_numero_voie IS NOT NULL\n",
    "        ) THEN 'True' WHEN (\n",
    "          intersection_numero_voie IS NULL \n",
    "          OR union_numero_voie IS NULL\n",
    "        ) THEN 'NULL' ELSE 'False' END AS test_list_num_voie, \n",
    "        datecreationetablissement, \n",
    "        date_debut_activite, \n",
    "        CASE WHEN datecreationetablissement = date_debut_activite THEN 'True' WHEN datecreationetablissement IS NULL \n",
    "        OR date_debut_activite IS NULL THEN 'NULL' --WHEN datecreationetablissement = '' \n",
    "        ELSE 'False' END AS test_date, \n",
    "        etatadministratifetablissement, \n",
    "        status_admin, \n",
    "        CASE WHEN etatadministratifetablissement = status_admin THEN 'True' WHEN etatadministratifetablissement IS NULL \n",
    "        OR status_admin IS NULL THEN 'NULL' WHEN etatadministratifetablissement = '' \n",
    "        OR status_admin = '' THEN 'NULL' ELSE 'False' END AS test_status_admin, \n",
    "        etablissementsiege, \n",
    "        status_ets, \n",
    "        CASE WHEN etablissementsiege = status_ets THEN 'True' WHEN etablissementsiege IS NULL \n",
    "        OR status_ets IS NULL THEN 'NULL' WHEN etablissementsiege = '' \n",
    "        OR status_ets = '' THEN 'NULL' ELSE 'False' END AS test_siege, \n",
    "        codecommuneetablissement, \n",
    "        code_commune, \n",
    "        CASE WHEN codecommuneetablissement = code_commune THEN 'True' WHEN codecommuneetablissement IS NULL \n",
    "        OR code_commune IS NULL THEN 'NULL' WHEN codecommuneetablissement = '' \n",
    "        OR code_commune = '' THEN 'NULL' ELSE 'False' END AS test_code_commune, \n",
    "        codepostaletablissement, \n",
    "        code_postal_matching, \n",
    "        numerovoieetablissement, \n",
    "        numero_voie_matching, \n",
    "        CASE WHEN numerovoieetablissement = numero_voie_matching THEN 'True' WHEN numerovoieetablissement IS NULL \n",
    "        OR numero_voie_matching IS NULL THEN 'NULL' WHEN numerovoieetablissement = '' \n",
    "        OR numero_voie_matching = '' THEN 'NULL' ELSE 'False' END AS test_numero_voie, \n",
    "        typevoieetablissement, \n",
    "        type_voie_matching, \n",
    "        CASE WHEN typevoieetablissement = type_voie_matching THEN 'True' WHEN typevoieetablissement IS NULL \n",
    "        OR type_voie_matching IS NULL THEN 'NULL' WHEN typevoieetablissement = '' \n",
    "        OR type_voie_matching = '' THEN 'NULL' ELSE 'False' END AS test_type_voie, \n",
    "        CASE WHEN cardinality(list_inpi) = 0 THEN NULL ELSE list_inpi END as list_inpi, \n",
    "        lenght_list_inpi, \n",
    "        CASE WHEN cardinality(list_insee) = 0 THEN NULL ELSE list_insee END as list_insee, \n",
    "        lenght_list_insee, \n",
    "        CASE WHEN cardinality(inpi_except) = 0 THEN NULL ELSE inpi_except END as inpi_except, \n",
    "        CASE WHEN cardinality(insee_except) = 0 THEN NULL ELSE insee_except END as insee_except, \n",
    "        intersection, \n",
    "        union_, \n",
    "        intersection / union_ as pct_intersection, \n",
    "        cardinality(inpi_except) AS len_inpi_except, \n",
    "        cardinality(insee_except) AS len_insee_except, \n",
    "        CASE WHEN intersection = union_ THEN 'CAS_1' WHEN intersection = 0 THEN 'CAS_2' WHEN lenght_list_inpi = intersection \n",
    "        AND intersection != union_ THEN 'CAS_3' WHEN lenght_list_insee = intersection \n",
    "        AND intersection != union_ THEN 'CAS_4' WHEN cardinality(insee_except) = cardinality(inpi_except) \n",
    "        AND intersection != 0 \n",
    "        AND cardinality(insee_except) > 0 THEN 'CAS_5' WHEN cardinality(insee_except) > cardinality(inpi_except) \n",
    "        AND intersection != 0 \n",
    "        AND cardinality(insee_except) > 0 \n",
    "        AND cardinality(inpi_except) > 0 THEN 'CAS_6' WHEN cardinality(insee_except) < cardinality(inpi_except) \n",
    "        AND intersection != 0 \n",
    "        AND cardinality(insee_except) > 0 \n",
    "        AND cardinality(inpi_except) > 0 THEN 'CAS_7' ELSE 'CAS_NO_ADRESSE' END AS status_cas, \n",
    "        enseigne, \n",
    "        enseigne1etablissement, \n",
    "        enseigne2etablissement, \n",
    "        enseigne3etablissement, \n",
    "        CASE WHEN cardinality(test) = 0 THEN 'NULL' WHEN enseigne = '' THEN 'NULL' WHEN temp_test_enseigne = TRUE THEN 'True' ELSE 'False' END AS test_enseigne \n",
    "      FROM \n",
    "        test_proba\n",
    "      \n",
    "    ) \n",
    "    \n",
    "    SELECT *\n",
    "    FROM (\n",
    "      WITH test AS(\n",
    "        SELECT *,\n",
    "        CASE WHEN status_cas = 'CAS_1' OR\n",
    "        status_cas = 'CAS_3' OR \n",
    "        status_cas = 'CAS_4' THEN 'True' ELSE 'False' END AS test_adresse_cas_1_3_4 \n",
    "        FROM test_rules\n",
    "        WHERE test_list_num_voie != 'False' and status_cas != 'CAS_2'\n",
    "        )\n",
    "    \n",
    "    SELECT \n",
    "      row_id, \n",
    "      test.index_id, \n",
    "      test.sequence_id, \n",
    "      test.siren, \n",
    "      test.siret,\n",
    "      \n",
    "      count_initial_insee, \n",
    "      count_inpi_siren_siret, \n",
    "      count_inpi_siren_sequence, \n",
    "      count_inpi_sequence_siret, \n",
    "      count_inpi_sequence_stat_cas_siret,\n",
    "      count_inpi_index_id_siret,\n",
    "      count_inpi_index_id_stat_cas_siret,\n",
    "      count_inpi_index_id_stat_cas,\n",
    "      CASE WHEN count_inpi_index_id_siret > 1 THEN 'True' ELSE 'False' END AS index_id_duplicate,\n",
    "      CASE WHEN count_inpi_sequence_siret = 1 THEN 'True' ELSE 'False' END AS test_sequence_siret,\n",
    "      CASE WHEN count_inpi_index_id_stat_cas_siret = 1 THEN 'True' ELSE 'False' END AS test_index_siret,\n",
    "      CASE WHEN count_initial_insee = count_inpi_siren_siret THEN 'True' ELSE 'False' END AS test_siren_insee_siren_inpi, \n",
    "    \n",
    "      CASE WHEN count_inpi_sequence_siret = count_inpi_sequence_stat_cas_siret THEN 'True' ELSE 'False' END AS test_sequence_siret_many_cas,\n",
    "    \n",
    "      list_numero_voie_matching_inpi, \n",
    "      list_numero_voie_matching_insee, \n",
    "      intersection_numero_voie, \n",
    "      union_numero_voie, \n",
    "      test_list_num_voie, \n",
    "      datecreationetablissement, \n",
    "      date_debut_activite, \n",
    "      test_date, \n",
    "      etatadministratifetablissement, \n",
    "      status_admin, \n",
    "      test_status_admin, \n",
    "      etablissementsiege, \n",
    "      status_ets, \n",
    "      test_siege, \n",
    "      codecommuneetablissement, \n",
    "      code_commune, \n",
    "      test_code_commune, \n",
    "      codepostaletablissement, \n",
    "      code_postal_matching, \n",
    "      numerovoieetablissement, \n",
    "      numero_voie_matching, \n",
    "      test_numero_voie, \n",
    "      typevoieetablissement, \n",
    "      type_voie_matching, \n",
    "      test_type_voie, \n",
    "      list_inpi, \n",
    "      lenght_list_inpi, \n",
    "      list_insee, \n",
    "      lenght_list_insee, \n",
    "      inpi_except, \n",
    "      insee_except, \n",
    "      intersection, \n",
    "      union_, \n",
    "      pct_intersection, \n",
    "      len_inpi_except, \n",
    "      len_insee_except, \n",
    "      test.status_cas, \n",
    "      test_adresse_cas_1_3_4,\n",
    "      index_id_dup_has_cas_1_3_4,\n",
    "      CASE\n",
    "      WHEN test_adresse_cas_1_3_4 = 'True' AND index_id_dup_has_cas_1_3_4 = 'True' AND count_inpi_index_id_siret > 1 THEN 'TO_KEEP' \n",
    "      WHEN test_adresse_cas_1_3_4 = 'False' AND index_id_dup_has_cas_1_3_4 = 'True' AND count_inpi_index_id_siret > 1 THEN 'TO_REMOVE'\n",
    "      WHEN count_inpi_index_id_siret = 1 THEN 'NULL'\n",
    "      ELSE 'TO_FIND' END AS test_duplicates_is_in_cas_1_3_4,\n",
    "      enseigne, \n",
    "      enseigne1etablissement, \n",
    "      enseigne2etablissement, \n",
    "      enseigne3etablissement, \n",
    "      test_enseigne \n",
    "    FROM \n",
    "      test \n",
    "      LEFT JOIN (\n",
    "        SELECT \n",
    "          siren, \n",
    "          COUNT(\n",
    "            DISTINCT(siret)\n",
    "          ) AS count_inpi_siren_siret \n",
    "        FROM \n",
    "          test \n",
    "        GROUP BY \n",
    "          siren\n",
    "      ) AS count_rows_sequence ON test.siren = count_rows_sequence.siren \n",
    "      LEFT JOIN (\n",
    "        SELECT \n",
    "          siren, \n",
    "          COUNT(\n",
    "            DISTINCT(sequence_id)\n",
    "          ) AS count_inpi_siren_sequence \n",
    "        FROM \n",
    "          test \n",
    "        GROUP BY \n",
    "          siren\n",
    "      ) AS count_rows_siren_sequence ON test.siren = count_rows_siren_sequence.siren \n",
    "      LEFT JOIN (\n",
    "        SELECT \n",
    "          sequence_id, \n",
    "          COUNT(\n",
    "            DISTINCT(siret)\n",
    "          ) AS count_inpi_sequence_siret \n",
    "        FROM \n",
    "          test \n",
    "        GROUP BY \n",
    "          sequence_id\n",
    "      ) AS count_rows_siret ON test.sequence_id = count_rows_siret.sequence_id\n",
    "    -- \n",
    "    LEFT JOIN (\n",
    "        SELECT \n",
    "          sequence_id, \n",
    "          status_cas,\n",
    "          COUNT(\n",
    "            DISTINCT(siret)\n",
    "          ) AS count_inpi_sequence_stat_cas_siret \n",
    "        FROM \n",
    "          test \n",
    "        GROUP BY \n",
    "          sequence_id,\n",
    "      status_cas\n",
    "      ) AS count_rows_status_cas_siret ON test.sequence_id = count_rows_status_cas_siret.sequence_id AND\n",
    "    test.status_cas = count_rows_status_cas_siret.status_cas\n",
    "    -- duplicate index\n",
    "    LEFT JOIN (\n",
    "        SELECT \n",
    "          index_id, \n",
    "          COUNT(\n",
    "            DISTINCT(siret)\n",
    "          ) AS count_inpi_index_id_siret \n",
    "        FROM \n",
    "          test \n",
    "        GROUP BY \n",
    "          index_id\n",
    "      ) AS count_rows_index_id_siret ON test.index_id = count_rows_index_id_siret.index_id\n",
    "    -- duplicate index cas\n",
    "    LEFT JOIN (\n",
    "        SELECT \n",
    "          index_id, \n",
    "          status_cas,\n",
    "          COUNT(\n",
    "            DISTINCT(siret)\n",
    "          ) AS count_inpi_index_id_stat_cas_siret \n",
    "        FROM \n",
    "          test_rules \n",
    "        GROUP BY \n",
    "          index_id,\n",
    "          status_cas\n",
    "      ) AS count_rows_index_status_cas_siret ON test.index_id = count_rows_index_status_cas_siret.index_id AND\n",
    "    test.status_cas = count_rows_index_status_cas_siret.status_cas\n",
    "    -- nb de cas par index\n",
    "    LEFT JOIN (\n",
    "        SELECT \n",
    "          index_id, \n",
    "          COUNT(\n",
    "            DISTINCT(status_cas)\n",
    "          ) AS count_inpi_index_id_stat_cas\n",
    "        FROM \n",
    "           test \n",
    "        GROUP BY \n",
    "          index_id\n",
    "      ) AS count_rows_index_status_cas ON test.index_id = count_rows_index_status_cas.index_id\n",
    "   LEFT JOIN (\n",
    "     SELECT \n",
    "     index_id,\n",
    "     MAX(test_adresse_cas_1_3_4) AS index_id_dup_has_cas_1_3_4\n",
    "     FROM test\n",
    "     GROUP BY index_id\n",
    "     ) AS  is_index_id_dup_has_cas_1_3_4 ON test.index_id = is_index_id_dup_has_cas_1_3_4.index_id\n",
    "  )\n",
    " )\n",
    "\"\"\"\n",
    "output = athena.run_query(\n",
    "        query=create_table,\n",
    "        database='inpi',\n",
    "        s3_output='INPI/sql_output'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create table par cas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation functions\n",
    "\n",
    "La fonction ci dessous va générer le tableau d'analayse via une query, et retourne un dataframe Pandas, tout en stockant le resultat dans le dossier suivant:\n",
    "\n",
    "- [calfdata/TEMP_ANALYSE_SIRETISATION/INDEX_20](https://s3.console.aws.amazon.com/s3/buckets/calfdata/TEMP_ANALYSE_SIRETISATION/INDEX_20/?region=eu-west-3&tab=overview)\n",
    "- [calfdata/TEMP_ANALYSE_SIRETISATION/INDEX_20_TRUE](https://s3.console.aws.amazon.com/s3/buckets/calfdata/TEMP_ANALYSE_SIRETISATION/INDEX_20_TRUE/?region=eu-west-3&tab=overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = (pd.DataFrame(data = {'index_unique': range(1,21)})\n",
    "       .to_csv('index_20.csv', index = False)\n",
    "      )\n",
    "\n",
    "s3.upload_file(file_to_upload = 'index_20.csv',\n",
    "            destination_in_s3 = 'TEMP_ANALYSE_SIRETISATION/INDEX_20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table = \"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS inpi.index_20 (\n",
    "`index_unique`                     integer\n",
    "    )\n",
    "     ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
    "    WITH SERDEPROPERTIES (\n",
    "   'separatorChar' = ',',\n",
    "   'quoteChar' = '\"'\n",
    "   )\n",
    "     LOCATION 's3://calfdata/TEMP_ANALYSE_SIRETISATION/INDEX_20'\n",
    "     TBLPROPERTIES ('has_encrypted_data'='false',\n",
    "              'skip.header.line.count'='1');\"\"\"\n",
    "output = athena.run_query(\n",
    "        query=create_table,\n",
    "        database='inpi',\n",
    "        s3_output='INPI/sql_output'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = range(1,10)\n",
    "b = [\"True\", \"False\", \"NULL\"]\n",
    "\n",
    "\n",
    "\n",
    "index = pd.MultiIndex.from_product([a, b], names = [\"index_unique\", \"groups\"])\n",
    "\n",
    "df_ = (pd.DataFrame(index = index)\n",
    "       .reset_index()\n",
    "       .sort_values(by = [\"index_unique\", \"groups\"])\n",
    "       .to_csv('index_20_true.csv', index = False)\n",
    "      )\n",
    "\n",
    "s3.upload_file(file_to_upload = 'index_20_true.csv',\n",
    "            destination_in_s3 = 'TEMP_ANALYSE_SIRETISATION/INDEX_20_TRUE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table = \"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS inpi.index_20_true (\n",
    "`index_unique`                     integer,\n",
    "`groups`                     string\n",
    "\n",
    "    )\n",
    "     ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
    "    WITH SERDEPROPERTIES (\n",
    "   'separatorChar' = ',',\n",
    "   'quoteChar' = '\"'\n",
    "   )\n",
    "     LOCATION 's3://calfdata/TEMP_ANALYSE_SIRETISATION/INDEX_20_TRUE'\n",
    "     TBLPROPERTIES ('has_encrypted_data'='false',\n",
    "              'skip.header.line.count'='1');\"\"\"\n",
    "output = athena.run_query(\n",
    "        query=create_table,\n",
    "        database='inpi',\n",
    "        s3_output='INPI/sql_output'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nombre ets par cas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT status_cas, COUNT(*) as count\n",
    "FROM ets_inpi_insee_cases \n",
    "GROUP BY status_cas\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nombre etb unique INSEE par cas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT status_cas, COUNT(DISTINCT(index_id)) as distinct_ets\n",
    "FROM ets_inpi_insee_cases \n",
    "GROUP BY status_cas\n",
    "ORDER BY status_cas\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT * \n",
    "FROM (\n",
    "SELECT status_cas, count_initial_insee, COUNT(*) as count\n",
    "FROM ets_inpi_insee_cases \n",
    "GROUP BY status_cas, count_initial_insee\n",
    "  )\n",
    "  WHERE count_initial_insee = 1\n",
    "ORDER BY status_cas, count_initial_insee\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution somme enseigne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "  approx_percentile(sum_enseigne, ARRAY[0.25,0.50,0.75,.80,.85,.86,.87, .88, .89,.90,0.95, 0.99]) as sum_enseigne\n",
    "FROM \n",
    "  ets_inpi_insee_cases \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nombre d'index a trouver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT COUNT(DISTINCT(index_id))\n",
    "FROM ets_inpi_insee_cases \n",
    "\"\"\"\n",
    "\n",
    "output = athena.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output='INPI/sql_output'\n",
    "        )\n",
    "\n",
    "results = False\n",
    "filename = 'index_a_trouver.csv'\n",
    "\n",
    "while results != True:\n",
    "    source_key = \"{}/{}.csv\".format(\n",
    "                                'INPI/sql_output',\n",
    "                                output['QueryExecutionId']\n",
    "                                       )\n",
    "    destination_key = \"{}/{}\".format(\n",
    "                                    'ANALYSE_PRE_SIRETISATION',\n",
    "                                    filename\n",
    "                                )\n",
    "\n",
    "    results = s3.copy_object_s3(\n",
    "                                    source_key = source_key,\n",
    "                                    destination_key = destination_key,\n",
    "                                    remove = True\n",
    "                                )\n",
    "    \n",
    "nb_index = (s3.read_df_from_s3(\n",
    "            key = 'ANALYSE_PRE_SIRETISATION/{}'.format(filename), sep = ',')\n",
    "         )\n",
    "nb_index.values[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verification nombre index post-filtre\n",
    "\n",
    "Lors de la création de la table `ets_inpi_insee_cases`, nous avons exclu les lignes dont le `status_cas` était différent de `CAS_2`, a savoir aucun mot en commun dans l'adresse. De plus, nous avons filré toutes les lignes n'ayant aucun chiffre en commun. \n",
    "\n",
    "un troisième filtre peut être appliqué, lorsque la variable `index_id` a des doublons (plusieurs siret), et qu'au moins une des lignes peut être retrouvée via le cas de figure 1, 3 ou 4. \n",
    "\n",
    "Ci dessous un exemple d'`index_id` qui satisfait la troisième condition:\n",
    "\n",
    "L'index 1142 a deux siret possibles, toutefois l'un des deux peut être retrouvé via le `cas_1`. La variable `test_adresse_cas_1_3_4` indique si la ligne fait partie des cas 1, 3 ou 4, alors que la variable `index_id_dup_has_cas_1_3_4` informe si la séquence à au moins une des lignes fait partie des cas 1, 3 ou 4. Le test `test_duplicates_is_in_cas_1_3_4` résume les possiblités, a savoir `TO_KEEP` si il faut garder la ligne, `TO_REMOVE` si il faut la supprimer, `TO_FIND` au cas ou la séquence ne possède pas de cas 1,3 ou 4 (cf exemple ci dessous) ou `NULL` si la séquence n'a pas de doublon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT index_id, sequence_id, siren, siret, count_inpi_index_id_siret,\n",
    "list_inpi,list_insee, inpi_except, insee_except, status_cas, test_adresse_cas_1_3_4,\n",
    "index_id_dup_has_cas_1_3_4, test_duplicates_is_in_cas_1_3_4  \n",
    "FROM ets_inpi_insee_cases \n",
    "WHERE index_id = 1142\n",
    "\"\"\"\n",
    "output = athena.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output='INPI/sql_output'\n",
    "        )\n",
    "\n",
    "results = False\n",
    "filename_ligne = 'exemple_index_id_1142.csv'\n",
    "\n",
    "while results != True:\n",
    "    source_key = \"{}/{}.csv\".format(\n",
    "                                'INPI/sql_output',\n",
    "                                output['QueryExecutionId']\n",
    "                                       )\n",
    "    destination_key = \"{}/{}\".format(\n",
    "                                    'ANALYSE_PRE_SIRETISATION',\n",
    "                                    filename_ligne\n",
    "                                )\n",
    "\n",
    "    results = s3.copy_object_s3(\n",
    "                                    source_key = source_key,\n",
    "                                    destination_key = destination_key,\n",
    "                                    remove = True\n",
    "                                )\n",
    "(s3.read_df_from_s3(\n",
    "            key = 'ANALYSE_PRE_SIRETISATION/{}'.format(filename_ligne), sep = ',')\n",
    " .set_index(['index_id', 'sequence_id', 'siren', 'count_inpi_index_id_siret'])\n",
    "         )    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple d'index id sans cas de figure 1, 3, ou 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT index_id, sequence_id, siren, siret, count_inpi_index_id_siret, \n",
    "list_inpi,list_insee, inpi_except, insee_except, test_adresse_cas_1_3_4,\n",
    "index_id_dup_has_cas_1_3_4, test_duplicates_is_in_cas_1_3_4  \n",
    "FROM ets_inpi_insee_cases \n",
    "WHERE index_id = 4560\n",
    "\"\"\"\n",
    "output = athena.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output='INPI/sql_output'\n",
    "        )\n",
    "\n",
    "results = False\n",
    "filename_ligne = 'exemple_index_id_4560.csv'\n",
    "\n",
    "while results != True:\n",
    "    source_key = \"{}/{}.csv\".format(\n",
    "                                'INPI/sql_output',\n",
    "                                output['QueryExecutionId']\n",
    "                                       )\n",
    "    destination_key = \"{}/{}\".format(\n",
    "                                    'ANALYSE_PRE_SIRETISATION',\n",
    "                                    filename_ligne\n",
    "                                )\n",
    "\n",
    "    results = s3.copy_object_s3(\n",
    "                                    source_key = source_key,\n",
    "                                    destination_key = destination_key,\n",
    "                                    remove = True\n",
    "                                )\n",
    "(s3.read_df_from_s3(\n",
    "            key = 'ANALYSE_PRE_SIRETISATION/{}'.format(filename_ligne), sep = ',')\n",
    " .set_index(['index_id', 'sequence_id', 'siren', 'count_inpi_index_id_siret'])\n",
    "         )    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le nombre d'index avant se filtre et de:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT COUNT(DISTINCT(index_id))\n",
    "FROM ets_inpi_insee_cases \n",
    "-- WHERE test_duplicates_is_in_cas_1_3_4 !=  'TO_REMOVE'\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "output = athena.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output='INPI/sql_output'\n",
    "        )\n",
    "\n",
    "results = False\n",
    "filename = 'index_a_trouver.csv'\n",
    "\n",
    "while results != True:\n",
    "    source_key = \"{}/{}.csv\".format(\n",
    "                                'INPI/sql_output',\n",
    "                                output['QueryExecutionId']\n",
    "                                       )\n",
    "    destination_key = \"{}/{}\".format(\n",
    "                                    'ANALYSE_PRE_SIRETISATION',\n",
    "                                    filename\n",
    "                                )\n",
    "\n",
    "    results = s3.copy_object_s3(\n",
    "                                    source_key = source_key,\n",
    "                                    destination_key = destination_key,\n",
    "                                    remove = True\n",
    "                                )\n",
    "    \n",
    "nb_index_before = (s3.read_df_from_s3(\n",
    "            key = 'ANALYSE_PRE_SIRETISATION/{}'.format(filename), sep = ',')\n",
    "         )\n",
    "nb_index_before.values[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le nombre d'index après se filtre et de:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT COUNT(DISTINCT(index_id))\n",
    "FROM ets_inpi_insee_cases \n",
    "WHERE test_duplicates_is_in_cas_1_3_4 !=  'TO_REMOVE'\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "output = athena.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output='INPI/sql_output'\n",
    "        )\n",
    "\n",
    "results = False\n",
    "filename = 'index_a_trouver_remove_test_duplicates_is_in_cas_1_3_4.csv'\n",
    "\n",
    "while results != True:\n",
    "    source_key = \"{}/{}.csv\".format(\n",
    "                                'INPI/sql_output',\n",
    "                                output['QueryExecutionId']\n",
    "                                       )\n",
    "    destination_key = \"{}/{}\".format(\n",
    "                                    'ANALYSE_PRE_SIRETISATION',\n",
    "                                    filename\n",
    "                                )\n",
    "\n",
    "    results = s3.copy_object_s3(\n",
    "                                    source_key = source_key,\n",
    "                                    destination_key = destination_key,\n",
    "                                    remove = True\n",
    "                                )\n",
    "    \n",
    "nb_index_after = (s3.read_df_from_s3(\n",
    "            key = 'ANALYSE_PRE_SIRETISATION/{}'.format(filename), sep = ',')\n",
    "         )\n",
    "nb_index_after.values[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le nombre d'index doit être identique. Si ce n'est pas le cas, il y a un problème"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_index_before.values[0][0] - nb_index_after.values[0][0] ==  0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le tableau ci dessous récapitule le nombre de lignes selon le status de `test_duplicates_is_in_cas_1_3_4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT count_inpi_index_id_siret,test_duplicates_is_in_cas_1_3_4,  COUNT(index_id) as nb_distinct\n",
    "FROM ets_inpi_insee_cases\n",
    "WHERE index_id_duplicate = 'True'\n",
    "GROUP BY count_inpi_index_id_siret, test_duplicates_is_in_cas_1_3_4\n",
    "ORDER BY count_inpi_index_id_siret, test_duplicates_is_in_cas_1_3_4\n",
    "\"\"\"\n",
    "\n",
    "output = athena.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output='INPI/sql_output'\n",
    "        )\n",
    "\n",
    "results = False\n",
    "filename_multi = 'duplicate_test_filename_multi.csv'\n",
    "\n",
    "while results != True:\n",
    "    source_key = \"{}/{}.csv\".format(\n",
    "                                'INPI/sql_output',\n",
    "                                output['QueryExecutionId']\n",
    "                                       )\n",
    "    destination_key = \"{}/{}\".format(\n",
    "                                    'ANALYSE_PRE_SIRETISATION',\n",
    "                                    filename_multi\n",
    "                                )\n",
    "\n",
    "    results = s3.copy_object_s3(\n",
    "                                    source_key = source_key,\n",
    "                                    destination_key = destination_key,\n",
    "                                    remove = True\n",
    "                                )  \n",
    "    \n",
    "(\n",
    "pd.concat([\n",
    "(test_ligne\n",
    " .groupby('test_duplicates_is_in_cas_1_3_4')['nb_distinct']\n",
    " .sum()\n",
    " .to_frame()\n",
    " .T\n",
    "),\n",
    "    (test_ligne\n",
    " .fillna(0)\n",
    " .set_index(['count_inpi_index_id_siret', 'test_duplicates_is_in_cas_1_3_4'])\n",
    " .unstack(-1)\n",
    "     .droplevel(level = 0, axis = 1)\n",
    ")], axis = 0\n",
    ")\n",
    "    .assign( \n",
    " total_row = lambda x : x.sum(axis = 1)\n",
    " )\n",
    ".style\n",
    " .format(\"{:,.0f}\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nombre ligne duplicate-index\n",
    "\n",
    "Dans le tableau ci dessous, on regarde le nombre de siret possible par index, quelque soit le cas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par exemple, il y a 251,612 lignes avec 2 siret possibles, qui constituent 125,694 index a trouver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Lignes\n",
    "query = \"\"\"\n",
    "SELECT count_inpi_index_id_siret, COUNT(*) as nb_distinct\n",
    "FROM ets_inpi_insee_cases \n",
    "GROUP BY count_inpi_index_id_siret\n",
    "ORDER BY count_inpi_index_id_siret\n",
    "\"\"\"\n",
    "\n",
    "output = athena.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output='INPI/sql_output'\n",
    "        )\n",
    "\n",
    "results = False\n",
    "filename_ligne = 'nb_duplicates_ligne.csv'\n",
    "\n",
    "while results != True:\n",
    "    source_key = \"{}/{}.csv\".format(\n",
    "                                'INPI/sql_output',\n",
    "                                output['QueryExecutionId']\n",
    "                                       )\n",
    "    destination_key = \"{}/{}\".format(\n",
    "                                    'ANALYSE_PRE_SIRETISATION',\n",
    "                                    filename_ligne\n",
    "                                )\n",
    "\n",
    "    results = s3.copy_object_s3(\n",
    "                                    source_key = source_key,\n",
    "                                    destination_key = destination_key,\n",
    "                                    remove = True\n",
    "                                )\n",
    "    \n",
    "##### Index\n",
    "query = \"\"\"\n",
    "SELECT count_inpi_index_id_siret, COUNT(DISTINCT(index_id)) as nb_distinct\n",
    "FROM ets_inpi_insee_cases \n",
    "GROUP BY count_inpi_index_id_siret\n",
    "ORDER BY count_inpi_index_id_siret\n",
    "\"\"\"\n",
    "\n",
    "output = athena.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output='INPI/sql_output'\n",
    "        )\n",
    "\n",
    "results = False\n",
    "filename_index = 'nb_duplicates_index.csv'\n",
    "\n",
    "while results != True:\n",
    "    source_key = \"{}/{}.csv\".format(\n",
    "                                'INPI/sql_output',\n",
    "                                output['QueryExecutionId']\n",
    "                                       )\n",
    "    destination_key = \"{}/{}\".format(\n",
    "                                    'ANALYSE_PRE_SIRETISATION',\n",
    "                                    filename_index\n",
    "                                )\n",
    "\n",
    "    results = s3.copy_object_s3(\n",
    "                                    source_key = source_key,\n",
    "                                    destination_key = destination_key,\n",
    "                                    remove = True\n",
    "                                )\n",
    "    \n",
    "test_ligne = (s3.read_df_from_s3(\n",
    "            key = 'ANALYSE_PRE_SIRETISATION/{}'.format(filename_ligne), sep = ',')\n",
    "         )\n",
    "\n",
    "test_index = (s3.read_df_from_s3(\n",
    "            key = 'ANALYSE_PRE_SIRETISATION/{}'.format(filename_index), sep = ',')\n",
    "         )\n",
    "(\n",
    "pd.concat([    \n",
    " pd.concat([\n",
    "    pd.concat(\n",
    "    [\n",
    "        test_ligne.sum().to_frame().T.rename(index = {0:'total'}), \n",
    "        test_ligne\n",
    "    ], axis = 0),\n",
    "    ],axis = 1,keys=[\"Lignes\"]),\n",
    "    (\n",
    " pd.concat([\n",
    "    pd.concat(\n",
    "    [\n",
    "        test_index.sum().to_frame().T.rename(index = {0:'total'}), \n",
    "        test_index\n",
    "    ], axis = 0),\n",
    "    ],axis = 1,keys=[\"Index\"])\n",
    ")],axis= 1\n",
    "    )\n",
    "    .style\n",
    "    .format(\"{:,.0f}\")\n",
    "                  .bar(subset= [\n",
    "                      ('Lignes','nb_distinct'),\n",
    "                      ('Index','nb_distinct'),\n",
    "                      \n",
    "                  ],\n",
    "                       color='#d65f5f')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nombre de cas par index dupliqué\n",
    "\n",
    "Le tableau ci dessous est intéréssant car il informe sur le nombre de cas de figure possible pour chacun des index dupliqués (plusieurs siret possible). Par exemple, lorsque le nombre de doublon par index est de deux, il est composé de 80,230 lignes (40,073 index) appartenant au cas de figure 1 et 171,382 lignes (85,621) au cas de figure 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Lignes\n",
    "query = \"\"\" \n",
    "SELECT count_inpi_index_id_siret,count_inpi_index_id_stat_cas, COUNT(index_id) as nb_distinct\n",
    "FROM ets_inpi_insee_cases \n",
    "GROUP BY count_inpi_index_id_stat_cas, count_inpi_index_id_siret\n",
    "ORDER BY count_inpi_index_id_siret, count_inpi_index_id_stat_cas\n",
    "\"\"\"\n",
    "output = athena.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output='INPI/sql_output'\n",
    "        )\n",
    "\n",
    "results = False\n",
    "filename_ligne = 'nb_cas_per_duplicate_ligne.csv'\n",
    "\n",
    "while results != True:\n",
    "    source_key = \"{}/{}.csv\".format(\n",
    "                                'INPI/sql_output',\n",
    "                                output['QueryExecutionId']\n",
    "                                       )\n",
    "    destination_key = \"{}/{}\".format(\n",
    "                                    'ANALYSE_PRE_SIRETISATION',\n",
    "                                    filename_ligne\n",
    "                                )\n",
    "\n",
    "    results = s3.copy_object_s3(\n",
    "                                    source_key = source_key,\n",
    "                                    destination_key = destination_key,\n",
    "                                    remove = True\n",
    "                                )\n",
    "\n",
    "#### Index\n",
    "query = \"\"\" \n",
    "SELECT count_inpi_index_id_siret,count_inpi_index_id_stat_cas, COUNT(Distinct(index_id)) as nb_distinct\n",
    "FROM ets_inpi_insee_cases \n",
    "GROUP BY count_inpi_index_id_stat_cas, count_inpi_index_id_siret\n",
    "ORDER BY count_inpi_index_id_siret, count_inpi_index_id_stat_cas\n",
    "\"\"\"\n",
    "output = athena.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output='INPI/sql_output'\n",
    "        )\n",
    "\n",
    "results = False\n",
    "filename_index = 'nb_cas_per_duplicate_index.csv'\n",
    "\n",
    "while results != True:\n",
    "    source_key = \"{}/{}.csv\".format(\n",
    "                                'INPI/sql_output',\n",
    "                                output['QueryExecutionId']\n",
    "                                       )\n",
    "    destination_key = \"{}/{}\".format(\n",
    "                                    'ANALYSE_PRE_SIRETISATION',\n",
    "                                    filename_index\n",
    "                                )\n",
    "\n",
    "    results = s3.copy_object_s3(\n",
    "                                    source_key = source_key,\n",
    "                                    destination_key = destination_key,\n",
    "                                    remove = True\n",
    "                                )\n",
    "test_ligne = s3.read_df_from_s3(\n",
    "            key = 'ANALYSE_PRE_SIRETISATION/{}'.format(filename_ligne), sep = ',')    \n",
    "test_index = s3.read_df_from_s3(\n",
    "            key = 'ANALYSE_PRE_SIRETISATION/{}'.format(filename_index), sep = ',')\n",
    "\n",
    "(\n",
    "    pd.concat([\n",
    "pd.concat([    \n",
    "pd.concat([\n",
    "(\n",
    " test_ligne\n",
    " .set_index(['count_inpi_index_id_siret','count_inpi_index_id_stat_cas'])\n",
    " .unstack(-1)\n",
    " .sum()\n",
    " .to_frame()\n",
    " .unstack(-1)\n",
    " .droplevel(level = 0, axis=1)\n",
    "),\n",
    "(\n",
    " test_ligne\n",
    " .set_index(['count_inpi_index_id_siret','count_inpi_index_id_stat_cas'])\n",
    " .unstack(-1)\n",
    " .fillna(0)\n",
    "    .droplevel(level = 0, axis=1)\n",
    ")], axis = 0)\n",
    "    .assign( \n",
    " total_row = lambda x : x.sum(axis = 1)\n",
    " )],axis = 1, keys=[\"Lignes\"]\n",
    "    ),\n",
    "\n",
    "pd.concat([    \n",
    "pd.concat([\n",
    "(\n",
    " test_index\n",
    " .set_index(['count_inpi_index_id_siret','count_inpi_index_id_stat_cas'])\n",
    " .unstack(-1)\n",
    " .sum()\n",
    " .to_frame()\n",
    " .unstack(-1)\n",
    " .droplevel(level = 0, axis=1)\n",
    "),\n",
    "(\n",
    " test_index\n",
    " .set_index(['count_inpi_index_id_siret','count_inpi_index_id_stat_cas'])\n",
    " .unstack(-1)\n",
    " .fillna(0)\n",
    "    .droplevel(level = 0, axis=1)\n",
    ")], axis = 0)\n",
    "    .assign( \n",
    " total_row = lambda x : x.sum(axis = 1)\n",
    " )],axis = 1, keys=[\"Index\"])],\n",
    "        axis= 1)\n",
    "    .style\n",
    "    .format(\"{:,.0f}\")\n",
    "                  .bar(subset= [\n",
    "                      ('Lignes','total_row'),\n",
    "                      ('Index','total_row'),\n",
    "                      \n",
    "                  ],\n",
    "                       color='#d65f5f')\n",
    "           \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le tableau précédent, nous avons regardé le nombre de cas possibles pour chacun des duplicates. Pour connaitre les cas de figure concernant les duplicates, il faut regarder le tableau ci dessous. Par exemple, il y a 16,594 lignes (14,491 index) pour lesquelles il y a deux doublons par index concernant le cas de figure 2.\n",
    "\n",
    "Le tableau nous informe aussi de cas de figure ou l'ensemble des mots de l'adresse sont identiques, avec aussi les numéros de voie, mais il y a encore des doublons. C'est le cas pour 110,798 lignes (86,652 index). Ce cas peut être trouvé dans la colonne `CAS_1` et `status_cas` supérieur à 1. Lorsque ce genre de cas arrive, il faut appliquer d'avantage de règles que nous véront plus tard dans le notebook.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lignes\n",
    "query = \"\"\" \n",
    "SELECT count_inpi_index_id_siret,status_cas, COUNT(index_id) as nb_distinct\n",
    "FROM ets_inpi_insee_cases \n",
    "GROUP BY status_cas, count_inpi_index_id_siret\n",
    "ORDER BY count_inpi_index_id_siret, status_cas\n",
    "\"\"\"\n",
    "output = athena.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output='INPI/sql_output'\n",
    "        )\n",
    "\n",
    "results = False\n",
    "filename_ligne = 'nb_cas_per_duplicate_ligne.csv'\n",
    "\n",
    "while results != True:\n",
    "    source_key = \"{}/{}.csv\".format(\n",
    "                                'INPI/sql_output',\n",
    "                                output['QueryExecutionId']\n",
    "                                       )\n",
    "    destination_key = \"{}/{}\".format(\n",
    "                                    'ANALYSE_PRE_SIRETISATION',\n",
    "                                    filename_ligne\n",
    "                                )\n",
    "\n",
    "    results = s3.copy_object_s3(\n",
    "                                    source_key = source_key,\n",
    "                                    destination_key = destination_key,\n",
    "                                    remove = True\n",
    "                                )\n",
    "### index\n",
    "query = \"\"\" \n",
    "SELECT count_inpi_index_id_siret,status_cas, COUNT(DISTINCT(index_id)) as nb_distinct\n",
    "FROM ets_inpi_insee_cases \n",
    "GROUP BY status_cas, count_inpi_index_id_siret\n",
    "ORDER BY count_inpi_index_id_siret, status_cas\n",
    "\"\"\"\n",
    "output = athena.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output='INPI/sql_output'\n",
    "        )\n",
    "\n",
    "results = False\n",
    "filename_index = 'nb_cas_per_duplicate_index.csv'\n",
    "\n",
    "while results != True:\n",
    "    source_key = \"{}/{}.csv\".format(\n",
    "                                'INPI/sql_output',\n",
    "                                output['QueryExecutionId']\n",
    "                                       )\n",
    "    destination_key = \"{}/{}\".format(\n",
    "                                    'ANALYSE_PRE_SIRETISATION',\n",
    "                                    filename_index\n",
    "                                )\n",
    "\n",
    "    results = s3.copy_object_s3(\n",
    "                                    source_key = source_key,\n",
    "                                    destination_key = destination_key,\n",
    "                                    remove = True\n",
    "                                )\n",
    "    \n",
    "test_ligne = s3.read_df_from_s3(\n",
    "            key = 'ANALYSE_PRE_SIRETISATION/{}'.format(filename_ligne), sep = ',')    \n",
    "test_index = s3.read_df_from_s3(\n",
    "            key = 'ANALYSE_PRE_SIRETISATION/{}'.format(filename_index), sep = ',')\n",
    "\n",
    "(\n",
    "    pd.concat([\n",
    "pd.concat([    \n",
    "pd.concat([\n",
    "(\n",
    " test_ligne\n",
    " .set_index(['count_inpi_index_id_siret','status_cas'])\n",
    " .unstack(-1)\n",
    " .sum()\n",
    " .to_frame()\n",
    " .unstack(-1)\n",
    " .droplevel(level = 0, axis=1)\n",
    "),\n",
    "(\n",
    " test_ligne\n",
    " .set_index(['count_inpi_index_id_siret','status_cas'])\n",
    " .unstack(-1)\n",
    " .fillna(0)\n",
    "    .droplevel(level = 0, axis=1)\n",
    ")], axis = 0)\n",
    "    .assign( \n",
    " total_row = lambda x : x.sum(axis = 1)\n",
    " )],axis = 1, keys=[\"Lignes\"]\n",
    "    ),\n",
    "\n",
    "pd.concat([    \n",
    "pd.concat([\n",
    "(\n",
    " test_index\n",
    " .set_index(['count_inpi_index_id_siret','status_cas'])\n",
    " .unstack(-1)\n",
    " .sum()\n",
    " .to_frame()\n",
    " .unstack(-1)\n",
    " .droplevel(level = 0, axis=1)\n",
    "),\n",
    "(\n",
    " test_index\n",
    " .set_index(['count_inpi_index_id_siret','status_cas'])\n",
    " .unstack(-1)\n",
    " .fillna(0)\n",
    "    .droplevel(level = 0, axis=1)\n",
    ")], axis = 0)\n",
    "    .assign( \n",
    " total_row = lambda x : x.sum(axis = 1)\n",
    " )],axis = 1, keys=[\"Index\"])],\n",
    "        axis= 1)\n",
    "    .style\n",
    "    .format(\"{:,.0f}\")\n",
    "                  .bar(subset= [\n",
    "                      ('Lignes','total_row'),\n",
    "                      ('Index','total_row'),\n",
    "                      \n",
    "                  ],\n",
    "                       color='#d65f5f')\n",
    "           \n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse `test_siren_insee_siren_inpi`\n",
    "\n",
    "Dans cette partie, nous nous intéréssons aux resultats des tests lorsque `test_siren_insee_siren_inpi` n'est pas égal à `False`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse `test_siren_insee_siren_inpi`\n",
    "\n",
    "Dans ce tableau, le nombre de lignes n'ayant pas de doublon et la méthode sur l'adresse correspond au 3 est égal à  223,608 (223,461). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Lignes\n",
    "query = \"\"\"\n",
    "SELECT status_cas,count_inpi_index_id_siret, COUNT(index_id) as nb_distinct\n",
    "FROM ets_inpi_insee_cases\n",
    "WHERE test_siren_insee_siren_inpi != 'False'  \n",
    "GROUP BY status_cas, count_inpi_index_id_siret\n",
    "ORDER BY status_cas\n",
    "\n",
    "\"\"\"\n",
    "output = athena.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output='INPI/sql_output'\n",
    "        )\n",
    "\n",
    "results = False\n",
    "filename_ligne = 'nb_duplicate_par_cas_test_siren_insee_siren_inpi_ligne.csv'\n",
    "\n",
    "while results != True:\n",
    "    source_key = \"{}/{}.csv\".format(\n",
    "                                'INPI/sql_output',\n",
    "                                output['QueryExecutionId']\n",
    "                                       )\n",
    "    destination_key = \"{}/{}\".format(\n",
    "                                    'ANALYSE_PRE_SIRETISATION',\n",
    "                                    filename_ligne\n",
    "                                )\n",
    "\n",
    "    results = s3.copy_object_s3(\n",
    "                                    source_key = source_key,\n",
    "                                    destination_key = destination_key,\n",
    "                                    remove = True\n",
    "                                )\n",
    "#### index\n",
    "query = \"\"\"\n",
    "SELECT status_cas,count_inpi_index_id_siret, COUNT(DISTINCT(index_id)) as nb_distinct\n",
    "FROM ets_inpi_insee_cases\n",
    "WHERE test_siren_insee_siren_inpi != 'False'  \n",
    "GROUP BY status_cas, count_inpi_index_id_siret\n",
    "ORDER BY status_cas\n",
    "\n",
    "\"\"\"\n",
    "output = athena.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output='INPI/sql_output'\n",
    "        )\n",
    "\n",
    "results = False\n",
    "filename_index = 'nb_duplicate_par_cas_test_siren_insee_siren_inpi_index.csv'\n",
    "\n",
    "while results != True:\n",
    "    source_key = \"{}/{}.csv\".format(\n",
    "                                'INPI/sql_output',\n",
    "                                output['QueryExecutionId']\n",
    "                                       )\n",
    "    destination_key = \"{}/{}\".format(\n",
    "                                    'ANALYSE_PRE_SIRETISATION',\n",
    "                                    filename_index\n",
    "                                )\n",
    "\n",
    "    results = s3.copy_object_s3(\n",
    "                                    source_key = source_key,\n",
    "                                    destination_key = destination_key,\n",
    "                                    remove = True\n",
    "                                )  \n",
    "    \n",
    "test_ligne = s3.read_df_from_s3(\n",
    "            key = 'ANALYSE_PRE_SIRETISATION/{}'.format(filename_ligne), sep = ',')    \n",
    "test_index = s3.read_df_from_s3(\n",
    "            key = 'ANALYSE_PRE_SIRETISATION/{}'.format(filename_index), sep = ',')\n",
    "\n",
    "(\n",
    "    pd.concat([\n",
    "pd.concat([    \n",
    "pd.concat([\n",
    "(\n",
    " test_ligne\n",
    " .set_index(['count_inpi_index_id_siret','status_cas'])\n",
    " .unstack(-1)\n",
    " .sum()\n",
    " .to_frame()\n",
    " .unstack(-1)\n",
    " .droplevel(level = 0, axis=1)\n",
    "),\n",
    "(\n",
    " test_ligne\n",
    " .set_index(['count_inpi_index_id_siret','status_cas'])\n",
    " .unstack(-1)\n",
    " .fillna(0)\n",
    "    .droplevel(level = 0, axis=1)\n",
    ")], axis = 0)\n",
    "    .assign( \n",
    " total_row = lambda x : x.sum(axis = 1)\n",
    " )],axis = 1, keys=[\"Lignes\"]\n",
    "    ),\n",
    "\n",
    "pd.concat([    \n",
    "pd.concat([\n",
    "(\n",
    " test_index\n",
    " .set_index(['count_inpi_index_id_siret','status_cas'])\n",
    " .unstack(-1)\n",
    " .sum()\n",
    " .to_frame()\n",
    " .unstack(-1)\n",
    " .droplevel(level = 0, axis=1)\n",
    "),\n",
    "(\n",
    " test_index\n",
    " .set_index(['count_inpi_index_id_siret','status_cas'])\n",
    " .unstack(-1)\n",
    " .fillna(0)\n",
    "    .droplevel(level = 0, axis=1)\n",
    ")], axis = 0)\n",
    "    .assign( \n",
    " total_row = lambda x : x.sum(axis = 1)\n",
    " )],axis = 1, keys=[\"Index\"])],\n",
    "        axis= 1)\n",
    "    .style\n",
    "    .format(\"{:,.0f}\")\n",
    "                  .bar(subset= [\n",
    "                      ('Lignes','total_row'),\n",
    "                      ('Index','total_row'),\n",
    "                      \n",
    "                  ],\n",
    "                       color='#d65f5f')\n",
    "           \n",
    ")      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse `test_list_num` par rapport aux autres tests\n",
    "\n",
    "Lors de la partie précédente, nous avons mis en évidence des cas ou l'adresse peut être identique en tout point mais possède des doublons. Pour cela, il faut appliquer d'autres tests. Nous avons recensé les tests suivants:\n",
    "\n",
    "- `test_sequence_siret`: Pertinence faible\n",
    "    - `count_inpi_sequence_siret = 1 THEN ‘True’ ELSE ‘False``\n",
    "    - Si la variable est ‘True’ alors, il n’y a pas de duplicate pour une séquence\n",
    "- `test_index_siret`: Pertinence faible\n",
    "    - `count_inpi_index_id_stat_cas_siret = 1 THEN 'True' ELSE 'False'`\n",
    "    - Si la variable est true, alors, il n’y a qu”un seul cas de figure par index \n",
    "- `test_siren_insee_siren_inpi`: Pertinence elevée\n",
    "    - `count_initial_insee = count_inpi_siren_siret THEN 'True' ELSE 'False'`\n",
    "    - Si la variable est ‘True’ alors tous les établissements ont été trouvé\n",
    "- `test_sequence_siret_many_cas`: Pertinence faible\n",
    "    - `count_inpi_sequence_siret = count_inpi_sequence_stat_cas_siret THEN 'True' ELSE 'False`\n",
    "    - test si la séquence appartient a plusieurs cas\n",
    "- `test_date`: Pertinence moyenne\n",
    "    - `WHEN datecreationetablissement = date_debut_activite THEN 'True' WHEN datecreationetablissement IS NULL \n",
    "        OR date_debut_activite IS NULL THEN 'NULL' --WHEN datecreationetablissement = '' \n",
    "        ELSE 'False'``\n",
    "    - Test si la date de création de l'établissement est égale à la date de création. \n",
    "- `test_status_admin`: Pertinence moyenne\n",
    "    - `WHEN etatadministratifetablissement = status_admin THEN 'True' WHEN etatadministratifetablissement IS NULL \n",
    "        OR status_admin IS NULL THEN 'NULL' WHEN etatadministratifetablissement = '' \n",
    "        OR status_admin = '' THEN 'NULL' ELSE 'False'``\n",
    "    - Test si l'établissement est fermé ou non. Pas radié mais fermé. L'INSEE n'indique pas les radiations, et le fichier ETS de l'INPI n'indique pas les radiations et n'indique pas les fermetures resultants de radiation. Pour cela il faut construire la variable via la table PM ou PP.\n",
    "- `test_siege`: Pertinence elevée\n",
    "    - `etablissementsiege = status_ets THEN 'True' WHEN etablissementsiege IS NULL \n",
    "        OR status_ets IS NULL THEN 'NULL' WHEN etablissementsiege = '' \n",
    "        OR status_ets = '' THEN 'NULL' ELSE 'False'``\n",
    "    - Test si le siret est un siège ou non. \n",
    "- `test_code_commune`: Pertinence faible\n",
    "    - `codecommuneetablissement = code_commune THEN 'True' WHEN codecommuneetablissement IS NULL \n",
    "        OR code_commune IS NULL THEN 'NULL' WHEN codecommuneetablissement = '' \n",
    "        OR code_commune = '' THEN 'NULL' ELSE 'False'``\n",
    "    - Test si le code commune est identique entre l'INPI et l'INSEE. Pas suffisament fiable\n",
    "- `test_type_voie`: Pertinence faible\n",
    "    - `numerovoieetablissement = numero_voie_matching THEN 'True' WHEN numerovoieetablissement IS NULL \n",
    "        OR numero_voie_matching IS NULL THEN 'NULL' WHEN numerovoieetablissement = '' \n",
    "        OR numero_voie_matching = '' THEN 'NULL' ELSE 'False'`\n",
    "    - Test si le type de voie est identique entre les deux variables. Methode d'extraction que nous avons utilisé n'est pas suffisement pertinente\n",
    "- `test_enseigne`: Pertinence moyenne\n",
    "    - `WHEN cardinality(test) = 0 THEN 'NULL' WHEN enseigne = '' THEN 'NULL' WHEN temp_test_enseigne = TRUE THEN 'True' ELSE 'False'`\n",
    "    - Test si l'enseigne est identique entre les variables. Aucun retraitement si ce n'est mise en majuscule et exclusion des accents. Ne regardepas les fautes d'orthographe\n",
    "    \n",
    "###  `test_list_num`:  Doublon ligne\n",
    "\n",
    "Le tableau ci dessous indique que lorsque l'index id n'a pas de doublon, 6,619,308 lignes ont passé le test, \t3,770,568 ne sont pas des sièges, 5,523,137 lignes avec siège."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_1 = \"\"\"\n",
    "SELECT \n",
    "index_unique, groups,\n",
    "\"\"\"\n",
    "top_2 = \" FROM index_20_true \"\n",
    "\n",
    "middle_1 = \"\"\n",
    "\n",
    "middle_2 =  \"\"\"\n",
    "\n",
    "-- {0}\n",
    "\n",
    "LEFT JOIN (\n",
    "\n",
    "SELECT {0}, count_inpi_index_id_siret, COUNT(index_id) as nb_dict_{0}\n",
    "FROM ets_inpi_insee_cases\n",
    "WHERE test_list_num_voie != 'False'  \n",
    "GROUP BY {0}, count_inpi_index_id_siret\n",
    "  ) as nb_{0}\n",
    "ON index_20_true.index_unique = nb_{0}.count_inpi_index_id_siret AND\n",
    "index_20_true.groups = nb_{0}.{0}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "bottom = \"ORDER BY index_unique, groups\"\n",
    "\n",
    "tests = [\n",
    "    \"test_sequence_siret\",\n",
    "    \"test_index_siret\",\n",
    "    \"test_siren_insee_siren_inpi\",\n",
    "    \"test_sequence_siret_many_cas\",\n",
    "    \"test_list_num_voie\",\n",
    "    \"test_date\",\n",
    "    \"test_status_admin\",\n",
    "    \"test_siege\",\n",
    "    \"test_code_commune\",\n",
    "    \"test_type_voie\",\n",
    "    \"test_enseigne\"] \n",
    "\n",
    "for i, test in enumerate(tests):\n",
    "    var = 'nb_dict_{}'.format(test)\n",
    "    if i == len(tests) -1:\n",
    "        top_1 += '{}'.format(var)\n",
    "    else:\n",
    "        top_1+='{},'.format(var)\n",
    "        \n",
    "    middle_1+= middle_2.format(test)\n",
    "\n",
    "query = top_1 + top_2 + middle_1 + bottom\n",
    "\n",
    "output = athena.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output='INPI/sql_output'\n",
    "        )\n",
    "\n",
    "results = False\n",
    "filename_ligne = 'nb_duplicate_par_cas_list_true_lignes_tests.csv'\n",
    "\n",
    "while results != True:\n",
    "    source_key = \"{}/{}.csv\".format(\n",
    "                                'INPI/sql_output',\n",
    "                                output['QueryExecutionId']\n",
    "                                       )\n",
    "    destination_key = \"{}/{}\".format(\n",
    "                                    'ANALYSE_PRE_SIRETISATION',\n",
    "                                    filename_ligne\n",
    "                                )\n",
    "\n",
    "    results = s3.copy_object_s3(\n",
    "                                    source_key = source_key,\n",
    "                                    destination_key = destination_key,\n",
    "                                    remove = True\n",
    "                                )  \n",
    "    \n",
    "test_ligne = s3.read_df_from_s3(\n",
    "            key = 'ANALYSE_PRE_SIRETISATION/{}'.format(filename_ligne), sep = ',')  \n",
    "(test_ligne\n",
    " .assign(groups = lambda x: x['groups'].fillna('NULL'))\n",
    " .fillna(0)\n",
    " .set_index(['index_unique', 'groups'])\n",
    " .unstack(-1)\n",
    " .style\n",
    " .format(\"{:,.0f}\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  `test_list_num`:  Doublon index\n",
    "\n",
    "Le tableau ci dessous indique que lorsque l'index id n'a pas de doublon, 1,640,499 lignes ont une divergence entre fermeture/ouverture et 7,639,370 lignes ont un status administratif identique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_1 = \"\"\"\n",
    "SELECT \n",
    "index_unique, groups,\n",
    "\"\"\"\n",
    "top_2 = \" FROM index_20_true \"\n",
    "\n",
    "middle_1 = \"\"\n",
    "\n",
    "middle_2 =  \"\"\"\n",
    "\n",
    "-- {0}\n",
    "\n",
    "LEFT JOIN (\n",
    "\n",
    "SELECT {0}, count_inpi_index_id_siret, COUNT(DISTINCT(index_id)) as nb_dict_{0}\n",
    "FROM ets_inpi_insee_cases\n",
    "WHERE test_list_num_voie != 'False'  \n",
    "GROUP BY {0}, count_inpi_index_id_siret\n",
    "  ) as nb_{0}\n",
    "ON index_20_true.index_unique = nb_{0}.count_inpi_index_id_siret AND\n",
    "index_20_true.groups = nb_{0}.{0}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "bottom = \"ORDER BY index_unique, groups\"\n",
    "\n",
    "tests = [\n",
    "    \"test_sequence_siret\",\n",
    "    \"test_index_siret\",\n",
    "    \"test_siren_insee_siren_inpi\",\n",
    "    \"test_sequence_siret_many_cas\",\n",
    "    \"test_list_num_voie\",\n",
    "    \"test_date\",\n",
    "    \"test_status_admin\",\n",
    "    \"test_siege\",\n",
    "    \"test_code_commune\",\n",
    "    \"test_type_voie\",\n",
    "    \"test_enseigne\"] \n",
    "\n",
    "for i, test in enumerate(tests):\n",
    "    var = 'nb_dict_{}'.format(test)\n",
    "    if i == len(tests) -1:\n",
    "        top_1 += '{}'.format(var)\n",
    "    else:\n",
    "        top_1+='{},'.format(var)\n",
    "        \n",
    "    middle_1+= middle_2.format(test)\n",
    "\n",
    "query = top_1 + top_2 + middle_1 + bottom\n",
    "\n",
    "output = athena.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output='INPI/sql_output'\n",
    "        )\n",
    "\n",
    "results = False\n",
    "filename_index = 'nb_duplicate_par_cas_list_true_index_tests.csv'\n",
    "\n",
    "while results != True:\n",
    "    source_key = \"{}/{}.csv\".format(\n",
    "                                'INPI/sql_output',\n",
    "                                output['QueryExecutionId']\n",
    "                                       )\n",
    "    destination_key = \"{}/{}\".format(\n",
    "                                    'ANALYSE_PRE_SIRETISATION',\n",
    "                                    filename_index\n",
    "                                )\n",
    "\n",
    "    results = s3.copy_object_s3(\n",
    "                                    source_key = source_key,\n",
    "                                    destination_key = destination_key,\n",
    "                                    remove = True\n",
    "                                )  \n",
    "    \n",
    "test_ligne = s3.read_df_from_s3(\n",
    "            key = 'ANALYSE_PRE_SIRETISATION/{}'.format(filename_index), sep = ',')  \n",
    "(test_ligne\n",
    " .assign(groups = lambda x: x['groups'].fillna('NULL'))\n",
    " .fillna(0)\n",
    " .set_index(['index_unique', 'groups'])\n",
    " .unstack(-1)\n",
    " .style\n",
    " .format(\"{:,.0f}\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification `test_siren_insee_siren_inpi` \n",
    "\n",
    "On regarde les tests lorsque `test_siren_insee_siren_inpi` est égal à true (pas de doublon) -> Lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_1 = \"\"\"\n",
    "SELECT \n",
    "index_unique, groups,\n",
    "\"\"\"\n",
    "top_2 = \" FROM index_20_true \"\n",
    "\n",
    "middle_1 = \"\"\n",
    "\n",
    "middle_2 =  \"\"\"\n",
    "\n",
    "-- {0}\n",
    "\n",
    "LEFT JOIN (\n",
    "\n",
    "SELECT {0}, count_inpi_index_id_siret, COUNT(index_id) as nb_dict_{0}\n",
    "FROM ets_inpi_insee_cases\n",
    "WHERE test_siren_insee_siren_inpi != 'False'  \n",
    "GROUP BY {0}, count_inpi_index_id_siret\n",
    "  ) as nb_{0}\n",
    "ON index_20_true.index_unique = nb_{0}.count_inpi_index_id_siret AND\n",
    "index_20_true.groups = nb_{0}.{0}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "bottom = \"ORDER BY index_unique, groups\"\n",
    "\n",
    "tests = [\n",
    "    #\"test_sequence_siret\",\n",
    "    \"test_index_siret\",\n",
    "    \"test_siren_insee_siren_inpi\",\n",
    "    \"test_sequence_siret_many_cas\",\n",
    "    \"test_list_num_voie\",\n",
    "    \"test_date\",\n",
    "    \"test_status_admin\",\n",
    "    \"test_siege\",\n",
    "    \"test_code_commune\",\n",
    "    \"test_type_voie\",\n",
    "    \"test_enseigne\"] \n",
    "\n",
    "for i, test in enumerate(tests):\n",
    "    var = 'nb_dict_{}'.format(test)\n",
    "    if i == len(tests) -1:\n",
    "        top_1 += '{}'.format(var)\n",
    "    else:\n",
    "        top_1+='{},'.format(var)\n",
    "        \n",
    "    middle_1+= middle_2.format(test)\n",
    "\n",
    "query = top_1 + top_2 + middle_1 + bottom\n",
    "\n",
    "output = athena.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output='INPI/sql_output'\n",
    "        )\n",
    "\n",
    "results = False\n",
    "filename_ligne = 'nb_duplicate_test_siren_insee_siren_inpi_tests.csv'\n",
    "\n",
    "while results != True:\n",
    "    source_key = \"{}/{}.csv\".format(\n",
    "                                'INPI/sql_output',\n",
    "                                output['QueryExecutionId']\n",
    "                                       )\n",
    "    destination_key = \"{}/{}\".format(\n",
    "                                    'ANALYSE_PRE_SIRETISATION',\n",
    "                                    filename_ligne\n",
    "                                )\n",
    "\n",
    "    results = s3.copy_object_s3(\n",
    "                                    source_key = source_key,\n",
    "                                    destination_key = destination_key,\n",
    "                                    remove = True\n",
    "                                )  \n",
    "    \n",
    "test_ligne = s3.read_df_from_s3(\n",
    "            key = 'ANALYSE_PRE_SIRETISATION/{}'.format(filename_ligne), sep = ',')  \n",
    "(test_ligne\n",
    " .assign(groups = lambda x: x['groups'].fillna('NULL'))\n",
    " .fillna(0)\n",
    " .set_index(['index_unique', 'groups'])\n",
    " .unstack(-1)\n",
    " .style\n",
    " .format(\"{:,.0f}\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse CAS 1,3,4 et doublons index\n",
    "\n",
    "Il y a environ 150k lignes qui ont un match parfait de l'adresse, des numéros de voie mais qui ont plusieurs siret. Dans cette partie, nous allons regarder les tests complémentaires pour determiner combien peuvent être récupérer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_1 = \"\"\"\n",
    "SELECT \n",
    "index_unique, groups,\n",
    "\"\"\"\n",
    "top_2 = \" FROM index_20_true \"\n",
    "\n",
    "middle_1 = \"\"\n",
    "\n",
    "middle_2 =  \"\"\"\n",
    "\n",
    "-- {0}\n",
    "\n",
    "LEFT JOIN (\n",
    "\n",
    "SELECT {0}, count_inpi_index_id_siret, COUNT(index_id) as nb_dict_{0}\n",
    "FROM ets_inpi_insee_cases\n",
    "WHERE index_id_duplicate = 'True' AND test_adresse_cas_1_3_4 = 'True'  \n",
    "GROUP BY {0}, count_inpi_index_id_siret\n",
    "  ) as nb_{0}\n",
    "ON index_20_true.index_unique = nb_{0}.count_inpi_index_id_siret AND\n",
    "index_20_true.groups = nb_{0}.{0}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "bottom = \"ORDER BY index_unique, groups\"\n",
    "\n",
    "tests = [\n",
    "    #\"test_sequence_siret\",\n",
    "    \"test_index_siret\",\n",
    "    \"test_siren_insee_siren_inpi\",\n",
    "    \"test_sequence_siret_many_cas\",\n",
    "    \"test_list_num_voie\",\n",
    "    \"test_date\",\n",
    "    \"test_status_admin\",\n",
    "    \"test_siege\",\n",
    "    \"test_code_commune\",\n",
    "    \"test_type_voie\",\n",
    "    \"test_enseigne\"] \n",
    "\n",
    "for i, test in enumerate(tests):\n",
    "    var = 'nb_dict_{}'.format(test)\n",
    "    if i == len(tests) -1:\n",
    "        top_1 += '{}'.format(var)\n",
    "    else:\n",
    "        top_1+='{},'.format(var)\n",
    "        \n",
    "    middle_1+= middle_2.format(test)\n",
    "\n",
    "query = top_1 + top_2 + middle_1 + bottom\n",
    "\n",
    "output = athena.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output='INPI/sql_output'\n",
    "        )\n",
    "\n",
    "results = False\n",
    "filename_ligne = 'duplicate_test_complementaire_match_adresse_full.csv'\n",
    "\n",
    "while results != True:\n",
    "    source_key = \"{}/{}.csv\".format(\n",
    "                                'INPI/sql_output',\n",
    "                                output['QueryExecutionId']\n",
    "                                       )\n",
    "    destination_key = \"{}/{}\".format(\n",
    "                                    'ANALYSE_PRE_SIRETISATION',\n",
    "                                    filename_ligne\n",
    "                                )\n",
    "\n",
    "    results = s3.copy_object_s3(\n",
    "                                    source_key = source_key,\n",
    "                                    destination_key = destination_key,\n",
    "                                    remove = True\n",
    "                                )  \n",
    "    \n",
    "test_ligne = s3.read_df_from_s3(\n",
    "            key = 'ANALYSE_PRE_SIRETISATION/{}'.format(filename_ligne), sep = ',')  \n",
    "(test_ligne\n",
    " .assign(groups = lambda x: x['groups'].fillna('NULL'))\n",
    " .fillna(0)\n",
    " .set_index(['index_unique', 'groups'])\n",
    " .unstack(-1)\n",
    " .style\n",
    " .format(\"{:,.0f}\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, shutil, urllib, ipykernel, json\n",
    "from pathlib import Path\n",
    "from notebook import notebookapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_report(extension = \"html\"):\n",
    "    \"\"\"\n",
    "    Create a report from the current notebook and save it in the \n",
    "    Report folder (Parent-> child directory)\n",
    "    \n",
    "    1. Exctract the current notbook name\n",
    "    2. Convert the Notebook \n",
    "    3. Move the newly created report\n",
    "    \n",
    "    Args:\n",
    "    extension: string. Can be \"html\", \"pdf\", \"md\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ### Get notebook name\n",
    "    connection_file = os.path.basename(ipykernel.get_connection_file())\n",
    "    kernel_id = connection_file.split('-', 1)[0].split('.')[0]\n",
    "\n",
    "    for srv in notebookapp.list_running_servers():\n",
    "        try:\n",
    "            if srv['token']=='' and not srv['password']:  \n",
    "                req = urllib.request.urlopen(srv['url']+'api/sessions')\n",
    "            else:\n",
    "                req = urllib.request.urlopen(srv['url']+ \\\n",
    "                                             'api/sessions?token=' + \\\n",
    "                                             srv['token'])\n",
    "            sessions = json.load(req)\n",
    "            notebookname = sessions[0]['name']\n",
    "        except:\n",
    "            pass  \n",
    "    \n",
    "    sep = '.'\n",
    "    path = os.getcwd()\n",
    "    #parent_path = str(Path(path).parent)\n",
    "    \n",
    "    ### Path report\n",
    "    #path_report = \"{}/Reports\".format(parent_path)\n",
    "    #path_report = \"{}/Reports\".format(path)\n",
    "    \n",
    "    ### Path destination\n",
    "    name_no_extension = notebookname.split(sep, 1)[0]\n",
    "    source_to_move = name_no_extension +'.{}'.format(extension)\n",
    "    dest = os.path.join(path,'Reports', source_to_move)\n",
    "    \n",
    "    ### Generate notebook\n",
    "    os.system('jupyter nbconvert --no-input --to {} {}'.format(\n",
    "    extension,notebookname))\n",
    "    \n",
    "    ### Move notebook to report folder\n",
    "    #time.sleep(5)\n",
    "    shutil.move(source_to_move, dest)\n",
    "    print(\"Report Available at this adress:\\n {}\".format(dest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_report(extension = \"html\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
