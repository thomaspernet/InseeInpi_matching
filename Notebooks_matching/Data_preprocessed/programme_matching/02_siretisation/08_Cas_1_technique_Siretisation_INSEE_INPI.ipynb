{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test nombre lignes siretise\n",
    "\n",
    "Copy paste from Coda to fill the information\n",
    "\n",
    "## Objective(s)\n",
    "\n",
    "- Lors de [l’US 7: Test nombre lignes siretise avec nouvelles regles de gestion](https://coda.io/d/CreditAgricole_dCtnoqIftTn/US-07-ETS-version-3_su0VF), une batterie d’analyse a été réalisé ce qui a permit de mettre en évidence des règles de gestion pour discriminer les lignes qui ont des doublons, mais aussi ce qui n’en n’ont pas. Effectivement, certaines lignes sans doublons n’ ont pas nécessairement une relation (i.e. l’adresse n’est pas celle du siret). \n",
    "\n",
    "  - Nous avons pu distinguer 4 grandes catégories pour séparer les lignes. Le tableau ci dessous récapitule les possibilités\n",
    "\n",
    "![00_ensemble_cas_siretisation.jpeg](https://codahosted.io/docs/CtnoqIftTn/blobs/bl-vmJJOeJ__6/455e18296e4076d4b2ec1dbcb5f1364af4b7824c8e96262f78095b67a3db4fd10422ee1adc6e06779452eeb0a47003a6f4a1e29995c559aa9531a8e6499db04a0e089b7e173ed187259fc4aa4a799935f8ead28ed9dfe555a307bdac2eb840e4991b963c)\n",
    "\n",
    "  - Dans cette US, nous allons traiter du cas ou il n’y a pas de doublon, et la relation entre l’adresse INSEE et INPI appartient au cas de figure 1,3 ou 4. Pour filtrer la table avec les lignes qui satisfont ses deux critères, il faut utiliser les variables:\n",
    "\n",
    "   - index_id_duplicate\n",
    "   - test_adresse_cas_1_3_4\n",
    "\n",
    "  - Les variables à utiliser pour les deux sont:\n",
    "\n",
    "   - test_siege\n",
    "   - test_enseigne\n",
    "\n",
    "  - Des que les lignes sont écartées, il faut réaliser un document json avec les informations sur la siretisation, qui indique les informations suivantes:\n",
    "\n",
    "  - Un csv avec le siret par index sera créé. Le CSV doit contenir les informations suivantes:\n",
    "\n",
    "  - Il faut aussi filtrer les lignes qui sont à éjecter du test test_duplicates_is_in_cas_1_3_4 a savoir les TO_REMOVE \n",
    "\n",
    "  - Include DataStudio (tick if yes): false\n",
    "\n",
    "## Metadata \n",
    "\n",
    "- Metadata parameters are available here: [Ressources_suDYJ#_luZqd](http://Ressources_suDYJ#_luZqd)\n",
    "\n",
    "- Task type:\n",
    "    - Jupyter Notebook\n",
    "- Users: :\n",
    "    - [Thomas Pernet](mailto:t.pernetcoudrier@gmail.com)\n",
    "- Watchers:\n",
    "\n",
    "  - [Thomas Pernet](mailto:t.pernetcoudrier@gmail.com)\n",
    "- Estimated Log points:\n",
    " - One being a simple task, 15 a very difficult one\n",
    "    -  7\n",
    "- Task tag\n",
    "    -  \\#sql-query,#matching,#siretisation,#regle-de-gestion,#cas-1\n",
    "- Toggl Tag\n",
    "   - \\#data-analysis \n",
    "  \n",
    "## Input Cloud Storage [AWS/GCP]\n",
    "\n",
    "If link from the internet, save it to the cloud first\n",
    "\n",
    "### Tables [AWS/BigQuery]\n",
    "\n",
    "- Select Provider: Athena\n",
    "\n",
    "  - Select table(s): ets_inpi_insee_cases\n",
    "\n",
    "  - Select only tables created from the same notebook, else copy/paste selection to add new input tables\n",
    "\n",
    "    - If table(s) does not exist, add them: \n",
    "\n",
    "      Add New Table\n",
    "\n",
    "    - Information:\n",
    "\n",
    "    - Region: \n",
    "\n",
    "      - NameEurope (Paris)\n",
    "        - Code: eu-west-3\n",
    "\n",
    "      - Database: inpi\n",
    "\n",
    "      - Notebook construction file: [07_pourcentage_siretisation_v3](https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/02_siretisation/07_pourcentage_siretisation_v3.md)\n",
    "    \n",
    "## Destination Output/Delivery\n",
    "\n",
    "- AWS\n",
    "\n",
    "  - Athena: \n",
    "\n",
    "      - Region: Europe (Paris)\n",
    "      - Database: inpi\n",
    "      - Tables (Add name new table): ets_inpi_insee_cases_filtered\n",
    "      - List new tables\n",
    "      - ets_inpi_insee_cases_filtered\n",
    "- S3(Add new filename to Database: [Ressources](https://coda.io/d/CreditAgricole_dCtnoqIftTn/Ressources_suDYJ))\n",
    "\n",
    "    - Origin: Jupyter notebook\n",
    "\n",
    "    - Bucket: calfdata\n",
    "\n",
    "    - Key: RESULTATS_SIRETISATION/LOGS_ANALYSE\n",
    "\n",
    "    - Filename(s): LOGS_CAS_1.json,RESULTAT_CAS_1.csv\n",
    "        - [RESULTATS_SIRETISATION/LOGS_ANALYSE/LOGS_CAS_1.json](https://s3.console.aws.amazon.com/s3/buckets/calfdata/RESULTATS_SIRETISATION/LOGS_ANALYSE)-\n",
    "        - [RESULTATS_SIRETISATION/TABLES_SIRET/RESULTAT_CAS_1.csv](https://s3.console.aws.amazon.com/s3/buckets/calfdata/RESULTATS_SIRETISATION/TABLES_SIRET)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Things to know (Steps, Attention points or new flow of information)\n",
    "\n",
    "### Sources of information  (meeting notes, Documentation, Query, URL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connexion serveur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsPy.aws_authorization import aws_connector\n",
    "from awsPy.aws_athena import service_athena\n",
    "from awsPy.aws_s3 import service_s3\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, shutil\n",
    "bucket = 'calfdata'\n",
    "path = os.getcwd()\n",
    "parent_path = str(Path(path).parent)\n",
    "path_cred = r\"{}/credential_AWS.json\".format(parent_path)\n",
    "\n",
    "region = ''\n",
    "bucket = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                       region = region)\n",
    "client= con.client_boto()\n",
    "s3 = service_s3.connect_S3(client = client,\n",
    "                      bucket = bucket, verbose = False) \n",
    "athena = service_athena.connect_athena(client = client,\n",
    "                      bucket = bucket) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation tables\n",
    "\n",
    "## Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "output = athena.run_query(\n",
    "            query=query,\n",
    "            database='',\n",
    "            s3_output=''\n",
    "        )\n",
    "\n",
    "results = False\n",
    "filename = 'XX.csv'\n",
    "\n",
    "while results != True:\n",
    "    source_key = \"{}/{}.csv\".format(\n",
    "                                'XX/XX',\n",
    "                                output['QueryExecutionId']\n",
    "                                       )\n",
    "    destination_key = \"{}/{}\".format(\n",
    "                                    'XX',\n",
    "                                    filename\n",
    "                                )\n",
    "\n",
    "    results = s3.copy_object_s3(\n",
    "                                    source_key = source_key,\n",
    "                                    destination_key = destination_key,\n",
    "                                    remove = True\n",
    "                                )\n",
    "    \n",
    "table = (s3.read_df_from_s3(\n",
    "            key = 'XX/{}'.format(filename), sep = ',')\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, shutil, urllib, ipykernel, json\n",
    "from pathlib import Path\n",
    "from notebook import notebookapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_report(extension = \"html\"):\n",
    "    \"\"\"\n",
    "    Create a report from the current notebook and save it in the \n",
    "    Report folder (Parent-> child directory)\n",
    "    \n",
    "    1. Exctract the current notbook name\n",
    "    2. Convert the Notebook \n",
    "    3. Move the newly created report\n",
    "    \n",
    "    Args:\n",
    "    extension: string. Can be \"html\", \"pdf\", \"md\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ### Get notebook name\n",
    "    connection_file = os.path.basename(ipykernel.get_connection_file())\n",
    "    kernel_id = connection_file.split('-', 1)[0].split('.')[0]\n",
    "\n",
    "    for srv in notebookapp.list_running_servers():\n",
    "        try:\n",
    "            if srv['token']=='' and not srv['password']:  \n",
    "                req = urllib.request.urlopen(srv['url']+'api/sessions')\n",
    "            else:\n",
    "                req = urllib.request.urlopen(srv['url']+ \\\n",
    "                                             'api/sessions?token=' + \\\n",
    "                                             srv['token'])\n",
    "            sessions = json.load(req)\n",
    "            notebookname = sessions[0]['name']\n",
    "        except:\n",
    "            pass  \n",
    "    \n",
    "    sep = '.'\n",
    "    path = os.getcwd()\n",
    "    #parent_path = str(Path(path).parent)\n",
    "    \n",
    "    ### Path report\n",
    "    #path_report = \"{}/Reports\".format(parent_path)\n",
    "    #path_report = \"{}/Reports\".format(path)\n",
    "    \n",
    "    ### Path destination\n",
    "    name_no_extension = notebookname.split(sep, 1)[0]\n",
    "    source_to_move = name_no_extension +'.{}'.format(extension)\n",
    "    dest = os.path.join(path,'Reports', source_to_move)\n",
    "    \n",
    "    ### Generate notebook\n",
    "    os.system('jupyter nbconvert --no-input --to {} {}'.format(\n",
    "    extension,notebookname))\n",
    "    \n",
    "    ### Move notebook to report folder\n",
    "    #time.sleep(5)\n",
    "    shutil.move(source_to_move, dest)\n",
    "    print(\"Report Available at this adress:\\n {}\".format(dest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_report(extension = \"html\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
