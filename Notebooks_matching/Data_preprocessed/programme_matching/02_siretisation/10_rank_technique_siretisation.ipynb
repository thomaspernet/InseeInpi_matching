{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test nombre lignes siretise\n",
    "\n",
    "Copy paste from Coda to fill the information\n",
    "\n",
    "## Objective(s)\n",
    "\n",
    "- Lors de [l’US 7: Test nombre lignes siretise avec nouvelles regles de gestion](https://coda.io/d/CreditAgricole_dCtnoqIftTn/US-07-ETS-version-3_su0VF), nous avons créé une table avec l’ensemble des possibilités de tests, trié par ordre de préférence. \n",
    "- Lors de l’US, [Creation table distance word2vec et merge table ets inpi insee cas](https://coda.io/d/CreditAgricole_dCtnoqIftTn/US-07-ETS-version-3_su0VF), nous avons crée deux variables pour les tests, a savoir le test sur la distance de cosine, et le test de levhenstein. \n",
    "  - Afin de séparer les doublons, il suffit de récupérer le rank minimum par index. Celui ci va nous donner le meilleur des probables. \n",
    "  - Il est bien sur possible d’avoir encore des doublons, dans ces cas la, il faut aller plus loin dans la rédaction des tests\n",
    "  - L’objectif de cette US est de récupérer le rank minimum de la table ets_inpi_insee_cases puis de faire une analyse brève des index récupérés. \n",
    "\n",
    "## Metadata \n",
    "\n",
    "- - Metadata parameters are available here: [Ressources_suDYJ#_luZqd](http://Ressources_suDYJ#_luZqd)\n",
    "\n",
    "  - Task type:\n",
    "\n",
    "    - Jupyter Notebook\n",
    "\n",
    "  - Users: :\n",
    "\n",
    "    - [Thomas Pernet](mailto:t.pernetcoudrier@gmail.com)\n",
    "\n",
    "  - Watchers:\n",
    "\n",
    "    - [Thomas Pernet](mailto:t.pernetcoudrier@gmail.com)\n",
    "\n",
    "  - Estimated Log points:\n",
    "\n",
    "    - One being a simple task, 15 a very difficult one\n",
    "        -  8\n",
    "\n",
    "  - Task tag\n",
    "\n",
    "    -  \\#sql-query,#matching,#similarite,#regle-de-gestion\n",
    "\n",
    "  - Toggl Tag\n",
    "\n",
    "    - \\#data-analysis\n",
    "  \n",
    "## Input Cloud Storage [AWS/GCP]\n",
    "\n",
    "If link from the internet, save it to the cloud first\n",
    "\n",
    "### Tables [AWS/BigQuery]\n",
    "\n",
    "- Batch 1:\n",
    "\n",
    "  - Select Provider: Athena\n",
    "\n",
    "    -  Select table(s): ets_inpi_insee_cases_distance\n",
    "\n",
    "    - Select only tables created from the same notebook, else copy/paste selection to add new input tables\n",
    "\n",
    "      - If table(s) does not exist, add them: \n",
    "\n",
    "        Add New Table\n",
    "\n",
    "      - Information:\n",
    "\n",
    "      - Region: \n",
    "\n",
    "        - NameEurope (Paris)\n",
    "          - Code: eu-west-3\n",
    "\n",
    "        - Database: inpi\n",
    "\n",
    "        - Notebook construction file: [07_bis_creation_distance](https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/02_siretisation/07_bis_creation_distance.md)\n",
    "    \n",
    "## Destination Output/Delivery\n",
    "\n",
    "- AWS\n",
    "\n",
    "    -  Athena: \n",
    "\n",
    "      -  Region: Europe (Paris)\n",
    "        - Database: inpi\n",
    "        - Tables (Add name new table): ets_inpi_insee_cases_rank\n",
    "        - List new tables\n",
    "        - ets_inpi_insee_cases_rank \n",
    "\n",
    "## Things to know (Steps, Attention points or new flow of information)\n",
    "\n",
    "### Sources of information  (meeting notes, Documentation, Query, URL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connexion serveur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsPy.aws_authorization import aws_connector\n",
    "from awsPy.aws_athena import service_athena\n",
    "from awsPy.aws_s3 import service_s3\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, shutil\n",
    "bucket = 'calfdata'\n",
    "path = os.getcwd()\n",
    "parent_path = str(Path(path).parent)\n",
    "path_cred = r\"{}/credential_AWS.json\".format(parent_path)\n",
    "\n",
    "region = 'eu-west-3'\n",
    "bucket = 'calfdata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                       region = region)\n",
    "client= con.client_boto()\n",
    "s3 = service_s3.connect_S3(client = client,\n",
    "                      bucket = bucket, verbose = False) \n",
    "#athena = service_athena.connect_athena(client = client,\n",
    "#                      bucket = bucket) \n",
    "s3_output = 'INPI/sql_output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation tables\n",
    "\n",
    "## Steps\n",
    "\n",
    "- Recupération du rank minimum par index\n",
    "- Filtre les lignes correspondant au rank minimum par index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "CREATE TABLE inpi.ets_inpi_insee_cases_rank\n",
    "WITH (\n",
    "  format='PARQUET'\n",
    ") AS\n",
    "WITH tb_min_rank AS (\n",
    "SELECT \n",
    "min_rank,\n",
    "  row_id, \n",
    "  ets_inpi_insee_cases_distance.index_id, \n",
    "  sequence_id, \n",
    "  siren, \n",
    "  siret,\n",
    "  list_inpi, \n",
    "  lenght_list_inpi, \n",
    "  list_insee, \n",
    "  lenght_list_insee, \n",
    "  inpi_except, \n",
    "  insee_except, \n",
    "  intersection, \n",
    "  union_, \n",
    "  pct_intersection, \n",
    "  len_inpi_except, \n",
    "  len_insee_except, \n",
    "  status_cas,\n",
    "  unzip_inpi,\n",
    "  unzip_insee,\n",
    "  max_cosine_distance,\n",
    "  test_distance_cosine,\n",
    "  levenshtein_distance,\n",
    "  test_distance_levhenstein, \n",
    "  count_initial_insee, \n",
    "  count_inpi_siren_siret, \n",
    "  count_inpi_siren_sequence, \n",
    "  count_inpi_sequence_siret, \n",
    "  count_inpi_sequence_stat_cas_siret, \n",
    "  count_inpi_index_id_siret, \n",
    "  count_inpi_index_id_stat_cas_siret, \n",
    "  count_inpi_index_id_stat_cas, \n",
    "  index_id_duplicate, \n",
    "  test_sequence_siret, \n",
    "  test_index_siret, \n",
    "  test_siren_insee_siren_inpi, \n",
    "  test_sequence_siret_many_cas, \n",
    "  list_numero_voie_matching_inpi, \n",
    "  list_numero_voie_matching_insee, \n",
    "  intersection_numero_voie, \n",
    "  union_numero_voie, \n",
    "  test_list_num_voie, \n",
    "  datecreationetablissement, \n",
    "  date_debut_activite, \n",
    "  test_date, \n",
    "  etatadministratifetablissement, \n",
    "  status_admin, \n",
    "  test_status_admin, \n",
    "  etablissementsiege, \n",
    "  status_ets, \n",
    "  test_siege, \n",
    "  codecommuneetablissement, \n",
    "  code_commune, \n",
    "  test_code_commune, \n",
    "  codepostaletablissement, \n",
    "  code_postal_matching, \n",
    "  numerovoieetablissement, \n",
    "  numero_voie_matching, \n",
    "  test_numero_voie, \n",
    "  typevoieetablissement, \n",
    "  type_voie_matching, \n",
    "  test_type_voie, \n",
    "  test_adresse_cas_1_3_4, \n",
    "  index_id_dup_has_cas_1_3_4, \n",
    "  test_duplicates_is_in_cas_1_3_4, \n",
    "  enseigne, \n",
    "  enseigne1etablissement, \n",
    "  enseigne2etablissement, \n",
    "  enseigne3etablissement, \n",
    "  test_enseigne,\n",
    "  key_except_to_test\n",
    "FROM ets_inpi_insee_cases_distance \n",
    "INNER JOIN (\n",
    "  SELECT index_id, MIN(rank) AS min_rank\n",
    "FROM ets_inpi_insee_cases_distance\n",
    "GROUP BY index_id\n",
    "  ) as tb_min_rank\n",
    "ON ets_inpi_insee_cases_distance.index_id = tb_min_rank.index_id AND\n",
    "ets_inpi_insee_cases_distance.rank = tb_min_rank.min_rank\n",
    "  ) \n",
    "  SELECT \n",
    "  min_rank,\n",
    "  row_id, \n",
    "  tb_min_rank.index_id, \n",
    "  count_index,\n",
    "  sequence_id, \n",
    "  siren, \n",
    "  siret,\n",
    "  list_inpi, \n",
    "  lenght_list_inpi, \n",
    "  list_insee, \n",
    "  lenght_list_insee, \n",
    "  inpi_except, \n",
    "  insee_except, \n",
    "  intersection, \n",
    "  union_, \n",
    "  pct_intersection, \n",
    "  len_inpi_except, \n",
    "  len_insee_except, \n",
    "  status_cas,\n",
    "  unzip_inpi,\n",
    "  unzip_insee,\n",
    "  max_cosine_distance,\n",
    "  test_distance_cosine,\n",
    "  levenshtein_distance,\n",
    "  test_distance_levhenstein, \n",
    "  count_initial_insee, \n",
    "  count_inpi_siren_siret, \n",
    "  count_inpi_siren_sequence, \n",
    "  count_inpi_sequence_siret, \n",
    "  count_inpi_sequence_stat_cas_siret, \n",
    "  count_inpi_index_id_siret, \n",
    "  count_inpi_index_id_stat_cas_siret, \n",
    "  count_inpi_index_id_stat_cas, \n",
    "  index_id_duplicate, \n",
    "  test_sequence_siret, \n",
    "  test_index_siret, \n",
    "  test_siren_insee_siren_inpi, \n",
    "  test_sequence_siret_many_cas, \n",
    "  list_numero_voie_matching_inpi, \n",
    "  list_numero_voie_matching_insee, \n",
    "  intersection_numero_voie, \n",
    "  union_numero_voie, \n",
    "  test_list_num_voie, \n",
    "  datecreationetablissement, \n",
    "  date_debut_activite, \n",
    "  test_date, \n",
    "  etatadministratifetablissement, \n",
    "  status_admin, \n",
    "  test_status_admin, \n",
    "  etablissementsiege, \n",
    "  status_ets, \n",
    "  test_siege, \n",
    "  codecommuneetablissement, \n",
    "  code_commune, \n",
    "  test_code_commune, \n",
    "  codepostaletablissement, \n",
    "  code_postal_matching, \n",
    "  numerovoieetablissement, \n",
    "  numero_voie_matching, \n",
    "  test_numero_voie, \n",
    "  typevoieetablissement, \n",
    "  type_voie_matching, \n",
    "  test_type_voie, \n",
    "  test_adresse_cas_1_3_4, \n",
    "  index_id_dup_has_cas_1_3_4, \n",
    "  test_duplicates_is_in_cas_1_3_4, \n",
    "  enseigne, \n",
    "  enseigne1etablissement, \n",
    "  enseigne2etablissement, \n",
    "  enseigne3etablissement, \n",
    "  test_enseigne,\n",
    "  key_except_to_test \n",
    "  FROM tb_min_rank\n",
    "  LEFT JOIN (\n",
    "    SELECT index_id, COUNT(*) AS count_index\n",
    "    FROM tb_min_rank\n",
    "    GROUP BY index_id\n",
    "    ) as tb_nb_index\n",
    "    ON tb_min_rank.index_id = tb_nb_index.index_id\n",
    "\"\"\"\n",
    "\n",
    "output = s3.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output=s3_output,\n",
    "  filename = None, ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count nombre lignes  & index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le nombre de lignes est de:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM ets_inpi_insee_cases_rank \n",
    "\"\"\"\n",
    "\n",
    "output = s3.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output='INPI/sql_output',\n",
    "      filename = 'cnt_nb_lignes_rank', ## Add filename to print dataframe\n",
    "      destination_key = None ### Add destination key if need to copy output\n",
    "        )\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le nombre d'index est de "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT COUNT(distinct(index_id))\n",
    "FROM ets_inpi_insee_cases_rank \n",
    "\"\"\"\n",
    "\n",
    "output = s3.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output='INPI/sql_output',\n",
    "      filename = 'cnt_nb_index_rank', ## Add filename to print dataframe\n",
    "      destination_key = None ### Add destination key if need to copy output\n",
    "        )\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation des doublons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT count_index, COUNT(*) as ligne_dup\n",
    "FROM ets_inpi_insee_cases_rank \n",
    "GROUP BY count_index \n",
    "ORDER BY count_index\n",
    "\"\"\"\n",
    "\n",
    "nb_ligne = s3.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output='INPI/sql_output',\n",
    "      filename = 'cnt_nb_dup_lignes_rank', ## Add filename to print dataframe\n",
    "      destination_key = None ### Add destination key if need to copy output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT count_index, COUNT(DISTINCT(index_id)) as index_dup\n",
    "FROM ets_inpi_insee_cases_rank \n",
    "GROUP BY count_index \n",
    "ORDER BY count_index\n",
    "\"\"\"\n",
    "\n",
    "nb_index = s3.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output='INPI/sql_output',\n",
    "      filename = 'cnt_nb_dup_index_rank', ## Add filename to print dataframe\n",
    "      destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "pd.concat([    \n",
    " pd.concat([\n",
    "    pd.concat(\n",
    "    [\n",
    "        nb_ligne.sum().to_frame().T.rename(index = {0:'total'}), \n",
    "        nb_ligne\n",
    "    ], axis = 0),\n",
    "    ],axis = 1,keys=[\"Lignes\"]),\n",
    "    (\n",
    " pd.concat([\n",
    "    pd.concat(\n",
    "    [\n",
    "        nb_index.sum().to_frame().T.rename(index = {0:'total'}), \n",
    "        nb_index\n",
    "    ], axis = 0),\n",
    "    ],axis = 1,keys=[\"Index\"])\n",
    ")],axis= 1\n",
    "    )\n",
    "    .style\n",
    "    .format(\"{:,.0f}\")\n",
    "                  .bar(subset= [\n",
    "                      ('Lignes','ligne_dup'),\n",
    "                      ('Index','index_dup'),\n",
    "                      \n",
    "                  ],\n",
    "                       color='#d65f5f')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse des ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "count_index, \n",
    "  approx_percentile(min_rank, ARRAY[.1, .15, .20, 0.25,0.50,0.75,.80,.85,.86,.87, .88, .89,.90,0.95, 0.99]) as pct_min_rank\n",
    "FROM \n",
    "  ets_inpi_insee_cases_rank\n",
    "GROUP BY count_index  \n",
    "ORDER BY count_index\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output='INPI/sql_output',\n",
    "      filename = 'distribution_rank', ## Add filename to print dataframe\n",
    "      destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query =\"\"\"\n",
    "SELECT *\n",
    "FROM regles_tests \n",
    "WHERE rank = 146\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output='INPI/sql_output',\n",
    "      filename = 'rules', ## Add filename to print dataframe\n",
    "      destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse des cas pour index unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT status_cas, count(*) as nb_unique\n",
    "FROM ets_inpi_insee_cases_rank \n",
    "WHERE count_index = 1\n",
    "GROUP BY status_cas\n",
    "ORDER BY status_cas\n",
    "\"\"\"\n",
    "\n",
    "tb = s3.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output='INPI/sql_output',\n",
    "      filename = 'nb_cas_index_unique_rank', ## Add filename to print dataframe\n",
    "      destination_key = None ### Add destination key if need to copy output\n",
    "        )\n",
    "(\n",
    "    tb.\n",
    "assign(\n",
    "cumsum = lambda x: x['nb_unique'].cumsum(),\n",
    "    pct_total = lambda x: x['nb_unique']/x['nb_unique'].sum(),\n",
    "    pct_total_cum = lambda x: x['pct_total'].cumsum()\n",
    ")\n",
    "    .style\n",
    "    .format(\"{0:,.2%}\", subset=[\"pct_total\", 'pct_total_cum'])\n",
    "    .bar(subset= ['cumsum'],\n",
    "                       color='#d65f5f')\n",
    "    .bar(subset= ['nb_unique'],\n",
    "                       color='#228B22')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, shutil, urllib, ipykernel, json\n",
    "from pathlib import Path\n",
    "from notebook import notebookapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_report(extension = \"html\"):\n",
    "    \"\"\"\n",
    "    Create a report from the current notebook and save it in the \n",
    "    Report folder (Parent-> child directory)\n",
    "    \n",
    "    1. Exctract the current notbook name\n",
    "    2. Convert the Notebook \n",
    "    3. Move the newly created report\n",
    "    \n",
    "    Args:\n",
    "    extension: string. Can be \"html\", \"pdf\", \"md\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ### Get notebook name\n",
    "    connection_file = os.path.basename(ipykernel.get_connection_file())\n",
    "    kernel_id = connection_file.split('-', 1)[0].split('.')[0]\n",
    "\n",
    "    for srv in notebookapp.list_running_servers():\n",
    "        try:\n",
    "            if srv['token']=='' and not srv['password']:  \n",
    "                req = urllib.request.urlopen(srv['url']+'api/sessions')\n",
    "            else:\n",
    "                req = urllib.request.urlopen(srv['url']+ \\\n",
    "                                             'api/sessions?token=' + \\\n",
    "                                             srv['token'])\n",
    "            sessions = json.load(req)\n",
    "            notebookname = sessions[0]['name']\n",
    "        except:\n",
    "            pass  \n",
    "    \n",
    "    sep = '.'\n",
    "    path = os.getcwd()\n",
    "    #parent_path = str(Path(path).parent)\n",
    "    \n",
    "    ### Path report\n",
    "    #path_report = \"{}/Reports\".format(parent_path)\n",
    "    #path_report = \"{}/Reports\".format(path)\n",
    "    \n",
    "    ### Path destination\n",
    "    name_no_extension = notebookname.split(sep, 1)[0]\n",
    "    source_to_move = name_no_extension +'.{}'.format(extension)\n",
    "    dest = os.path.join(path,'Reports', source_to_move)\n",
    "    \n",
    "    ### Generate notebook\n",
    "    os.system('jupyter nbconvert --no-input --to {} {}'.format(\n",
    "    extension,notebookname))\n",
    "    \n",
    "    ### Move notebook to report folder\n",
    "    #time.sleep(5)\n",
    "    shutil.move(source_to_move, dest)\n",
    "    print(\"Report Available at this adress:\\n {}\".format(dest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_report(extension = \"html\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nteract": {
   "version": "0.24.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
