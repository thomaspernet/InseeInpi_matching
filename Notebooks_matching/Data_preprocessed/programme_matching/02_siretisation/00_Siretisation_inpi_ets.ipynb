{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programme de Matching\n",
    "\n",
    "## Algorithme\n",
    "\n",
    "### origin\n",
    "\n",
    "Comme la taille de la donnée est trop élevée, il faut prendre un sous échantillon pour faire la siretisation. Le sous échantillonage se fait avec l'origine. \n",
    "\n",
    "Input:\n",
    "- INSEE:\n",
    "    - NEW: `data/input/INSEE/NEW/insee_1745311_NEW.csv`\n",
    "    - Initial, Partiel, EVT: `data/input/INSEE/InitialPartielEVT/insee_8272605_InitialPartielEVT.csv`\n",
    "- INPI:\n",
    "    - NEW: `data/input/SIREN_INPI/NEW/inpi_initial_partiel_evt_new_ets_status_final_NEW_0.csv`\n",
    "    - Initial, Partiel, EVT: `data/input/SIREN_INPI/InitialPartielEVT/inpi_initial_partiel_evt_new_ets_status_final_InitialPartielEVT_0`\n",
    "\n",
    "Output:\n",
    "- `data/output/` + `ORIGIN`\n",
    "    - contient les matches\n",
    "- `data/input/INPI/special_treatment/`+ `ORIGIN`\n",
    "    - contient les non matches\n",
    "- `data/logs/`+ `ORIGIN`\n",
    "    - contient les logs\n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "current_dir = os.getcwd()\n",
    "from inpi_insee import siretisation\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "param = {\n",
    "    #'insee': 'data/input/INSEE/InitialPartielEVTNEW/insee_1557220_InitialPartielEVTNEW.csv' ### PP\n",
    "    'insee': 'data/input/INSEE/InitialPartielEVTNEW/insee_9368683_InitialPartielEVTNEW.csv'  ### ETS\n",
    "    #'insee': 'data/input/INSEE/NEW/insee_1745311_NEW.csv' ### ETS\n",
    "}\n",
    "# 4824158 SIREN a trouver!\n",
    "al_siret = siretisation.siretisation_inpi(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from itertools import compress, product\n",
    "\n",
    "#def combinations(items):\n",
    "#    return ( set(compress(items,mask)) for \n",
    "#            mask in product(*[[0,1]]*len(items)))\n",
    "\n",
    "#all_list = ['ncc',\n",
    "#             'Code_Postal','Code_Commune',\n",
    "#             'INSEE','digit_inpi']\n",
    "#test = list(combinations(items = all_list))[1:]\n",
    "#sort_list = sorted(test[1:], key=lambda k: len(k), reverse=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_inpi = ['ncc','code_postal_matching','code_commune','INSEE','digit_inpi']\n",
    "list_insee = ['libelleCommuneEtablissement',\n",
    "            'codePostalEtablissement', 'codeCommuneEtablissement',\n",
    "            'typeVoieEtablissement','numeroVoieEtablissement']\n",
    "\n",
    "sort_list = [\n",
    " {'ncc', 'code_postal_matching', 'code_commune', 'INSEE', 'digit_inpi'},\n",
    " {'ncc', 'code_postal_matching', 'code_commune', 'INSEE'},\n",
    " {'ncc', 'code_postal_matching', 'code_commune', 'digit_inpi'},\n",
    " {'ncc', 'code_postal_matching', 'code_commune'},   \n",
    " {'ncc', 'code_postal_matching'},\n",
    " {'ncc'},\n",
    " {'code_postal_matching'},\n",
    " {'code_commune'}\n",
    "]\n",
    "len(sort_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_possibilities = []\n",
    "for i in sort_list:\n",
    "    left =[]\n",
    "    right = []\n",
    "    for j in i:\n",
    "        left.append(j)\n",
    "        right.append(list_insee[list_inpi.index(j)])\n",
    "    left.insert(0,'siren')\n",
    "    right.insert(0,'siren')\n",
    "    \n",
    "    dic_ = {\n",
    "    'match':{\n",
    "        'inpi':left,\n",
    "        'insee':right,\n",
    "    }\n",
    "}\n",
    "    list_possibilities.append(dic_)\n",
    "list_possibilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indiquer le fichiers a siretiser. Si pas en local, le télécharger depuis le S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = 'inpi_initial_partiel_evt_new_ets_status_final_test_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from awsPy.aws_authorization import aws_connector\n",
    "#from awsPy.aws_s3 import service_s3\n",
    "#from pathlib import Path\n",
    "#import pandas as pd\n",
    "#bucket = 'calfdata'\n",
    "#path = os.getcwd()\n",
    "#parent_path = str(Path(path).parent)\n",
    "#path_cred = r\"{}/programme_matching/credential_AWS.json\".format(parent_path)\n",
    "#con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "#                                        region = 'eu-west-3')\n",
    "#client= con.client_boto()\n",
    "#s3 = service_s3.connect_S3(client = client,\n",
    "#                      bucket = 'calfdata') \n",
    "#s3.download_file(\n",
    "#    key= 'INPI/TC_1/02_preparation_donnee/Stock/ETB/{}_0.csv'.format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_ets = 'data/input/INPI/{}_{}.csv'.format(filename, key)\n",
    "#df_ets\n",
    "#'data/input/INPI/{}_{}.csv'.format(filename, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import shutil\n",
    "#try:\n",
    "#    os.remove(\"data/input/INPI/{}_0.gz\".format(filename))\n",
    "#except:\n",
    "#    pass\n",
    "#shutil.move(\"{}_0.gz\".format(filename),\n",
    "#            \"data/input/INPI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut prendre l'`origin` et `filename` que l'on souhaite sitetiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "files = glob.glob('data/logs/InitialPartielEVTNEW/*')\n",
    "for f in files:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = \"InitialPartielEVTNEW\"\n",
    "filename = \"inpi_initial_partiel_evt_new_ets_status_final_InitialPartielEVTNEW\" ####ETS\n",
    "#filename = \"inpi_initial_partiel_evt_new_ets_status_final_InitialPartielEVTNEW\"\n",
    "#origin = \"NEW\"\n",
    "#filename = \"inpi_initial_partiel_evt_new_ets_status_final_NEW\"\n",
    "#### make dir\n",
    "parent_dir = 'data/output/'\n",
    "parent_dir_1 = 'data/input/INPI/special_treatment/'\n",
    "parent_dir_2 = 'data/logs/'\n",
    "\n",
    "for d in [parent_dir,parent_dir_1,parent_dir_2]:\n",
    "    path = os.path.join(d, origin) \n",
    "    try:\n",
    "        os.mkdir(path) \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "inpi_col = ['siren',\n",
    "            'index',\n",
    "            'type',\n",
    "            'code_postal_matching',\n",
    "            'ville',\n",
    "            'code_commune',\n",
    "            'pays',\n",
    "            'count_initial_inpi',\n",
    "            'ncc',\n",
    "            'adresse_new_clean_reg',\n",
    "            'adress_new',\n",
    "            'INSEE',\n",
    "            'date_debut_activite',\n",
    "            'digit_inpi',\n",
    "            'len_digit_address_inpi',\n",
    "            'list_digit_inpi'\n",
    "            ]\n",
    "\n",
    "inpi_dtype = {\n",
    "    'siren': 'object',\n",
    "    'index': 'int',\n",
    "    'type': 'object',\n",
    "    'code_postal_matching': 'object',\n",
    "    'ville': 'object',\n",
    "    'code_commune': 'object',\n",
    "    'pays': 'object',\n",
    "    'count_initial_inpi': 'int',\n",
    "    'ncc': 'object',\n",
    "    'adresse_new_clean_reg': 'object',\n",
    "    'adress_new':'object',\n",
    "    'INSEE': 'object',\n",
    "    'date_debut_activite': 'object',\n",
    "    'digit_inpi': 'object',\n",
    "    'len_digit_address_inpi':'object'\n",
    "}\n",
    "\n",
    "for key, values in enumerate(list_possibilities):\n",
    "    df_ets = 'data/input/INPI/{0}/{1}_{2}.csv'.format(origin, filename, key)\n",
    "\n",
    "    inpi = al_siret.import_dask(file=df_ets,\n",
    "                                usecols=inpi_col,\n",
    "                                dtype=inpi_dtype,\n",
    "                                parse_dates=False)\n",
    "\n",
    "    df_no_duplication, df_duplication = al_siret.step_one(\n",
    "        df_input=inpi,\n",
    "        left_on=values['match']['inpi'],\n",
    "        right_on=values['match']['insee']\n",
    "    )\n",
    "\n",
    "    # Step 2: No duplication\n",
    "    pure_match = al_siret.step_two_assess_test(df=df_no_duplication,\n",
    "                                               var_group=values['match']['inpi'])\n",
    "\n",
    "    pure_match.to_csv('data/output/{0}/{1}_{2}_pure_match.gz'.format(\n",
    "        origin,\n",
    "        key,\n",
    "        filename),\n",
    "                      compression='gzip', index= False)\n",
    "    # Step 2: duplication\n",
    "    try:\n",
    "        df_not_duplicate, sp = al_siret.step_two_duplication(df_duplication,\n",
    "                                                        var_group = \n",
    "                                                         values['match']['inpi'])\n",
    "    except: \n",
    "        ### it's empty\n",
    "        df_not_duplicate = df_duplication\n",
    "        sp = df_duplication\n",
    "    \n",
    "    \n",
    "    (df_not_duplicate\n",
    "        .to_csv('data/output/{0}/{1}_{2}_not_duplicate.gz'.format(\n",
    "            origin,\n",
    "            key,\n",
    "            filename),\n",
    "                compression='gzip', index= False))\n",
    "\n",
    "    (sp.to_csv(\n",
    "        'data/input/INPI/special_treatment/{0}/{1}_{2}_special_treatment.gz'.format(\n",
    "        origin,key, filename),compression='gzip', index= False))\n",
    "\n",
    "    # Input -> Save for the next loop \n",
    "    inpi.loc[\n",
    "        (~inpi['index'].isin(pure_match['index'].unique()))\n",
    "        & (~inpi['index'].isin(df_not_duplicate['index'].unique()))\n",
    "        & (~inpi['index'].isin(sp['index'].unique()))\n",
    "    ].compute().to_csv('data/input/INPI/{0}/{1}_{2}.csv'.format(\n",
    "        origin,\n",
    "        filename,\n",
    "        key+1),\n",
    "                       index= False)\n",
    "\n",
    "    #### Creation LOG\n",
    "    if key ==0:\n",
    "        total_to_siret_intial = inpi.compute().shape[0]\n",
    "        total_siren_initial = inpi.compute()['siren'].nunique()\n",
    "    \n",
    "    ### Total rows in df inpi to match\n",
    "    total_to_siret_current = inpi.compute().shape[0]\n",
    "    total_siren_current = inpi.compute()['siren'].nunique() # unique siren \n",
    "    \n",
    "    ### DF with no duplication after merge INSEE\n",
    "    total_rows_no_dup = df_no_duplication[\"index\"].nunique()\n",
    "    total_rows_no_dup_unique_siren = df_no_duplication['siren'].nunique()\n",
    "    \n",
    "    ### DF with duplication after merge INSEE\n",
    "    total_rows_dup = df_duplication[\"index\"].nunique() # total duplication\n",
    "    total_rows_dup_unique_siren = df_duplication[\"siren\"].nunique()\n",
    "    \n",
    "    total_rows_dup_matched = df_not_duplicate[\"index\"].nunique() #no duplication\n",
    "    total_rows_dup_matched_unique_siren = df_not_duplicate[\"siren\"].nunique()\n",
    "    \n",
    "    total_rows_dup_not_matched = sp[\"index\"].nunique() # special treatmnent\n",
    "    total_rows_dup_not_matched_unique_siren = sp[\"siren\"].nunique()\n",
    "    \n",
    "    ### compare with initial\n",
    "    total_match_rows_current = total_rows_no_dup + total_rows_dup_matched\n",
    "    perc_total_match_rows_initial = total_match_rows_current / \\\n",
    "    total_to_siret_intial\n",
    "    \n",
    "    total_match_siren_current = total_rows_no_dup_unique_siren + \\\n",
    "    total_rows_dup_matched_unique_siren\n",
    "    \n",
    "    perc_total_match_siren_initial = total_match_siren_current / \\\n",
    "    total_siren_initial \n",
    "    \n",
    "    ### compare with current\n",
    "    perc_total_match_rows_current = total_match_rows_current / \\\n",
    "    total_to_siret_current\n",
    "\n",
    "    perc_total_match_siren_current = total_match_siren_current / \\\n",
    "    total_siren_current\n",
    "    \n",
    "    \n",
    "    dic_ = {\n",
    "        'key':key,\n",
    "        'total_to_siret_intial':total_to_siret_intial,\n",
    "        'total_stotal_siren_initialiren': total_siren_initial,\n",
    "        'total_to_siret_current':total_to_siret_current,\n",
    "        'total_siren_current': total_siren_current,\n",
    "        'total_match_rows_current':total_match_rows_current,\n",
    "        'perc_total_match_rows_initial':perc_total_match_rows_initial,\n",
    "        'total_match_siren_current':total_match_siren_current,\n",
    "        'perc_total_match_siren_initial':perc_total_match_siren_initial,\n",
    "        'perc_total_match_rows_current':perc_total_match_rows_current,\n",
    "        'perc_total_match_siren_current':perc_total_match_siren_current,\n",
    "        'df_no_duplication': {\n",
    "            'nb_index': total_rows_no_dup,\n",
    "            'unique_siren':total_rows_no_dup_unique_siren\n",
    "        },\n",
    "        'df_duplication': {\n",
    "            'nb_index': total_rows_dup,\n",
    "            'unique_siren':total_rows_dup_unique_siren,\n",
    "            'df_not_duplicate_index': {\n",
    "                'nb_index':total_rows_dup_matched,\n",
    "               'unique_siren':total_rows_dup_unique_siren\n",
    "            },\n",
    "            'df_sp_index': {\n",
    "                'nb_index':total_rows_dup_not_matched,\n",
    "               'unique_siren':total_rows_dup_not_matched_unique_siren\n",
    "            }\n",
    "        },\n",
    "        'check': total_to_siret_current -\n",
    "        total_rows_no_dup +\n",
    "        total_rows_dup\n",
    "    }\n",
    "\n",
    "    with open('data/logs/{0}/{1}_{2}_logs.json'.format(origin, key,filename), 'w') as f:\n",
    "        json.dump(dic_, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#al_siret.step_two_assess_test(df=df_no_duplication,\n",
    "#                                               var_group=values['match']['inpi'])\n",
    "df_duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpi.compute().dtypes.apply(lambda x: x.name).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_duplication.dtypes.apply(lambda x: x.name).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_duplication.shape\n",
    "df_duplication[\"siren\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Special Treatment\n",
    "\n",
    "exemple:\n",
    "\n",
    "- 752085324\n",
    "- 342122546: libelle type dans l'adresse complementaire a l'insee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recuperation via list_digit_ INPI/INSEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,8):\n",
    "    test = pd.read_csv(\n",
    "        r'data\\input\\INPI\\special_treatment\\{}_special_treatment.gz'.format(i),\n",
    "                   low_memory=False)\n",
    "    print((test\n",
    " .loc[lambda x:\n",
    "      (x['list_digit_inpi'] == x['list_digit_insee'])\n",
    "     ]\n",
    "     )['siret'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\n",
    "        r'data\\input\\INPI\\special_treatment\\{}_special_treatment.gz'.format(1),\n",
    "                   low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['siren'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "97*97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[lambda x: \n",
    "         x['siren'].isin(['752085324'])\n",
    "        & (x['list_digit_inpi'] == x['list_digit_insee'])\n",
    "        ][['list_digit_inpi', 'list_digit_insee', 'siret']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.assign(special_digit = lambda x:x['libelleVoieEtablissement'].str.findall(r\"(\\d+)\").apply(\n",
    "#        lambda x:'&'.join([i for i in x])))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
