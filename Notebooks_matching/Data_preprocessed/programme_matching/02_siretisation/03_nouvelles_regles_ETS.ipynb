{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programme de Matching: Nouvelles règles de gestion\n",
    "\n",
    "## Algorithme\n",
    "\n",
    "### origin\n",
    "\n",
    "Comme la taille de la donnée est trop élevée, il faut prendre un sous échantillon pour faire la siretisation. Le sous échantillonage se fait avec l'origine. \n",
    "\n",
    "Input:\n",
    "- INSEE:\n",
    "    - NEW: `data/input/INSEE/NEW/insee_1745311_NEW.csv`\n",
    "    - Initial, Partiel, EVT: `data/input/INSEE/InitialPartielEVT/insee_8272605_InitialPartielEVT.csv`\n",
    "- INPI:\n",
    "    - NEW: `data/input/SIREN_INPI/NEW/inpi_initial_partiel_evt_new_ets_status_final_NEW_0.csv`\n",
    "    - Initial, Partiel, EVT: `data/input/SIREN_INPI/InitialPartielEVT/inpi_initial_partiel_evt_new_ets_status_final_InitialPartielEVT_0`\n",
    "\n",
    "Output:\n",
    "- `data/output/` + `ORIGIN`\n",
    "    - contient les matches\n",
    "- `data/input/INPI/special_treatment/`+ `ORIGIN`\n",
    "    - contient les non matches\n",
    "- `data/logs/`+ `ORIGIN`\n",
    "    - contient les logs\n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "os.chdir('../')\n",
    "current_dir = os.getcwd()\n",
    "from inpi_insee import siretisation\n",
    "import pandas as pd\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "param = {\n",
    "    #'insee': 'data/input/INSEE/InitialPartielEVTNEW/insee_1557220_InitialPartielEVTNEW.csv' ### PP\n",
    "    'insee': 'data/input/INSEE/InitialPartielEVTNEW/insee_9368683_InitialPartielEVTNEW.csv'  ### ETS\n",
    "    #'insee': 'data/input/INSEE/NEW/insee_1745311_NEW.csv' ### ETS\n",
    "}\n",
    "# 4824158 SIREN a trouver!\n",
    "al_siret = siretisation.siretisation_inpi(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from itertools import compress, product\n",
    "\n",
    "#def combinations(items):\n",
    "#    return ( set(compress(items,mask)) for \n",
    "#            mask in product(*[[0,1]]*len(items)))\n",
    "\n",
    "#all_list = ['ncc',\n",
    "#             'Code_Postal','Code_Commune',\n",
    "#             'INSEE','digit_inpi']\n",
    "#test = list(combinations(items = all_list))[1:]\n",
    "#sort_list = sorted(test[1:], key=lambda k: len(k), reverse=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_inpi = ['ncc','code_postal_matching','code_commune','voie_matching','numero_voie_matching']\n",
    "list_insee = ['libelleCommuneEtablissement',\n",
    "            'codePostalEtablissement', 'codeCommuneEtablissement',\n",
    "            'typeVoieEtablissement','numeroVoieEtablissement']\n",
    "\n",
    "sort_list = [\n",
    " {'ncc', 'code_postal_matching', 'code_commune', 'voie_matching', 'numero_voie_matching'},\n",
    " {'ncc', 'code_postal_matching', 'code_commune', 'voie_matching'},\n",
    " {'ncc', 'code_postal_matching', 'code_commune', 'numero_voie_matching'},\n",
    " {'ncc', 'code_postal_matching', 'code_commune'},   \n",
    " {'ncc', 'code_postal_matching'},\n",
    " {'ncc'},\n",
    " {'code_postal_matching'},\n",
    " {'code_commune'}\n",
    "]\n",
    "len(sort_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_possibilities = []\n",
    "for i in sort_list:\n",
    "    left =[]\n",
    "    right = []\n",
    "    for j in i:\n",
    "        left.append(j)\n",
    "        right.append(list_insee[list_inpi.index(j)])\n",
    "    left.insert(0,'siren')\n",
    "    right.insert(0,'siren')\n",
    "    \n",
    "    dic_ = {\n",
    "    'match':{\n",
    "        'inpi':left,\n",
    "        'insee':right,\n",
    "    }\n",
    "}\n",
    "    list_possibilities.append(dic_)\n",
    "list_possibilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indiquer le fichiers a siretiser. Si pas en local, le télécharger depuis le S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsPy.aws_authorization import aws_connector\n",
    "from awsPy.aws_s3 import service_s3\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "bucket = 'calfdata'\n",
    "path = os.getcwd()\n",
    "parent_path = str(Path(path).parent)\n",
    "path_cred = r\"{}/programme_matching/credential_AWS.json\".format(parent_path)\n",
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                       region = 'eu-west-3')\n",
    "client= con.client_boto()\n",
    "s3 = service_s3.connect_S3(client = client,\n",
    "                      bucket = 'calfdata') \n",
    "#s3.download_file(\n",
    "#    key= 'INPI/TC_1/02_preparation_donnee/ETS_TEMP/ets_preparation_python_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_ets = 'data/input/INPI/{}_1.csv'.format(filename)\n",
    "#df_ets\n",
    "#'data/input/INPI/{}_{}.csv'.format(filename, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "try:\n",
    "    os.remove(\"data/input/INPI/{}_0.gz\".format(filename))\n",
    "except:\n",
    "    pass\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = 'ets_preparation_python_1'\n",
    "#shutil.move(\"{}.csv\".format(filename),\n",
    "#            \"data/input/INPI/InitialPartielEVTNEW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut prendre l'`origin` et `filename` que l'on souhaite sitetiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "files = glob.glob('data/logs/InitialPartielEVTNEW/*')\n",
    "for f in files:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = \"InitialPartielEVTNEW\"\n",
    "filename = \"ets_preparation_python\" ####ETS\n",
    "#filename = \"inpi_initial_partiel_evt_new_ets_status_final_InitialPartielEVTNEW\"\n",
    "#origin = \"NEW\"\n",
    "#filename = \"inpi_initial_partiel_evt_new_ets_status_final_NEW\"\n",
    "#### make dir\n",
    "parent_dir = 'data/output/'\n",
    "parent_dir_1 = 'data/input/INPI/special_treatment/'\n",
    "parent_dir_2 = 'data/logs/'\n",
    "\n",
    "for d in [parent_dir,parent_dir_1,parent_dir_2]:\n",
    "    path = os.path.join(d, origin) \n",
    "    try:\n",
    "        os.mkdir(path) \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parametres et fonctions\n",
    "\n",
    "- `import_dask`: Charge csv en Dask DataFrame pour clusteriser les calculs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_duplication(df):\n",
    "        \"\"\"\n",
    "        Split un dataframe si l'index (la variable, pas l'index) contient des\n",
    "        doublons.\n",
    "\n",
    "        L'idée est de distinguer les doublons resultants du merge avec l'INSEE\n",
    "\n",
    "        Args:\n",
    "        - df: Pandas dataframe contenant au moins une variable \"index\"\n",
    "\n",
    "        Returns:\n",
    "        - Un Dictionary avec:\n",
    "            - not_duplication: Dataframe ne contenant pas les doublons\n",
    "            - duplication: Dataframe contenant les doublons\n",
    "            - report_dup: Une Serie avec le nombres de doublons\n",
    "        \"\"\"\n",
    "        if 'count_duplicates_' in df.columns:\n",
    "            df = df.drop(columns = 'count_duplicates_')\n",
    "\n",
    "        df = df.merge(\n",
    "            (df\n",
    "                .groupby('index')['index']\n",
    "                .count()\n",
    "                .rename('count_duplicates_')\n",
    "                .reset_index()\n",
    "                )\n",
    "                )\n",
    "        \n",
    "        dic_ = {\n",
    "            'not_duplication':df[df['count_duplicates_'].isin([1])],\n",
    "            'duplication' : df[~df['count_duplicates_'].isin([1])],\n",
    "            'report_dup':df[\n",
    "            ~df['count_duplicates_'].isin([1])\n",
    "            ]['count_duplicates_'].value_counts()\n",
    "            }\n",
    "\n",
    "        return dic_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_regex(regex, test_str):\n",
    "        \"\"\"\n",
    "        Performe une recherche regex entre deux colonnes.\n",
    "\n",
    "        Args:\n",
    "        - regex: string: charactère contenant la formule regex\n",
    "        - test_str: String: String contenant le regex a trouver\n",
    "\n",
    "        Return:\n",
    "        Boolean, True si le regex est trouvé, sinon False\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            matches = re.search(regex, test_str)\n",
    "            if matches:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        except:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpi_col = ['siren',\n",
    "            'code_greffe',\n",
    "            'nom_greffe',\n",
    "            'numero_gestion',\n",
    "            'id_etablissement',\n",
    "            'origin',\n",
    "            'file_timestamp',\n",
    "            'date_greffe',\n",
    "            #'nom_commercial',\n",
    "            'enseigne',\n",
    "            'libelle_evt',\n",
    "            'last_libele_evt',\n",
    "            'index',\n",
    "            'type',\n",
    "            'code_postal_matching',\n",
    "            #'ville',\n",
    "            'code_commune',\n",
    "            #'pays',\n",
    "            #'count_initial_inpi',\n",
    "            'ncc',\n",
    "            'adresse_new_clean_reg',\n",
    "            'adress_new',\n",
    "            'voie_matching',\n",
    "            'date_début_activité',\n",
    "            'numero_voie_matching',\n",
    "            #'len_digit_address_inpi',\n",
    "            #'list_digit_inpi'\n",
    "            ]\n",
    "\n",
    "#['count_initial_inpi', 'list_digit_inpi', 'pays', 'len_digit_address_inpi', 'nom_commercial', 'ville']\n",
    "\n",
    "inpi_dtype = {\n",
    "    'siren': 'object',\n",
    "    'code_greffe': 'object',\n",
    "            'nom_greffe': 'object',\n",
    "            'numero_gestion': 'object',\n",
    "            'id_etablissement': 'object',\n",
    "            'origin': 'object',\n",
    "            'file_timestamp': 'object',\n",
    "            'date_greffe': 'object',\n",
    "    #'nom_commercial': 'object',\n",
    "            'enseigne': 'object',\n",
    "            'libelle_evt': 'object',\n",
    "    'id_etablissement': 'object',\n",
    "    'index': 'int',\n",
    "    'type': 'object',\n",
    "    'code_postal_matching': 'object',\n",
    "    'ville': 'object',\n",
    "    'code_commune': 'object',\n",
    "    'pays': 'object',\n",
    "    #'count_initial_inpi': 'int',\n",
    "    'ncc': 'object',\n",
    "    'adresse_new_clean_reg': 'object',\n",
    "    'adress_new':'object',\n",
    "    'voie_matching': 'object',\n",
    "    'date_début_activité': 'object',\n",
    "    'numero_voie_matching': 'object',\n",
    "    #'len_digit_address_inpi':'object'\n",
    "}\n",
    "\n",
    "insee_col = ['siren',\n",
    "         'siret',\n",
    "         'dateCreationEtablissement',\n",
    "         \"etablissementSiege\",\n",
    "         \"etatAdministratifEtablissement\",\n",
    "         'complementAdresseEtablissement',\n",
    "         'numeroVoieEtablissement',\n",
    "         'indiceRepetitionEtablissement',\n",
    "         'typeVoieEtablissement',\n",
    "         'libelleVoieEtablissement',\n",
    "         'codePostalEtablissement',\n",
    "         'libelleCommuneEtablissement',\n",
    "         'libelleCommuneEtrangerEtablissement',\n",
    "         'distributionSpecialeEtablissement',\n",
    "         'codeCommuneEtablissement',\n",
    "         'codeCedexEtablissement',\n",
    "         'libelleCedexEtablissement',\n",
    "         'codePaysEtrangerEtablissement',\n",
    "         'libellePaysEtrangerEtablissement',\n",
    "         'count_initial_insee',\n",
    "             'len_digit_address_insee',\n",
    "             'list_digit_insee']\n",
    "\n",
    "insee_dtype = {\n",
    "             'siren': 'object',\n",
    "             'siret': 'object',\n",
    "             \"etablissementSiege\": \"object\",\n",
    "             \"etatAdministratifEtablissement\": \"object\",\n",
    "             #'dateCreationEtablissement': 'object',\n",
    "             'complementAdresseEtablissement': 'object',\n",
    "             'numeroVoieEtablissement': 'object',\n",
    "             'indiceRepetitionEtablissement': 'object',\n",
    "             'typeVoieEtablissement': 'object',\n",
    "             'libelleVoieEtablissement': 'object',\n",
    "             'codePostalEtablissement': 'object',\n",
    "             'libelleCommuneEtablissement': 'object',\n",
    "             'libelleCommuneEtrangerEtablissement': 'object',\n",
    "             'distributionSpecialeEtablissement': 'object',\n",
    "             'codeCommuneEtablissement': 'object',\n",
    "             'codeCedexEtablissement': 'object',\n",
    "             'libelleCedexEtablissement': 'object',\n",
    "             'codePaysEtrangerEtablissement': 'object',\n",
    "             'libellePaysEtrangerEtablissement': 'object',\n",
    "             'count_initial_insee': 'int',\n",
    "             'len_digit_address_insee':'object'\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calcul nb siren/siret\n",
    "upper_word = pd.read_csv('data/input/Parameters/upper_stop.csv'\n",
    "        ).iloc[:,0].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voie = (pd.read_csv('data/input/Parameters/voie.csv').assign(upper = lambda x: \n",
    "             x['possibilite'].str.isupper()).loc[lambda x: \n",
    "                                                 x['upper'].isin([True])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import sidetable\n",
    "def jackard_distance(inpi, insee):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    w1 = set(inpi)\n",
    "    w2 = set(insee)\n",
    "    \n",
    "    try:\n",
    "        return nltk.jaccard_distance(w1, w2)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_conformite_test(df):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    dic_ = {\n",
    "        'test_siege_principale':\n",
    "    df['test_siege_principal'].value_counts(),\n",
    "        \n",
    "        'test_siege_secondaire':\n",
    "    df['test_siege_secondaire'].value_counts(),\n",
    "        \n",
    "        'divergence_siege':\n",
    "    df['divergence_siege'].value_counts(),\n",
    "        \n",
    "        'test_etb_ferme':\n",
    "    df['test_status_ets_ferme'].value_counts(),\n",
    "        \n",
    "        'test_etb_ouvert':\n",
    "    df['test_status_ets_ouvert'].value_counts(),\n",
    "        \n",
    "        'divergence_etatadmin':\n",
    "    df['divergence_etatadmin'].value_counts(),\n",
    "        \n",
    "        'test_join_address':\n",
    "    df['test_join_address'].value_counts(),\n",
    "        \n",
    "        'test_address_libelle':\n",
    "    df['test_address_libelle'].value_counts(),\n",
    "        \n",
    "        'test_address_complement':\n",
    "    df['test_address_complement'].value_counts(),\n",
    "        \n",
    "        \n",
    "        'test_debut_activite_egal':\n",
    "    (df.loc[lambda x: \n",
    "      ~x['date_début_activité'].isin([np.datetime64('NaT')])\n",
    "     ]['test_date_egal']\n",
    " .value_counts()\n",
    "),\n",
    "        \n",
    "  'test_debut_activite_sup':      \n",
    "    (df.loc[lambda x: \n",
    "      ~x['date_début_activité'].isin([np.datetime64('NaT')])\n",
    "     ]['test_date_sup']\n",
    " .value_counts()\n",
    ")\n",
    "    }\n",
    "    return dic_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Règle de gestion\n",
    "\n",
    "Dans la première étape, nous allons merger les données de l'INSEE avec les données de l'INPI. Dans le cadre de ce notebook, nous allons utiliser uniquement les variables de matching suivantes:\n",
    "\n",
    "- Siren\n",
    "- Numéro de voie\n",
    "- Code postal\n",
    "- type de voie\n",
    "- Code commune\n",
    "- Ville\n",
    "\n",
    "``` \n",
    "'siren', 'digit_inpi', 'code_postal_matching', 'INSEE', 'code_commune', 'ncc' \n",
    "``` \n",
    "\n",
    "Des lors que la table a été matché et soustrait des index qui n'ont pas de correspondance a l'INSEE, nous pouvons effectuer les tests suivants:\n",
    "\n",
    "## Test Adresse\n",
    "\n",
    "- `test_address_libelle` :\n",
    "    - Si un des mots contenus dans la variable `adresse_new_clean_reg` est présente dans la variable `libelleVoieEtablissement` alors True\n",
    "- `test_address_complement`:\n",
    "    - Si un des mots contenus dans la variable `adresse_new_clean_reg` est présente dans la variable `libelleVoieEtablissement` alors True\n",
    "- `test_join_address`:\n",
    "    - Si `test_address_libelle` et `test_address_complement` sont égales a True, alors True\n",
    "- `jacquard`:\n",
    "    - Calcul de la distance entre `adresse_inpi_clean` et `adress_insee_reconstitue`\n",
    "\n",
    "## Test Siege\n",
    "\n",
    "- `test_siege_principal`:\n",
    "    - Si `type` contient `SIE` ou `SEP` ou `PRI` et `etablissementSiege` est égal a True, alors True\n",
    "- `test_siege_secondaire`:\n",
    "    - Si `type` contient `SEC` et `etablissementSiege` est égal a False, alors True\n",
    "- `divergence_siege`:\n",
    "    - Si `test_siege_principal` est égale à False et `test_siege_secondaire` est égal à False, alors True\n",
    "\n",
    "## Test Etat \n",
    "\n",
    "- `test_status_ets_ferme`:\n",
    "    - Si `last_libele_evt` est égale à `Etablissement supprimé` et `etatAdministratifEtablissement` est égal à `F` alors True\n",
    "- `test_status_ets_ouvert`:\n",
    "    - Si `last_libele_evt` est égale à `Etablissement ouvert` et `etatAdministratifEtablissement` est égal à `A` alors True\n",
    "- `divergence_etatadmin`: Si `test_status_ets_ferme` est égal à False et `test_status_ets_ouvert` est égal à False, alors True\n",
    "\n",
    "## Test Date\n",
    "\n",
    "- `test_date_egal`:\n",
    "    -  `date_début_activité` est égale à `dateCreationEtablissement` alors True \n",
    "- `test_date_sup`:\n",
    "    - `date_début_activité` est supérieure à `dateCreationEtablissement` alors True \n",
    "- `divergence_etatadmin`:\n",
    "    - Si `test_date_egal` est égal à False et `test_date_sup` est égal à False, alors True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_possibilities[0]['match']['inpi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un premier temps, on import et on merge les dataframes Insee et INPI. Pour rappel, il y a 10,648,546 d'observations à siretiser. Pour le vérifier, il suffit de regarder la variable index, qui est le numéro de la ligne dans la base initiale de l'INPI.\n",
    "\n",
    "A noté que nous convertissons la variable `date_début_activité` en format date, car nous avons besoin de comparer cette variable avec `dateCreationEtablissement` lors de nos tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ets = 'data/input/INPI/{0}/{1}_{2}.csv'.format(origin, filename, 1)\n",
    "\n",
    "inpi = (al_siret.import_dask(file=df_ets,\n",
    "                                usecols=inpi_col,\n",
    "                                dtype=inpi_dtype,\n",
    "                                parse_dates=False)\n",
    "       )\n",
    "insee = al_siret.import_dask(\n",
    "        file=al_siret.insee,\n",
    "        usecols=insee_col,\n",
    "        dtype=insee_dtype,\n",
    "        parse_dates = ['dateCreationEtablissement'])\n",
    "\n",
    "temp = inpi.merge(insee,\n",
    "                          how='left',\n",
    "                          left_on=list_possibilities[0]['match']['inpi'],\n",
    "                          right_on= list_possibilities[0]['match']['insee'],\n",
    "                          indicator=True,\n",
    "                          suffixes=['_insee', '_inpi'])\n",
    "temp = temp.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est assez simple de voir que le merge a abouti a la création de doublon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.shape[0] - temp['index'].max() + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons appliquer des règles de gestion sur les combinaisons matchées (les `both`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['_merge'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_check = (temp[temp['_merge']\n",
    "                 .isin(['both'])]\n",
    "            .drop(columns= '_merge')\n",
    "            .merge(voie, left_on = 'typeVoieEtablissement', right_on ='INSEE', how = 'left')\n",
    "            #### création addresse\n",
    "            .assign(\n",
    "            date_début_activité = lambda x: pd.to_datetime(\n",
    "            x['date_début_activité'], errors = 'coerce'),\n",
    "                numeroVoieEtablissement_ = lambda x: x['numeroVoieEtablissement'].fillna(''),   \n",
    "        adresse_insee_clean=lambda x: x['libelleVoieEtablissement'].str.normalize(\n",
    "                'NFKD')\n",
    "            .str.encode('ascii', errors='ignore')\n",
    "            .str.decode('utf-8')\n",
    "            .str.replace('[^\\w\\s]|\\d+', ' ')\n",
    "            .str.upper(),\n",
    "    adress_insee_reconstitue = lambda x: \n",
    "            x['numeroVoieEtablissement_'] + ' '+ \\\n",
    "            x['possibilite'] + ' ' + \\\n",
    "            x['libelleVoieEtablissement']   \n",
    "        )\n",
    "            .assign(\n",
    "            \n",
    "        adresse_insee_clean = lambda x: x['adresse_insee_clean'].apply(\n",
    "        lambda x:' '.join([word for word in str(x).split() if word not in\n",
    "        (upper_word)])),\n",
    "            \n",
    "        adresse_inpi_reconstitue = lambda x: x['adress_new'].apply(\n",
    "        lambda x:' '.join([word for word in str(x).split() if word not in\n",
    "        (upper_word)])),   \n",
    "            \n",
    "        adress_insee_reconstitue = lambda x: x['adress_insee_reconstitue'].apply(\n",
    "        lambda x:' '.join([word for word in str(x).split() if word not in\n",
    "        (upper_word)])), \n",
    "        )\n",
    "            #### création tests\n",
    "            .assign(   \n",
    "                test_siege_principal = lambda x: np.where(\n",
    "        np.logical_and(\n",
    "        x['type'].isin(['SIE', 'SEP', 'PRI']),\n",
    "        x['etablissementSiege'].isin(['true'])\n",
    "        ),\n",
    "        True, False\n",
    "        ),\n",
    "                test_siege_secondaire = lambda x:np.where(\n",
    "        np.logical_and(\n",
    "        x['type'].isin(['SEC']),\n",
    "        x['etablissementSiege'].isin(['false'])\n",
    "        ),\n",
    "        True, False\n",
    "        ),\n",
    "                test_status_ets_ferme = lambda x: np.where(\n",
    "        np.logical_and(\n",
    "        x['last_libele_evt'].isin(['Etablissement supprimé']),\n",
    "        x['etatAdministratifEtablissement'].isin(['F'])\n",
    "        ),\n",
    "        True, False\n",
    "        ),\n",
    "                test_status_ets_ouvert = lambda x: np.where(\n",
    "        np.logical_and(\n",
    "        x['last_libele_evt'].isin(['Etablissement ouvert']),\n",
    "        x['etatAdministratifEtablissement'].isin(['A'])\n",
    "        ),\n",
    "        True, False\n",
    "        ),\n",
    "                test_date_egal = lambda x:np.where(\n",
    "        x['date_début_activité'] ==\n",
    "                     x['dateCreationEtablissement']\n",
    "        ,\n",
    "        True, False\n",
    "        ),\n",
    "                test_date_sup = lambda x: np.where(\n",
    "        x['date_début_activité'] <=\n",
    "                     x['dateCreationEtablissement']\n",
    "        ,\n",
    "        True, False\n",
    "        ),\n",
    "                divergence_siege = lambda x: np.where(\n",
    "        np.logical_and(\n",
    "        x['test_siege_principal'].isin([False]),\n",
    "        x['test_siege_secondaire'].isin([False])\n",
    "        ),\n",
    "        True, False\n",
    "        ),\n",
    "        divergence_etatadmin = lambda x: np.where(\n",
    "        np.logical_and(\n",
    "        x['test_status_ets_ferme'].isin([False]),\n",
    "        x['test_status_ets_ouvert'].isin([False])\n",
    "        ),\n",
    "        True, False\n",
    "        )\n",
    "            )\n",
    "            )\n",
    "to_check.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test 1: address\n",
    "df_2 = dd.from_pandas(to_check, npartitions=10)\n",
    "df_2['test_address_libelle'] = df_2.map_partitions(\n",
    "            lambda df:\n",
    "                df.apply(lambda x:\n",
    "                    find_regex(\n",
    "                     x['adresse_new_clean_reg'],\n",
    "                     x['libelleVoieEtablissement']), axis=1)\n",
    "                     ).compute()\n",
    "\n",
    "df_2['test_address_complement'] = df_2.map_partitions(\n",
    "            lambda df:\n",
    "                df.apply(lambda x:\n",
    "                    find_regex(\n",
    "                     x['adresse_new_clean_reg'],\n",
    "                     x['complementAdresseEtablissement']), axis=1)\n",
    "                     ).compute()\n",
    "\n",
    "df_2['jacquard'] = df_2.map_partitions(\n",
    "            lambda df:\n",
    "                df.apply(lambda x:\n",
    "                    jackard_distance(\n",
    "                     x['adresse_inpi_reconstitue'],\n",
    "                     x['adress_insee_reconstitue']), axis=1)\n",
    "                     ).compute()\n",
    "\n",
    "df_2 = df_2.compute()\n",
    "## test join Adress\n",
    "df_2.loc[\n",
    "        (df_2['test_address_libelle'] == True)\n",
    "        &(df_2['test_address_complement'] == True),\n",
    "        'test_join_address'] = True\n",
    "\n",
    "df_2.loc[\n",
    "        (df_2['test_join_address'] != True),\n",
    "        'test_join_address'] = False\n",
    "\n",
    "df_2 = df_2.assign(\n",
    "    min_jacquard = lambda x:\n",
    "    x.groupby(['siren', 'code_greffe', 'nom_greffe', 'numero_gestion',\n",
    "               'id_etablissement'])['jacquard'].transform('min'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2['jacquard'] = df_2.map_partitions(\n",
    "            lambda df:\n",
    "                df.apply(lambda x:\n",
    "                    jackard_distance(\n",
    "                     x['adresse_inpi_reconstitue'],\n",
    "                     x['adress_insee_reconstitue']), axis=1)\n",
    "                     ).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Nombre de duplicate\n",
    "df_2.shape[0] -df_2['index'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_2[['adress_insee_reconstitue', 'adresse_inpi_reconstitue', 'jacquard',\n",
    "#      'test_address_libelle', 'min_jacquard']].head(22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 5: Selection meilleurs candidats\n",
    "\n",
    "Dans cette dernière étape, on ne garde que les valeurs ont les tests ont été concluant sur le regex de l'adresse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_conformite_test(df = df_2)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
