{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programme de Matching: Nouvelles règles de gestion\n",
    "\n",
    "## Algorithme\n",
    "\n",
    "### origin\n",
    "\n",
    "Comme la taille de la donnée est trop élevée, il faut prendre un sous échantillon pour faire la siretisation. Le sous échantillonage se fait avec l'origine. \n",
    "\n",
    "Input:\n",
    "- INSEE:\n",
    "    - NEW: `data/input/INSEE/NEW/insee_1745311_NEW.csv`\n",
    "    - Initial, Partiel, EVT: `data/input/INSEE/InitialPartielEVT/insee_8272605_InitialPartielEVT.csv`\n",
    "- INPI:\n",
    "    - NEW: `data/input/SIREN_INPI/NEW/inpi_initial_partiel_evt_new_ets_status_final_NEW_0.csv`\n",
    "    - Initial, Partiel, EVT: `data/input/SIREN_INPI/InitialPartielEVT/inpi_initial_partiel_evt_new_ets_status_final_InitialPartielEVT_0`\n",
    "\n",
    "Output:\n",
    "- `data/output/` + `ORIGIN`\n",
    "    - contient les matches\n",
    "- `data/input/INPI/special_treatment/`+ `ORIGIN`\n",
    "    - contient les non matches\n",
    "- `data/logs/`+ `ORIGIN`\n",
    "    - contient les logs\n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "os.chdir('../')\n",
    "current_dir = os.getcwd()\n",
    "from inpi_insee import siretisation\n",
    "import pandas as pd\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "param = {\n",
    "    #'insee': 'data/input/INSEE/InitialPartielEVTNEW/insee_1557220_InitialPartielEVTNEW.csv' ### PP\n",
    "    'insee': 'data/input/INSEE/InitialPartielEVTNEW/insee_9368683_InitialPartielEVTNEW.csv'  ### ETS\n",
    "    #'insee': 'data/input/INSEE/NEW/insee_1745311_NEW.csv' ### ETS\n",
    "}\n",
    "# 4824158 SIREN a trouver!\n",
    "al_siret = siretisation.siretisation_inpi(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from itertools import compress, product\n",
    "\n",
    "#def combinations(items):\n",
    "#    return ( set(compress(items,mask)) for \n",
    "#            mask in product(*[[0,1]]*len(items)))\n",
    "\n",
    "#all_list = ['ncc',\n",
    "#             'Code_Postal','Code_Commune',\n",
    "#             'INSEE','digit_inpi']\n",
    "#test = list(combinations(items = all_list))[1:]\n",
    "#sort_list = sorted(test[1:], key=lambda k: len(k), reverse=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_inpi = ['ncc','code_postal_matching','code_commune','INSEE','digit_inpi']\n",
    "list_insee = ['libelleCommuneEtablissement',\n",
    "            'codePostalEtablissement', 'codeCommuneEtablissement',\n",
    "            'typeVoieEtablissement','numeroVoieEtablissement']\n",
    "\n",
    "sort_list = [\n",
    " {'ncc', 'code_postal_matching', 'code_commune', 'INSEE', 'digit_inpi'},\n",
    " {'ncc', 'code_postal_matching', 'code_commune', 'INSEE'},\n",
    " {'ncc', 'code_postal_matching', 'code_commune', 'digit_inpi'},\n",
    " {'ncc', 'code_postal_matching', 'code_commune'},   \n",
    " {'ncc', 'code_postal_matching'},\n",
    " {'ncc'},\n",
    " {'code_postal_matching'},\n",
    " {'code_commune'}\n",
    "]\n",
    "len(sort_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_possibilities = []\n",
    "for i in sort_list:\n",
    "    left =[]\n",
    "    right = []\n",
    "    for j in i:\n",
    "        left.append(j)\n",
    "        right.append(list_insee[list_inpi.index(j)])\n",
    "    left.insert(0,'siren')\n",
    "    right.insert(0,'siren')\n",
    "    \n",
    "    dic_ = {\n",
    "    'match':{\n",
    "        'inpi':left,\n",
    "        'insee':right,\n",
    "    }\n",
    "}\n",
    "    list_possibilities.append(dic_)\n",
    "list_possibilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indiquer le fichiers a siretiser. Si pas en local, le télécharger depuis le S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = 'inpi_initial_partiel_evt_new_ets_status_final_test_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from awsPy.aws_authorization import aws_connector\n",
    "#from awsPy.aws_s3 import service_s3\n",
    "#from pathlib import Path\n",
    "#import pandas as pd\n",
    "#bucket = 'calfdata'\n",
    "#path = os.getcwd()\n",
    "#parent_path = str(Path(path).parent)\n",
    "#path_cred = r\"{}/programme_matching/credential_AWS.json\".format(parent_path)\n",
    "#con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "#                                        region = 'eu-west-3')\n",
    "#client= con.client_boto()\n",
    "#s3 = service_s3.connect_S3(client = client,\n",
    "#                      bucket = 'calfdata') \n",
    "#s3.download_file(\n",
    "#    key= 'INPI/TC_1/02_preparation_donnee/Stock/ETB/{}_0.csv'.format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_ets = 'data/input/INPI/{}_{}.csv'.format(filename, key)\n",
    "#df_ets\n",
    "#'data/input/INPI/{}_{}.csv'.format(filename, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import shutil\n",
    "#try:\n",
    "#    os.remove(\"data/input/INPI/{}_0.gz\".format(filename))\n",
    "#except:\n",
    "#    pass\n",
    "#shutil.move(\"{}_0.gz\".format(filename),\n",
    "#            \"data/input/INPI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut prendre l'`origin` et `filename` que l'on souhaite sitetiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "files = glob.glob('data/logs/InitialPartielEVTNEW/*')\n",
    "for f in files:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = \"InitialPartielEVTNEW\"\n",
    "filename = \"inpi_initial_partiel_evt_new_ets_status_final_InitialPartielEVTNEW\" ####ETS\n",
    "#filename = \"inpi_initial_partiel_evt_new_ets_status_final_InitialPartielEVTNEW\"\n",
    "#origin = \"NEW\"\n",
    "#filename = \"inpi_initial_partiel_evt_new_ets_status_final_NEW\"\n",
    "#### make dir\n",
    "parent_dir = 'data/output/'\n",
    "parent_dir_1 = 'data/input/INPI/special_treatment/'\n",
    "parent_dir_2 = 'data/logs/'\n",
    "\n",
    "for d in [parent_dir,parent_dir_1,parent_dir_2]:\n",
    "    path = os.path.join(d, origin) \n",
    "    try:\n",
    "        os.mkdir(path) \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parametres et fonctions\n",
    "\n",
    "- `import_dask`: Charge csv en Dask DataFrame pour clusteriser les calculs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_duplication(df):\n",
    "        \"\"\"\n",
    "        Split un dataframe si l'index (la variable, pas l'index) contient des\n",
    "        doublons.\n",
    "\n",
    "        L'idée est de distinguer les doublons resultants du merge avec l'INSEE\n",
    "\n",
    "        Args:\n",
    "        - df: Pandas dataframe contenant au moins une variable \"index\"\n",
    "\n",
    "        Returns:\n",
    "        - Un Dictionary avec:\n",
    "            - not_duplication: Dataframe ne contenant pas les doublons\n",
    "            - duplication: Dataframe contenant les doublons\n",
    "            - report_dup: Une Serie avec le nombres de doublons\n",
    "        \"\"\"\n",
    "        if 'count_duplicates_' in df.columns:\n",
    "            df = df.drop(columns = 'count_duplicates_')\n",
    "\n",
    "        df = df.merge(\n",
    "            (df\n",
    "                .groupby('index')['index']\n",
    "                .count()\n",
    "                .rename('count_duplicates_')\n",
    "                .reset_index()\n",
    "                )\n",
    "                )\n",
    "        \n",
    "        dic_ = {\n",
    "            'not_duplication':df[df['count_duplicates_'].isin([1])],\n",
    "            'duplication' : df[~df['count_duplicates_'].isin([1])],\n",
    "            'report_dup':df[\n",
    "            ~df['count_duplicates_'].isin([1])\n",
    "            ]['count_duplicates_'].value_counts()\n",
    "            }\n",
    "\n",
    "        return dic_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_regex(regex, test_str):\n",
    "        \"\"\"\n",
    "        Performe une recherche regex entre deux colonnes.\n",
    "\n",
    "        Args:\n",
    "        - regex: string: charactère contenant la formule regex\n",
    "        - test_str: String: String contenant le regex a trouver\n",
    "\n",
    "        Return:\n",
    "        Boolean, True si le regex est trouvé, sinon False\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            matches = re.search(regex, test_str)\n",
    "            if matches:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        except:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpi_col = ['siren',\n",
    "            'index',\n",
    "            'type',\n",
    "            'code_postal_matching',\n",
    "            'ville',\n",
    "            'code_commune',\n",
    "            'pays',\n",
    "            'count_initial_inpi',\n",
    "            'ncc',\n",
    "            'adresse_new_clean_reg',\n",
    "            'adress_new',\n",
    "            'INSEE',\n",
    "            'date_début_activité',\n",
    "            'digit_inpi',\n",
    "            'len_digit_address_inpi',\n",
    "            'list_digit_inpi'\n",
    "            ]\n",
    "\n",
    "inpi_dtype = {\n",
    "    'siren': 'object',\n",
    "    'index': 'int',\n",
    "    'type': 'object',\n",
    "    'code_postal_matching': 'object',\n",
    "    'ville': 'object',\n",
    "    'code_commune': 'object',\n",
    "    'pays': 'object',\n",
    "    'count_initial_inpi': 'int',\n",
    "    'ncc': 'object',\n",
    "    'adresse_new_clean_reg': 'object',\n",
    "    'adress_new':'object',\n",
    "    'INSEE': 'object',\n",
    "    'date_début_activité': 'object',\n",
    "    'digit_inpi': 'object',\n",
    "    'len_digit_address_inpi':'object'\n",
    "}\n",
    "\n",
    "insee_col = ['siren',\n",
    "         'siret',\n",
    "         'dateCreationEtablissement',\n",
    "         \"etablissementSiege\",\n",
    "         \"etatAdministratifEtablissement\",\n",
    "         'complementAdresseEtablissement',\n",
    "         'numeroVoieEtablissement',\n",
    "         'indiceRepetitionEtablissement',\n",
    "         'typeVoieEtablissement',\n",
    "         'libelleVoieEtablissement',\n",
    "         'codePostalEtablissement',\n",
    "         'libelleCommuneEtablissement',\n",
    "         'libelleCommuneEtrangerEtablissement',\n",
    "         'distributionSpecialeEtablissement',\n",
    "         'codeCommuneEtablissement',\n",
    "         'codeCedexEtablissement',\n",
    "         'libelleCedexEtablissement',\n",
    "         'codePaysEtrangerEtablissement',\n",
    "         'libellePaysEtrangerEtablissement',\n",
    "         'count_initial_insee','len_digit_address_insee','list_digit_insee']\n",
    "\n",
    "insee_dtype = {\n",
    "             'siren': 'object',\n",
    "             'siret': 'object',\n",
    "             \"etablissementSiege\": \"object\",\n",
    "             \"etatAdministratifEtablissement\": \"object\",\n",
    "             #'dateCreationEtablissement': 'object',\n",
    "             'complementAdresseEtablissement': 'object',\n",
    "             'numeroVoieEtablissement': 'object',\n",
    "             'indiceRepetitionEtablissement': 'object',\n",
    "             'typeVoieEtablissement': 'object',\n",
    "             'libelleVoieEtablissement': 'object',\n",
    "             'codePostalEtablissement': 'object',\n",
    "             'libelleCommuneEtablissement': 'object',\n",
    "             'libelleCommuneEtrangerEtablissement': 'object',\n",
    "             'distributionSpecialeEtablissement': 'object',\n",
    "             'codeCommuneEtablissement': 'object',\n",
    "             'codeCedexEtablissement': 'object',\n",
    "             'libelleCedexEtablissement': 'object',\n",
    "             'codePaysEtrangerEtablissement': 'object',\n",
    "             'libellePaysEtrangerEtablissement': 'object',\n",
    "             'count_initial_insee': 'int',\n",
    "             'len_digit_address_insee':'object'\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pas a pas\n",
    "\n",
    "Dans l'ensemble, le processus de siretisation fonctionne de la manière suivante:\n",
    "\n",
    "1. Merge INSEE-INPI\n",
    "2. Differenciation entre les doublons et non doublons\n",
    "    - Recupération non doublons -> matched\n",
    "    - Recupération doublons -> tests à réaliser\n",
    "3. Batterie de tests via des regles de gestions sur la date\n",
    "4. Batterie de tests via des regles de gestions sur l'adresse et type d'établissement\n",
    "5. Selection des meilleurs candidats via les règles de gestion\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 1: Merge INSEE-INPI\n",
    "\n",
    "Dans la première étape, nous allons merger les données de l'INSEE avec les données de l'INPI. Dans le cadre de ce notebook, nous allons utiliser uniquement les variables de matching suivantes:\n",
    "\n",
    "- Siren\n",
    "- Numéro de voie\n",
    "- Code postal\n",
    "- type de voie\n",
    "- Code commune\n",
    "- Ville\n",
    "\n",
    "``` \n",
    "'siren', 'digit_inpi', 'code_postal_matching', 'INSEE', 'code_commune', 'ncc' \n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_possibilities[0]['match']['inpi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un premier temps, on import et on merge les dataframes Insee et INPI. Pour rappel, il y a 10,648,546 d'observations à siretiser. Pour le vérifier, il suffit de regarder la variable index, qui est le numéro de la ligne dans la base initiale de l'INPI.\n",
    "\n",
    "A noté que nous convertissons la variable `date_début_activité` en format date, car nous avons besoin de comparer cette variable avec `dateCreationEtablissement` lors de nos tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ets = 'data/input/INPI/{0}/{1}_{2}.csv'.format(origin, filename, 0)\n",
    "\n",
    "inpi = (al_siret.import_dask(file=df_ets,\n",
    "                                usecols=inpi_col,\n",
    "                                dtype=inpi_dtype,\n",
    "                                parse_dates=False)\n",
    "       )\n",
    "insee = al_siret.import_dask(\n",
    "        file=al_siret.insee,\n",
    "        usecols=insee_col,\n",
    "        dtype=insee_dtype,\n",
    "        parse_dates = ['dateCreationEtablissement'])\n",
    "\n",
    "temp = inpi.merge(insee,\n",
    "                          how='left',\n",
    "                          left_on=list_possibilities[0]['match']['inpi'],\n",
    "                          right_on= list_possibilities[0]['match']['insee'],\n",
    "                          indicator=True,\n",
    "                          suffixes=['_insee', '_inpi'])\n",
    "temp = temp.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est assez simple de voir que le merge a abouti a la création de doublon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.shape[0] - temp['index'].max() + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons appliquer des règles de gestion sur les combinaisons matchées (les `both`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['_merge'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_check = (temp[temp['_merge']\n",
    "                 .isin(['both'])]\n",
    "            .drop(columns= '_merge')\n",
    "            .assign(\n",
    "            date_début_activité = lambda x: pd.to_datetime(\n",
    "            x['date_début_activité'], errors = 'coerce')\n",
    "            )\n",
    "                 )\n",
    "to_check.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 2: Separation des doublons\n",
    "\n",
    "Comme nous venons de le voir, sur les lignes matchées, il y a à peu près 85k lignes de doublons. De fait, nous devons écarter les lignes avec des doublons de ceux qui ne le sont pas. Nous avons créer une fonction, appelée `split_duplication` qui va compter le nombre de lignes ayant le même index (la variable). \n",
    "\n",
    "La fonction retourne un dictionnaire, avec un dataframe contenant uniquement les lignes sans doublon, un dataframe avec les doublons et enfin, un rapport sur le nombre d'index avec des doublons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1 = split_duplication(df = to_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci dessous, un tableau avec le nombre de doublons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1['report_dup']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le dictionnaire est composé d'un DataFrame avec les valeurs sans doublons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: doublon -> non\n",
    "test_1['not_duplication'] = test_1['not_duplication'].assign(\n",
    "        origin_test = 'test_1_no_duplication'\n",
    "        )\n",
    "test_1['not_duplication'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a environ 98% des lignes de siren sur d'avoir le bon siret. Toutefois, nous devons quand même effectuer des tests pour etre certains du résultat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1['not_duplication'].shape[0] / to_check['index'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etape 3: Test sur les doublons, via la date\n",
    "\n",
    "Dans cette étape la, nous allons procéder a des tests sur la date afin d'écarter les établissements enregistrés avant la date de création de l'activité. En effet, la variable `date_début_activité` indique la date de création de l'entreprise (ou activité si personne physique), alors que la variable `dateCreationEtablissement` informe sur la date de création de l'établissement. Il n'est donc pas possible que la création de l'établissement se fasse avant la date de création de l'entreprise. \n",
    "\n",
    "Pour chaque règle, nous appliquons la fonction `split_duplication` pour être sur que nous avons bien écarté les doublons.\n",
    "\n",
    "Tout d'abord, nous ne gardons que les lignes ou la date de début d'activité est identique à la date de création de l'établissement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test 2: Date equal -> oui\n",
    "test_2_oui = test_1['duplication'][\n",
    "        (test_1['duplication']['date_début_activité'] ==\n",
    "                     test_1['duplication']['dateCreationEtablissement'])\n",
    "                     ]\n",
    "test_2_bis = split_duplication(df = test_2_oui)\n",
    "#### Test 2: Date equal -> oui, Test 2 bis: doublon: non\n",
    "\n",
    "test_2_bis['not_duplication'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, nous regardons les lignes qui ont une date de création de l'établissement supérieur à la date de création de l'entreprise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Test 2: Date equal -> oui, Test 2 bis: doublon: oui\n",
    "## Test 2: Date equal -> non\n",
    "### Test 2: Date equal -> non -> test 3: Date sup -> oui\n",
    "test_3_oui = test_1['duplication'].loc[\n",
    "        (test_1['duplication']['dateCreationEtablissement'] >\n",
    "        test_1['duplication']['date_début_activité'])\n",
    "        & (~test_1['duplication']['index'].isin(test_2_oui['index'].to_list()))\n",
    "        ]\n",
    "##### Test 2: Date equal -> non -> test 3: Date sup -> oui\n",
    "##### Test 3 bis: doublon:\n",
    "test_3_oui_bis = split_duplication(df = test_3_oui)\n",
    "test_3_oui_bis['not_duplication'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On attribut a chaque dataframe, l'origne de la récupération de l'information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2_bis['not_duplication'] = test_2_bis['not_duplication'].assign(\n",
    "        origin_test = 'test_2_no_duplication'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2_bis['duplication'] = test_2_bis['duplication'].assign(\n",
    "        origin_test = 'test_2_duplication'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Test 3 bis: doublon: non\n",
    "test_3_oui_bis['not_duplication'] = \\\n",
    "        test_3_oui_bis['not_duplication'].assign(\n",
    "         origin_test = 'test_3_no_duplication'\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Test 3 bis: doublon:oui\n",
    "test_3_oui_bis['duplication'] = \\\n",
    "        test_3_oui_bis['duplication'].assign(\n",
    "         origin_test = 'test_3_duplication'\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a deux dataframes. Le `df_no_duplication` contient des siren/siret correctement matchés, alors que le dataframe `df_duplication` a besoin de davatantage de traitement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Append to dataframes\n",
    "df_no_duplication = pd.concat([\n",
    "        test_1['not_duplication'],\n",
    "        test_2_bis['not_duplication'],\n",
    "        test_3_oui_bis['not_duplication']\n",
    "        ], axis = 0)\n",
    "\n",
    "df_duplication = pd.concat([\n",
    "        test_2_bis['duplication'],\n",
    "        test_3_oui_bis['duplication']\n",
    "        ], axis =0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etape 4: Test sur les doublons, via l'adresse et autre regles specifiques\n",
    "\n",
    "Dans cette dernière étape, nous allons effectuer un dernière ensemble de tests avant de faire le trie sur les éléments que nous gardons. \n",
    "\n",
    "Tout d'abord, nous allons compter le nombre de siren qu'il y a selon les variables utilisées lors du matching. Ensuite, nous appliquons le regex sur les variables de l'adresse à l'INPI. Puis on réalise les tests sur le siège, la date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calcul nb siren/siret\n",
    "df_ = (df_duplication\n",
    "        .merge(\n",
    "        (df_duplication\n",
    "        .groupby(list_possibilities[0]['match']['inpi'])['siren']\n",
    "             .count()\n",
    "             .rename('count_siren_siret')\n",
    "             .reset_index()\n",
    "             ),how = 'left'\n",
    "             )\n",
    "             )\n",
    "df_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_['index'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex sur l'adresse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test 1: address\n",
    "df_2 = dd.from_pandas(df_, npartitions=10)\n",
    "df_2['test_address_libelle'] = df_2.map_partitions(\n",
    "            lambda df:\n",
    "                df.apply(lambda x:\n",
    "                    find_regex(\n",
    "                     x['adresse_new_clean_reg'],\n",
    "                     x['libelleVoieEtablissement']), axis=1)\n",
    "                     ).compute()\n",
    "\n",
    "df_2['test_address_complement'] = df_2.map_partitions(\n",
    "            lambda df:\n",
    "                df.apply(lambda x:\n",
    "                    find_regex(\n",
    "                     x['adresse_new_clean_reg'],\n",
    "                     x['complementAdresseEtablissement']), axis=1)\n",
    "                     ).compute()\n",
    "\n",
    "df_2 = df_2.compute()\n",
    "## test join Adress\n",
    "df_2.loc[\n",
    "        (df_2['test_address_libelle'] == True)\n",
    "        &(df_2['test_address_complement'] == True),\n",
    "        'test_join_address'] = True\n",
    "\n",
    "df_2.loc[\n",
    "        (df_2['test_join_address'] != True),\n",
    "        'test_join_address'] = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test 4: voie\n",
    "df_2.loc[\n",
    "        df_2['INSEE'] == df_2['typeVoieEtablissement'],\n",
    "        'test_voie'\n",
    "        ] = True\n",
    "\n",
    "df_2.loc[\n",
    "        df_2['INSEE'] != df_2['typeVoieEtablissement'],\n",
    "        'test_voie'\n",
    "        ] = False\n",
    "\n",
    "        ## Test 5: numero voie\n",
    "df_2.loc[\n",
    "        (df_2['digit_inpi'] == df_2['numeroVoieEtablissement']),\n",
    "        'test_numero'\n",
    "        ] = True\n",
    "\n",
    "df_2.loc[\n",
    "        df_2['digit_inpi'] != df_2['numeroVoieEtablissement'],\n",
    "        'test_numero'\n",
    "        ] = False\n",
    "\n",
    "df_2.loc[\n",
    " df_2['digit_inpi'].isin([np.nan])\n",
    " & df_2['numeroVoieEtablissement'].isin([np.nan]),\n",
    "        'test_numero'\n",
    "        ] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.loc[lambda x : x['test_numero'].isin([False])].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test sur la date\n",
    "\n",
    "En plus de la date, on filtre les lignes qui ont seulement un établissement par siren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test 2: Date\n",
    "df_2.loc[\n",
    "        (df_2['dateCreationEtablissement'] >=\n",
    "        df_2['date_début_activité'])\n",
    "        | (df_2['date_début_activité'].isin([np.nan]))\n",
    "        | (df_2['count_siren_siret'].isin([1])\n",
    "        & df_2['count_initial_insee'].isin([1])),\n",
    "        'test_date'] = True\n",
    "df_2.loc[df_2['test_date'].isin([np.nan]),'test_1'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test sur le siège\n",
    "\n",
    "Ici, on vérifie que le status de l'établissement est bien identique à l'INSEE et à l'INPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test 3: siege\n",
    "df_2['test_siege'] = np.where(\n",
    "        np.logical_and(\n",
    "        df_2['type'].isin(['SEP', 'SIE']),\n",
    "        df_2['etablissementSiege'].isin(['true'])\n",
    "        ),\n",
    "        True, False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Final test: count unique index\n",
    "df_2 = df_2.merge(\n",
    "        (df_2\n",
    "        .groupby('index')['index']\n",
    "        .count()\n",
    "        .rename('count_duplicates_final')\n",
    "        .reset_index()\n",
    "        )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['test_join_address',\n",
    "          'test_address_libelle',\n",
    "          'test_address_complement',\n",
    "          'test_date','test_siege',\n",
    "          'test_voie', 'test_numero']:\n",
    "    print(df_2[i].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 5: Selection meilleurs candidats\n",
    "\n",
    "Dans cette dernière étape, on ne garde que les valeurs ont les tests ont été concluant sur le regex de l'adresse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_not_duplicate = pd.DataFrame()\n",
    "copy_duplicate = df_2.copy()\n",
    "duplicates_ = df_2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['test_join_address','test_address_libelle',\n",
    "         'test_address_complement']:\n",
    "         ### split duplication\n",
    "            test_1 = split_duplication(\n",
    "            copy_duplicate[\n",
    "            copy_duplicate[i].isin([True])]\n",
    "    )\n",
    "\n",
    "            ### append unique\n",
    "            df_not_duplicate = (\n",
    "            df_not_duplicate\n",
    "            .append(test_1['not_duplication']\n",
    "            .assign(test = i)\n",
    "            )\n",
    "            )\n",
    "\n",
    "            copy_duplicate = (copy_duplicate\n",
    "                   .loc[~copy_duplicate['index'].isin(\n",
    "                       pd.concat([\n",
    "                           test_1['duplication'],\n",
    "                           test_1['not_duplication']\n",
    "                       ], axis = 0)['index']\n",
    "                       .drop_duplicates())])\n",
    "\n",
    "            # Special treatment\n",
    "            sp = (df_2[\n",
    "            ~df_2['index']\n",
    "            .isin(df_not_duplicate['index'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_not_duplicate.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtre matché\n",
    "\n",
    "On sauvegarde pour le prochain loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input -> Save for the next loop \n",
    "inpi.loc[\n",
    "        (~inpi['index'].isin(df_no_duplication['index'].unique()))\n",
    "        & (~inpi['index'].isin(df_not_duplicate['index'].unique()))\n",
    "        & (~inpi['index'].isin(sp['index'].unique()))\n",
    "    ].compute().shape"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
