{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation table distance word2vec et merge table ets inpi insee cas \n",
    "\n",
    "Copy paste from Coda to fill the information\n",
    "\n",
    "## Objective(s)\n",
    "\n",
    "* Création d’une table avec un rank qui récapitule par ordre de préférence les relations entre les tests. Pour cela, on va utiliser 4 variables:\n",
    "     *  status_cas \n",
    "     * index_id_duplicate \n",
    "     *  test_ligne_num_voie \n",
    "     *  test_siege \n",
    "     *  test_enseigne\n",
    "         * Un produit cartésien va être réalisé sur l’ensemble de ses tests pour avoir une matrice avec 162 cas possibles triés par ordre de préférence\n",
    "     * test_siren_insee_siren_inpi\n",
    "      *  count_inpi_siren_sequence,\n",
    "        * Pour un siren donné, combien de séquence (etb au sens de l’INPI) possible. Si → 3, cela signifie que pour un même siren, il y a 3 établissements au sens de l’INPI\n",
    "        * count_initial_insee = count_inpi_siren_siret THEN 'True' ELSE 'False'\n",
    "        * Si la variable est ‘True’ alors tous les établissements ont été trouvé    \n",
    "    * test_distance_cosine \n",
    "    * test_distance_levhenstein \n",
    "     * La création de la table ets_inpi_insee_cases doit contenir les variables suivantes:\n",
    "    * Rank: \n",
    "      * ordre chronologique des tests. 1 étant le meilleur des cas, car tous les tests ont été réussi. \n",
    "\n",
    "    - max_distance_cosine\n",
    "    - test_distance_costine:\n",
    "        - test si la distance max est supérieur a .6\n",
    "    - levhenstein_distance\n",
    "    - test_levhenstein\n",
    "        - test si l'edit distance est inférieure ou égale a 1\n",
    "\n",
    "  - Dans cette US, nous allons créer ses deux variables et les ajouter à la table ets_inpi_insee_cases. Une nouvelle table sera créé, appelée ets_inpi_insee_cases_distance . Les nouvelles variables a ajouter sont les suivantes:\n",
    "\n",
    "  - unzip_inpi, \n",
    "\n",
    "    - mot ayant servi coté inpi pour trouver la distance\n",
    "\n",
    "  -  unzip_insee, \n",
    "\n",
    "    - mot ayant servi coté inse pour trouver la distance\n",
    "\n",
    "  - max_cosine_distance, \n",
    "\n",
    "    - distance maximum de l’index\n",
    "\n",
    "  -  test as key_except_to_test\n",
    "\n",
    "    - liste contenant les clés valeurs des mots non communs\n",
    "\n",
    "  - Une table intermédiaire contenant le max de la distance sera calculé, avec la Levhenstein aussi. La table s’appelle  ets_inpi_distance_max_word2vec \n",
    "\n",
    "## Metadata \n",
    "\n",
    "- Metadata parameters are available here: [Ressources_suDYJ#_luZqd](http://Ressources_suDYJ#_luZqd)\n",
    "\n",
    "  - Task type:\n",
    "\n",
    "     - Jupyter Notebook\n",
    "\n",
    "  - Users: :\n",
    "\n",
    "      - [Thomas Pernet](mailto:t.pernetcoudrier@gmail.com)\n",
    "\n",
    "  - Watchers:\n",
    "\n",
    "      - [Thomas Pernet](mailto:t.pernetcoudrier@gmail.com)\n",
    "\n",
    "  - Estimated Log points:\n",
    "\n",
    "      - One being a simple task, 15 a very difficult one\n",
    "        -  10\n",
    "\n",
    "  - Task tag\n",
    "\n",
    "      - \\#machine-learning,#sql-query,#computation,#word2vec\n",
    "\n",
    "  - Toggl Tag\n",
    "\n",
    "      - \\#variable-computation\n",
    "  \n",
    "## Input Cloud Storage [AWS/GCP]\n",
    "\n",
    "If link from the internet, save it to the cloud first\n",
    "\n",
    "### Tables [AWS/BigQuery]\n",
    "\n",
    "- Batch 1:\n",
    "\n",
    "  - Select Provider: Athena\n",
    "\n",
    "    - Select table(s): ets_inpi_insee_cases\n",
    "\n",
    "    - Select only tables created from the same notebook, else copy/paste selection to add new input tables\n",
    "\n",
    "      - If table(s) does not exist, add them: \n",
    "\n",
    "        Add New Table\n",
    "\n",
    "      - Information:\n",
    "\n",
    "      - Region: \n",
    "\n",
    "        - NameEurope (Paris)\n",
    "          - Code: eu-west-3\n",
    "\n",
    "        - Database: inpi\n",
    "\n",
    "        - Notebook construction file: [07_pourcentage_siretisation_v3](https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/02_siretisation/07_pourcentage_siretisation_v3.md)\n",
    "    \n",
    "## Destination Output/Delivery\n",
    "\n",
    "- AWS\n",
    "\n",
    "    - Athena: \n",
    "\n",
    "      - Region: Europe (Paris)\n",
    "        - Database: inpi\n",
    "        - Tables (Add name new table): ets_inpi_distance_max_word2vec,ets_inpi_insee_cases_distance\n",
    "        - List new tables\n",
    "        - ets_inpi_distance_max_word2vec, ets_inpi_insee_cases_distance\n",
    "\n",
    "## Things to know (Steps, Attention points or new flow of information)\n",
    "\n",
    "### Sources of information  (meeting notes, Documentation, Query, URL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connexion serveur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsPy.aws_authorization import aws_connector\n",
    "from awsPy.aws_athena import service_athena\n",
    "from awsPy.aws_s3 import service_s3\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, shutil\n",
    "bucket = 'calfdata'\n",
    "path = os.getcwd()\n",
    "parent_path = str(Path(path).parent)\n",
    "path_cred = r\"{}/credential_AWS.json\".format(parent_path)\n",
    "\n",
    "region = 'eu-west-3'\n",
    "bucket = 'calfdata'\n",
    "s3_output = 'INPI/sql_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                       region = region)\n",
    "client= con.client_boto()\n",
    "s3 = service_s3.connect_S3(client = client,\n",
    "                      bucket = bucket, verbose = False) \n",
    "#athena = service_athena.connect_athena(client = client,\n",
    "#                      bucket = bucket) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests index a dedoublonné\n",
    "\n",
    "Pour faciliter la discrimination des doublons et éviter d'écrire d'innombrable lignes, nous avons créé une table regroupement l'ensemble des tests selon un ordre de préférence. Il y a 4 variables qui, en l'état, peuvent être utilisées pour filtrer les doublons\n",
    "\n",
    "- `status_cas`\n",
    "- `test_ligne_num_voie`\n",
    "- `test_siege`\n",
    "- `test_enseigne`\n",
    "\n",
    "L'idée dans cette partie est de mergé la table des doublons avec une table contenant tous les tests, par ordre de préférence. Chacun des index va être matché selon cette table, puis, le rank minimum va être gardé. Le rank minimum étant le quadruplet le plus contraignant selon les variables citées précédement. Par exemple, le quadruplet le plus contraignant est : CAS_1  (status_cas),True(test_list_num_voie),\tTrue(test_siege),\tTrue (test_enseigne) puis le second est  CAS_1  (status_cas),True(test_list_num_voie),\tTrue(test_siege),\tNULL (test_enseigne). Ainsi de suite. Au total, il y a 162 possibilitées. \n",
    "\n",
    "L'idée générale pour dédoublonner les lignes est de prendre le rank du test minimum, c'est a dire celui qui satisfait le plus de conditions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "DROP TABLE `regles_tests`;\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "        query=query,\n",
    "        database='inpi',\n",
    "        s3_output='INPI/sql_output'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_cas = ['CAS_1','CAS_3','CAS_4', 'CAS_5','CAS_7', 'CAS_6']\n",
    "index_id_duplicate = ['TRUE', 'FALSE']\n",
    "test_list_num_voie = ['TRUE', 'NULL', 'FALSE']\n",
    "test_siege = ['TRUE','NULL','FALSE']\n",
    "test_enseigne =  ['TRUE','NULL', 'FALSE']\n",
    "test_siren_insee_siren_inpi = ['TRUE', 'FALSE']\n",
    "test_distance_cosine = ['TRUE', 'FALSE', 'NULL']\n",
    "test_distance_levhenstein = ['TRUE', 'FALSE', 'NULL']\n",
    "test_date = ['TRUE','NULL','FALSE']\n",
    "test_status_admin = ['TRUE', 'FALSE']\n",
    "\n",
    "index = pd.MultiIndex.from_product([\n",
    "    status_cas,\n",
    "    index_id_duplicate,\n",
    "    test_list_num_voie,\n",
    "    test_siren_insee_siren_inpi,\n",
    "    test_siege,\n",
    "    test_enseigne,\n",
    "    test_distance_cosine,\n",
    "    test_distance_levhenstein,\n",
    "    test_date,\n",
    "    test_status_admin\n",
    "],\n",
    "                                   names = [\"status_cas\",\n",
    "                                            'index_id_duplicate',\n",
    "                                            \"test_list_num_voie\",\n",
    "                                            \"test_siren_insee_siren_inpi\",\n",
    "                                           'test_siege', \n",
    "                                           'test_enseigne',\n",
    "                                           'test_distance_cosine',\n",
    "                                           'test_distance_levhenstein',\n",
    "                                           'test_date',\n",
    "                                           'test_status_admin'])\n",
    "\n",
    "df_ = (pd.DataFrame(index = index)\n",
    "       .reset_index()\n",
    "       .assign(rank = lambda x: x.index + 1)\n",
    "       #.to_csv('Regle_tests.csv', index = False)\n",
    "      )\n",
    "df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_.to_csv('Regle_tests.csv', index = False)\n",
    "s3.upload_file(file_to_upload = 'Regle_tests.csv',\n",
    "            destination_in_s3 = 'TEMP_ANALYSE_SIRETISATION/REGLES_TESTS')\n",
    "\n",
    "create_table = \"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS inpi.REGLES_TESTS (\n",
    "`status_cas`                     string,\n",
    "`index_id_duplicate`                     string,\n",
    "`test_list_num_voie`                     string,\n",
    "`test_siren_insee_siren_inpi`                     string,\n",
    "`test_siege`                     string,\n",
    "`test_enseigne`                     string,\n",
    "`test_distance_cosine`                     string,\n",
    "`test_distance_levhenstein`                     string,\n",
    "`test_date`                     string,\n",
    "`test_status_admin`                     string,\n",
    "`rank`                     integer\n",
    "\n",
    "    )\n",
    "     ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
    "    WITH SERDEPROPERTIES (\n",
    "   'separatorChar' = ',',\n",
    "   'quoteChar' = '\"'\n",
    "   )\n",
    "     LOCATION 's3://calfdata/TEMP_ANALYSE_SIRETISATION/REGLES_TESTS'\n",
    "     TBLPROPERTIES ('has_encrypted_data'='false',\n",
    "              'skip.header.line.count'='1');\"\"\"\n",
    "output = s3.run_query(\n",
    "        query=create_table,\n",
    "        database='inpi',\n",
    "        s3_output='INPI/sql_output'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation tables\n",
    "\n",
    "## Steps\n",
    "\n",
    "- Filtrer les cas 5 à 7. \n",
    "- créer deux colonnes avec le pseudo-produit cartésien (table INPI vers table INSEE). Autrement dit, on ne souhaite pas comparer les mots au sein de la même liste, mais entre les listes. \n",
    "  - Table `ets_inpi_insee_cases` \n",
    "- Merge la liste des poids dans la table `list_mots_insee_inpi_word2vec_weights` \n",
    "- Calcul de la Cosine distance (dot product sur la magnitude)\n",
    "- Calcul de la Cosine distance maximum par group `index_id`\n",
    "- Recupération de la combinaison maximum par group\n",
    "- Création de la table `ets_inpi_insee_word2vec` pour analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "DROP TABLE `inpi.ets_inpi_distance_max_word2vec`;\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "        query=query,\n",
    "        database='inpi',\n",
    "        s3_output='INPI/sql_output'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "CREATE TABLE inpi.ets_inpi_distance_max_word2vec\n",
    "WITH (\n",
    "  format='PARQUET'\n",
    ") AS\n",
    "WITH dataset AS (\n",
    "  SELECT \n",
    "    row_id, \n",
    "    index_id, \n",
    "    status_cas, \n",
    "    inpi_except, \n",
    "    insee_except, \n",
    "    pct_intersection, \n",
    "    len_inpi_except, \n",
    "    len_insee_except, \n",
    "    transform(\n",
    "      sequence(\n",
    "        1, \n",
    "        CARDINALITY(insee_except)\n",
    "      ), \n",
    "      x -> insee_except\n",
    "    ), \n",
    "    ZIP(\n",
    "      inpi_except, \n",
    "      transform(\n",
    "        sequence(\n",
    "          1, \n",
    "          CARDINALITY(inpi_except)\n",
    "        ), \n",
    "        x -> insee_except\n",
    "      )\n",
    "    ) as test \n",
    "  FROM \n",
    "    inpi.ets_inpi_insee_cases \n",
    "  where \n",
    "    (\n",
    "      status_cas = 'CAS_5' \n",
    "      OR status_cas = 'CAS_6' \n",
    "      OR status_cas = 'CAS_7'\n",
    "    ) \n",
    "  -- AND index_id = 8759351\n",
    ") \n",
    "SELECT \n",
    "  * \n",
    "FROM \n",
    "  (\n",
    "    WITH distance AS (\n",
    "      SELECT \n",
    "        * \n",
    "      FROM \n",
    "        (\n",
    "          WITH list_weights_insee_inpi AS (\n",
    "            SELECT \n",
    "              row_id, \n",
    "              index_id, \n",
    "              status_cas, \n",
    "              inpi_except, \n",
    "              insee_except, \n",
    "              len_inpi_except, \n",
    "              len_insee_except, \n",
    "              unzip_inpi, \n",
    "              unzip_insee, \n",
    "              list_weights_inpi, \n",
    "              list_weights_insee \n",
    "            FROM \n",
    "              (\n",
    "                SELECT \n",
    "                  row_id, \n",
    "                  index_id, \n",
    "                  status_cas, \n",
    "                  inpi_except, \n",
    "                  insee_except, \n",
    "                  len_inpi_except, \n",
    "                  len_insee_except, \n",
    "                  unzip.field0 as unzip_inpi, \n",
    "                  unzip.field1 as insee, \n",
    "                  test \n",
    "                FROM \n",
    "                  dataset CROSS \n",
    "                  JOIN UNNEST(test) AS new (unzip)\n",
    "              ) CROSS \n",
    "              JOIN UNNEST(insee) as test (unzip_insee) \n",
    "              LEFT JOIN (\n",
    "                SELECT \n",
    "                  words, \n",
    "                  list_weights as list_weights_inpi \n",
    "                FROM \n",
    "                  machine_learning.list_mots_insee_inpi_word2vec_weights\n",
    "              ) tb_weight_inpi ON unzip_inpi = tb_weight_inpi.words \n",
    "              LEFT JOIN (\n",
    "                SELECT \n",
    "                  words, \n",
    "                  list_weights as list_weights_insee \n",
    "                FROM \n",
    "                  machine_learning.list_mots_insee_inpi_word2vec_weights\n",
    "              ) tb_weight_insee ON unzip_insee = tb_weight_insee.words \n",
    "          ) \n",
    "          SELECT \n",
    "            row_id, \n",
    "            index_id, \n",
    "            status_cas, \n",
    "            inpi_except, \n",
    "            insee_except, \n",
    "            unzip_inpi, \n",
    "            unzip_insee, \n",
    "            len_inpi_except, \n",
    "            len_insee_except, \n",
    "            REDUCE(\n",
    "              zip_with(\n",
    "                list_weights_inpi, \n",
    "                list_weights_insee, \n",
    "                (x, y) -> x * y\n",
    "              ), \n",
    "              CAST(\n",
    "                ROW(0.0) AS ROW(sum DOUBLE)\n",
    "              ), \n",
    "              (s, x) -> CAST(\n",
    "                ROW(x + s.sum) AS ROW(sum DOUBLE)\n",
    "              ), \n",
    "              s -> s.sum\n",
    "            ) / (\n",
    "              SQRT(\n",
    "                REDUCE(\n",
    "                  transform(\n",
    "                    list_weights_inpi, \n",
    "                    (x) -> POW(x, 2)\n",
    "                  ), \n",
    "                  CAST(\n",
    "                    ROW(0.0) AS ROW(sum DOUBLE)\n",
    "                  ), \n",
    "                  (s, x) -> CAST(\n",
    "                    ROW(x + s.sum) AS ROW(sum DOUBLE)\n",
    "                  ), \n",
    "                  s -> s.sum\n",
    "                )\n",
    "              ) * SQRT(\n",
    "                REDUCE(\n",
    "                  transform(\n",
    "                    list_weights_insee, \n",
    "                    (x) -> POW(x, 2)\n",
    "                  ), \n",
    "                  CAST(\n",
    "                    ROW(0.0) AS ROW(sum DOUBLE)\n",
    "                  ), \n",
    "                  (s, x) -> CAST(\n",
    "                    ROW(x + s.sum) AS ROW(sum DOUBLE)\n",
    "                  ), \n",
    "                  s -> s.sum\n",
    "                )\n",
    "              )\n",
    "            ) AS cosine_distance \n",
    "          FROM \n",
    "            list_weights_insee_inpi\n",
    "        )\n",
    "    ) \n",
    "    SELECT \n",
    "      row_id, \n",
    "      dataset.index_id, \n",
    "      inpi_except, \n",
    "      insee_except, \n",
    "      unzip_inpi, \n",
    "      unzip_insee, \n",
    "      max_cosine_distance,\n",
    "      CASE WHEN max_cosine_distance >= .6 THEN 'TRUE' ELSE 'FALSE' END AS test_distance_cosine,\n",
    "      test as key_except_to_test,\n",
    "      levenshtein_distance(unzip_inpi, unzip_insee) AS levenshtein_distance,\n",
    "      CASE WHEN levenshtein_distance(unzip_inpi, unzip_insee) <=1  THEN 'TRUE' ELSE 'FALSE' END AS test_distance_levhenstein\n",
    "    \n",
    "    FROM \n",
    "      dataset \n",
    "      LEFT JOIN (\n",
    "        SELECT \n",
    "          distance.index_id, \n",
    "          unzip_inpi, \n",
    "          unzip_insee, \n",
    "          max_cosine_distance \n",
    "        FROM \n",
    "          distance \n",
    "          RIGHT JOIN (\n",
    "            SELECT \n",
    "              index_id, \n",
    "              MAX(cosine_distance) as max_cosine_distance \n",
    "            FROM \n",
    "              distance \n",
    "            GROUP BY \n",
    "              index_id\n",
    "          ) as tb_max_distance ON distance.index_id = tb_max_distance.index_id \n",
    "          AND distance.cosine_distance = tb_max_distance.max_cosine_distance\n",
    "      ) as tb_max_distance_lookup ON dataset.index_id = tb_max_distance_lookup.index_id\n",
    "  )\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "output = s3.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output=s3_output,\n",
    "  filename = None, ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breve analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution Distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT approx_percentile(\n",
    "  max_cosine_distance, ARRAY[\n",
    "    0.25,\n",
    "    0.50,\n",
    "    0.60,\n",
    "    0.70,\n",
    "    0.75,\n",
    "    0.80,\n",
    "    0.85,\n",
    "    0.95,\n",
    "    0.99]\n",
    "  )\n",
    "  FROM ets_inpi_distance_max_word2vec \n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output=s3_output,\n",
    "  filename = 'distance_cosine', ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\" \n",
    "SELECT test_distance_costine, COUNT(*)\n",
    "FROM ets_inpi_distance_max_word2vec \n",
    "GROUP BY test_distance_costine\n",
    "\"\"\"\n",
    "\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output=s3_output,\n",
    "  filename = 'test_distance_cosine', ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT approx_percentile(\n",
    "  levenshtein_distance, ARRAY[\n",
    "    0.25,\n",
    "    0.50,\n",
    "    0.60,\n",
    "    0.70,\n",
    "    0.75,\n",
    "    0.80,\n",
    "    0.85,\n",
    "    0.95,\n",
    "    0.99]\n",
    "  )\n",
    "  FROM ets_inpi_distance_max_word2vec \n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output=s3_output,\n",
    "  filename = 'levenshtein_distance', ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\" \n",
    "SELECT test_levhenstein, COUNT(*)\n",
    "FROM ets_inpi_distance_max_word2vec \n",
    "GROUP BY test_levhenstein\n",
    "\"\"\"\n",
    "\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output=s3_output,\n",
    "  filename = 'test_levhenstein', ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create table ajout distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "DROP TABLE `inpi.ets_inpi_insee_cases_distance`;\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "        query=query,\n",
    "        database='inpi',\n",
    "        s3_output='INPI/sql_output'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "CREATE TABLE inpi.ets_inpi_insee_cases_distance\n",
    "WITH (\n",
    "  format='PARQUET'\n",
    ") AS\n",
    "WITH tb_distance AS (\n",
    "SELECT \n",
    "  ets_inpi_insee_cases.row_id, \n",
    "  ets_inpi_insee_cases.index_id, \n",
    "  sequence_id, \n",
    "  siren, \n",
    "  siret,\n",
    "  list_inpi, \n",
    "  lenght_list_inpi, \n",
    "  list_insee, \n",
    "  lenght_list_insee, \n",
    "  ets_inpi_insee_cases.inpi_except, \n",
    "  ets_inpi_insee_cases.insee_except, \n",
    "  intersection, \n",
    "  union_, \n",
    "  pct_intersection, \n",
    "  len_inpi_except, \n",
    "  len_insee_except, \n",
    "  status_cas,\n",
    "  unzip_inpi,\n",
    "  unzip_insee,\n",
    "  max_cosine_distance,\n",
    "  CASE WHEN test_distance_cosine IS NULL THEN 'NULL' ELSE test_distance_cosine END AS test_distance_cosine,\n",
    "  -- test_distance_costine,\n",
    "  levenshtein_distance,\n",
    "  CASE WHEN test_distance_levhenstein IS NULL THEN 'NULL' ELSE test_distance_levhenstein END AS test_distance_levhenstein,\n",
    "  -- test_levhenstein, \n",
    "  count_initial_insee, \n",
    "  count_inpi_siren_siret, \n",
    "  count_inpi_siren_sequence, \n",
    "  count_inpi_sequence_siret, \n",
    "  count_inpi_sequence_stat_cas_siret, \n",
    "  count_inpi_index_id_siret, \n",
    "  count_inpi_index_id_stat_cas_siret, \n",
    "  count_inpi_index_id_stat_cas, \n",
    "  index_id_duplicate, \n",
    "  test_sequence_siret, \n",
    "  test_index_siret, \n",
    "  test_siren_insee_siren_inpi, \n",
    "  test_sequence_siret_many_cas, \n",
    "  list_numero_voie_matching_inpi, \n",
    "  list_numero_voie_matching_insee, \n",
    "  intersection_numero_voie, \n",
    "  union_numero_voie, \n",
    "  test_list_num_voie, \n",
    "  datecreationetablissement, \n",
    "  date_debut_activite, \n",
    "  test_date, \n",
    "  etatadministratifetablissement, \n",
    "  status_admin, \n",
    "  test_status_admin, \n",
    "  etablissementsiege, \n",
    "  status_ets, \n",
    "  test_siege, \n",
    "  codecommuneetablissement, \n",
    "  code_commune, \n",
    "  test_code_commune, \n",
    "  codepostaletablissement, \n",
    "  code_postal_matching, \n",
    "  numerovoieetablissement, \n",
    "  numero_voie_matching, \n",
    "  test_numero_voie, \n",
    "  typevoieetablissement, \n",
    "  type_voie_matching, \n",
    "  test_type_voie, \n",
    "  test_adresse_cas_1_3_4, \n",
    "  index_id_dup_has_cas_1_3_4, \n",
    "  test_duplicates_is_in_cas_1_3_4, \n",
    "  enseigne, \n",
    "  enseigne1etablissement, \n",
    "  enseigne2etablissement, \n",
    "  enseigne3etablissement, \n",
    "  test_enseigne,\n",
    "  key_except_to_test\n",
    "FROM \n",
    "  ets_inpi_insee_cases\n",
    "LEFT JOIN\n",
    "ets_inpi_distance_max_word2vec \n",
    "ON ets_inpi_insee_cases.row_id = ets_inpi_distance_max_word2vec.row_id\n",
    ")\n",
    "SELECT \n",
    "  rank, \n",
    "  row_id, \n",
    "  index_id, \n",
    "  sequence_id, \n",
    "  siren, \n",
    "  siret,\n",
    "  list_inpi, \n",
    "  lenght_list_inpi, \n",
    "  list_insee, \n",
    "  lenght_list_insee, \n",
    "  inpi_except, \n",
    "  insee_except, \n",
    "  intersection, \n",
    "  union_, \n",
    "  pct_intersection, \n",
    "  len_inpi_except, \n",
    "  len_insee_except, \n",
    "  tb_distance.status_cas,\n",
    "  unzip_inpi,\n",
    "  unzip_insee,\n",
    "  max_cosine_distance,\n",
    "  tb_distance.test_distance_cosine,\n",
    "  -- test_distance_costine,\n",
    "  levenshtein_distance,\n",
    "  tb_distance.test_distance_levhenstein,\n",
    "  -- test_levhenstein, \n",
    "  count_initial_insee, \n",
    "  count_inpi_siren_siret, \n",
    "  count_inpi_siren_sequence, \n",
    "  count_inpi_sequence_siret, \n",
    "  count_inpi_sequence_stat_cas_siret, \n",
    "  count_inpi_index_id_siret, \n",
    "  count_inpi_index_id_stat_cas_siret, \n",
    "  count_inpi_index_id_stat_cas, \n",
    "  tb_distance.index_id_duplicate, \n",
    "  test_sequence_siret, \n",
    "  test_index_siret, \n",
    "  tb_distance.test_siren_insee_siren_inpi, \n",
    "  test_sequence_siret_many_cas, \n",
    "  list_numero_voie_matching_inpi, \n",
    "  list_numero_voie_matching_insee, \n",
    "  intersection_numero_voie, \n",
    "  union_numero_voie, \n",
    "  tb_distance.test_list_num_voie, \n",
    "  datecreationetablissement, \n",
    "  date_debut_activite, \n",
    "  tb_distance.test_date, \n",
    "  etatadministratifetablissement, \n",
    "  status_admin, \n",
    "  tb_distance.test_status_admin, \n",
    "  etablissementsiege, \n",
    "  status_ets, \n",
    "  tb_distance.test_siege, \n",
    "  codecommuneetablissement, \n",
    "  code_commune, \n",
    "  test_code_commune, \n",
    "  codepostaletablissement, \n",
    "  code_postal_matching, \n",
    "  numerovoieetablissement, \n",
    "  numero_voie_matching, \n",
    "  test_numero_voie, \n",
    "  typevoieetablissement, \n",
    "  type_voie_matching, \n",
    "  test_type_voie, \n",
    "  test_adresse_cas_1_3_4, \n",
    "  index_id_dup_has_cas_1_3_4, \n",
    "  test_duplicates_is_in_cas_1_3_4, \n",
    "  enseigne, \n",
    "  enseigne1etablissement, \n",
    "  enseigne2etablissement, \n",
    "  enseigne3etablissement, \n",
    "  tb_distance.test_enseigne,\n",
    "  key_except_to_test\n",
    "FROM tb_distance\n",
    "LEFT JOIN regles_tests \n",
    "  ON  tb_distance.status_cas = regles_tests.status_cas \n",
    "  \n",
    "  AND tb_distance.index_id_duplicate = regles_tests.index_id_duplicate \n",
    "  \n",
    "  AND tb_distance.test_list_num_voie = regles_tests.test_list_num_voie \n",
    "  AND tb_distance.test_siren_insee_siren_inpi = regles_tests.test_siren_insee_siren_inpi\n",
    "  AND tb_distance.test_siege = regles_tests.test_siege \n",
    "  AND tb_distance.test_enseigne = regles_tests.test_enseigne\n",
    "  \n",
    "  AND tb_distance.test_distance_cosine = regles_tests.test_distance_cosine \n",
    "  AND tb_distance.test_distance_levhenstein = regles_tests.test_distance_levhenstein\n",
    "  \n",
    "  AND tb_distance.test_date = regles_tests.test_date \n",
    "  AND tb_distance.test_status_admin = regles_tests.test_status_admin\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = s3.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output=s3_output,\n",
    "  filename = None, ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count nombre lignes  & index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le nombre de lignes est de:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM ets_inpi_insee_cases_distance \n",
    "\"\"\"\n",
    "\n",
    "output = s3.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output='INPI/sql_output',\n",
    "      filename = 'cnt_nb_lignes_rank', ## Add filename to print dataframe\n",
    "      destination_key = None ### Add destination key if need to copy output\n",
    "        )\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le nombre d'index est de "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT COUNT(distinct(index_id))\n",
    "FROM ets_inpi_insee_cases_distance \n",
    "\"\"\"\n",
    "\n",
    "output = s3.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output='INPI/sql_output',\n",
    "      filename = 'cnt_nb_index_rank', ## Add filename to print dataframe\n",
    "      destination_key = None ### Add destination key if need to copy output\n",
    "        )\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, shutil, urllib, ipykernel, json\n",
    "from pathlib import Path\n",
    "from notebook import notebookapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_report(extension = \"html\"):\n",
    "    \"\"\"\n",
    "    Create a report from the current notebook and save it in the \n",
    "    Report folder (Parent-> child directory)\n",
    "    \n",
    "    1. Exctract the current notbook name\n",
    "    2. Convert the Notebook \n",
    "    3. Move the newly created report\n",
    "    \n",
    "    Args:\n",
    "    extension: string. Can be \"html\", \"pdf\", \"md\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ### Get notebook name\n",
    "    connection_file = os.path.basename(ipykernel.get_connection_file())\n",
    "    kernel_id = connection_file.split('-', 1)[0].split('.')[0]\n",
    "\n",
    "    for srv in notebookapp.list_running_servers():\n",
    "        try:\n",
    "            if srv['token']=='' and not srv['password']:  \n",
    "                req = urllib.request.urlopen(srv['url']+'api/sessions')\n",
    "            else:\n",
    "                req = urllib.request.urlopen(srv['url']+ \\\n",
    "                                             'api/sessions?token=' + \\\n",
    "                                             srv['token'])\n",
    "            sessions = json.load(req)\n",
    "            notebookname = sessions[0]['name']\n",
    "        except:\n",
    "            pass  \n",
    "    \n",
    "    sep = '.'\n",
    "    path = os.getcwd()\n",
    "    #parent_path = str(Path(path).parent)\n",
    "    \n",
    "    ### Path report\n",
    "    #path_report = \"{}/Reports\".format(parent_path)\n",
    "    #path_report = \"{}/Reports\".format(path)\n",
    "    \n",
    "    ### Path destination\n",
    "    name_no_extension = notebookname.split(sep, 1)[0]\n",
    "    source_to_move = name_no_extension +'.{}'.format(extension)\n",
    "    dest = os.path.join(path,'Reports', source_to_move)\n",
    "    \n",
    "    ### Generate notebook\n",
    "    os.system('jupyter nbconvert --no-input --to {} {}'.format(\n",
    "    extension,notebookname))\n",
    "    \n",
    "    ### Move notebook to report folder\n",
    "    #time.sleep(5)\n",
    "    shutil.move(source_to_move, dest)\n",
    "    print(\"Report Available at this adress:\\n {}\".format(dest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_report(extension = \"html\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nteract": {
   "version": "0.24.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
