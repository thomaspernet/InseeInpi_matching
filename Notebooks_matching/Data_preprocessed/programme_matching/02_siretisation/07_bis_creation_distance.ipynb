{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test nombre lignes siretise\n",
    "\n",
    "Copy paste from Coda to fill the information\n",
    "\n",
    "## Objective(s)\n",
    "\n",
    "- Lors de [l’US 7: Test nombre lignes siretise avec nouvelles regles de gestion](https://coda.io/d/CreditAgricole_dCtnoqIftTn/US-07-ETS-version-3_su0VF), nous avons créé une table avec l’ensemble des possibilités de tests, trié par ordre de préférence. \n",
    "\n",
    "  - Toutefois, il manque deux variables:\n",
    "\n",
    "    - max_distance_cosine\n",
    "    - test_distance_costine:\n",
    "        - test si la distance max est supérieur a .6\n",
    "    - levhenstein_distance\n",
    "    - test_levhenstein\n",
    "        - test si l'edit distance est inférieure ou égale a 1\n",
    "\n",
    "  - Dans cette US, nous allons créer ses deux variables et les ajouter à la table ets_inpi_insee_cases. Une nouvelle table sera créé, appelée ets_inpi_insee_cases_distance . Les nouvelles variables a ajouter sont les suivantes:\n",
    "\n",
    "  - unzip_inpi, \n",
    "\n",
    "    - mot ayant servi coté inpi pour trouver la distance\n",
    "\n",
    "  -  unzip_insee, \n",
    "\n",
    "    - mot ayant servi coté inse pour trouver la distance\n",
    "\n",
    "  - max_cosine_distance, \n",
    "\n",
    "    - distance maximum de l’index\n",
    "\n",
    "  -  test as key_except_to_test\n",
    "\n",
    "    - liste contenant les clés valeurs des mots non communs\n",
    "\n",
    "  - Une table intermédiaire contenant le max de la distance sera calculé, avec la Levhenstein aussi. La table s’appelle  ets_inpi_distance_max_word2vec \n",
    "\n",
    "## Metadata \n",
    "\n",
    "- Metadata parameters are available here: [Ressources_suDYJ#_luZqd](http://Ressources_suDYJ#_luZqd)\n",
    "\n",
    "  - Task type:\n",
    "\n",
    "     - Jupyter Notebook\n",
    "\n",
    "  - Users: :\n",
    "\n",
    "      - [Thomas Pernet](mailto:t.pernetcoudrier@gmail.com)\n",
    "\n",
    "  - Watchers:\n",
    "\n",
    "      - [Thomas Pernet](mailto:t.pernetcoudrier@gmail.com)\n",
    "\n",
    "  - Estimated Log points:\n",
    "\n",
    "      - One being a simple task, 15 a very difficult one\n",
    "        -  10\n",
    "\n",
    "  - Task tag\n",
    "\n",
    "      - \\#machine-learning,#sql-query,#computation,#word2vec\n",
    "\n",
    "  - Toggl Tag\n",
    "\n",
    "      - \\#variable-computation\n",
    "  \n",
    "## Input Cloud Storage [AWS/GCP]\n",
    "\n",
    "If link from the internet, save it to the cloud first\n",
    "\n",
    "### Tables [AWS/BigQuery]\n",
    "\n",
    "- Batch 1:\n",
    "\n",
    "  - Select Provider: Athena\n",
    "\n",
    "    - Select table(s): ets_inpi_insee_cases\n",
    "\n",
    "    - Select only tables created from the same notebook, else copy/paste selection to add new input tables\n",
    "\n",
    "      - If table(s) does not exist, add them: \n",
    "\n",
    "        Add New Table\n",
    "\n",
    "      - Information:\n",
    "\n",
    "      - Region: \n",
    "\n",
    "        - NameEurope (Paris)\n",
    "          - Code: eu-west-3\n",
    "\n",
    "        - Database: inpi\n",
    "\n",
    "        - Notebook construction file: [07_pourcentage_siretisation_v3](https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/02_siretisation/07_pourcentage_siretisation_v3.md)\n",
    "    \n",
    "## Destination Output/Delivery\n",
    "\n",
    "- AWS\n",
    "\n",
    "    - Athena: \n",
    "\n",
    "      - Region: Europe (Paris)\n",
    "        - Database: inpi\n",
    "        - Tables (Add name new table): ets_inpi_distance_max_word2vec,ets_inpi_insee_cases_distance\n",
    "        - List new tables\n",
    "        - ets_inpi_distance_max_word2vec, ets_inpi_insee_cases_distance\n",
    "\n",
    "## Things to know (Steps, Attention points or new flow of information)\n",
    "\n",
    "### Sources of information  (meeting notes, Documentation, Query, URL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connexion serveur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsPy.aws_authorization import aws_connector\n",
    "from awsPy.aws_athena import service_athena\n",
    "from awsPy.aws_s3 import service_s3\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, shutil\n",
    "bucket = 'calfdata'\n",
    "path = os.getcwd()\n",
    "parent_path = str(Path(path).parent)\n",
    "path_cred = r\"{}/credential_AWS.json\".format(parent_path)\n",
    "\n",
    "region = 'eu-west-3'\n",
    "bucket = 'calfdata'\n",
    "s3_output = 'INPI/sql_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                       region = region)\n",
    "client= con.client_boto()\n",
    "s3 = service_s3.connect_S3(client = client,\n",
    "                      bucket = bucket, verbose = False) \n",
    "#athena = service_athena.connect_athena(client = client,\n",
    "#                      bucket = bucket) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation tables\n",
    "\n",
    "## Steps\n",
    "\n",
    "- Filtrer les cas 5 à 7. \n",
    "- créer deux colonnes avec le pseudo-produit cartésien (table INPI vers table INSEE). Autrement dit, on ne souhaite pas comparer les mots au sein de la même liste, mais entre les listes. \n",
    "  - Table `ets_inpi_insee_cases` \n",
    "- Merge la liste des poids dans la table `list_mots_insee_inpi_word2vec_weights` \n",
    "- Calcul de la Cosine distance (dot product sur la magnitude)\n",
    "- Calcul de la Cosine distance maximum par group `index_id`\n",
    "- Recupération de la combinaison maximum par group\n",
    "- Création de la table `ets_inpi_insee_word2vec` pour analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "CREATE TABLE inpi.ets_inpi_distance_max_word2vec\n",
    "WITH (\n",
    "  format='PARQUET'\n",
    ") AS\n",
    "WITH dataset AS (\n",
    "  SELECT \n",
    "    row_id, \n",
    "    index_id, \n",
    "    status_cas, \n",
    "    inpi_except, \n",
    "    insee_except, \n",
    "    pct_intersection, \n",
    "    len_inpi_except, \n",
    "    len_insee_except, \n",
    "    transform(\n",
    "      sequence(\n",
    "        1, \n",
    "        CARDINALITY(insee_except)\n",
    "      ), \n",
    "      x -> insee_except\n",
    "    ), \n",
    "    ZIP(\n",
    "      inpi_except, \n",
    "      transform(\n",
    "        sequence(\n",
    "          1, \n",
    "          CARDINALITY(inpi_except)\n",
    "        ), \n",
    "        x -> insee_except\n",
    "      )\n",
    "    ) as test \n",
    "  FROM \n",
    "    inpi.ets_inpi_insee_cases \n",
    "  where \n",
    "    (\n",
    "      status_cas = 'CAS_5' \n",
    "      OR status_cas = 'CAS_6' \n",
    "      OR status_cas = 'CAS_7'\n",
    "    ) \n",
    "  -- AND index_id = 8759351\n",
    ") \n",
    "SELECT \n",
    "  * \n",
    "FROM \n",
    "  (\n",
    "    WITH distance AS (\n",
    "      SELECT \n",
    "        * \n",
    "      FROM \n",
    "        (\n",
    "          WITH list_weights_insee_inpi AS (\n",
    "            SELECT \n",
    "              row_id, \n",
    "              index_id, \n",
    "              status_cas, \n",
    "              inpi_except, \n",
    "              insee_except, \n",
    "              len_inpi_except, \n",
    "              len_insee_except, \n",
    "              unzip_inpi, \n",
    "              unzip_insee, \n",
    "              list_weights_inpi, \n",
    "              list_weights_insee \n",
    "            FROM \n",
    "              (\n",
    "                SELECT \n",
    "                  row_id, \n",
    "                  index_id, \n",
    "                  status_cas, \n",
    "                  inpi_except, \n",
    "                  insee_except, \n",
    "                  len_inpi_except, \n",
    "                  len_insee_except, \n",
    "                  unzip.field0 as unzip_inpi, \n",
    "                  unzip.field1 as insee, \n",
    "                  test \n",
    "                FROM \n",
    "                  dataset CROSS \n",
    "                  JOIN UNNEST(test) AS new (unzip)\n",
    "              ) CROSS \n",
    "              JOIN UNNEST(insee) as test (unzip_insee) \n",
    "              LEFT JOIN (\n",
    "                SELECT \n",
    "                  words, \n",
    "                  list_weights as list_weights_inpi \n",
    "                FROM \n",
    "                  machine_learning.list_mots_insee_inpi_word2vec_weights\n",
    "              ) tb_weight_inpi ON unzip_inpi = tb_weight_inpi.words \n",
    "              LEFT JOIN (\n",
    "                SELECT \n",
    "                  words, \n",
    "                  list_weights as list_weights_insee \n",
    "                FROM \n",
    "                  machine_learning.list_mots_insee_inpi_word2vec_weights\n",
    "              ) tb_weight_insee ON unzip_insee = tb_weight_insee.words \n",
    "          ) \n",
    "          SELECT \n",
    "            row_id, \n",
    "            index_id, \n",
    "            status_cas, \n",
    "            inpi_except, \n",
    "            insee_except, \n",
    "            unzip_inpi, \n",
    "            unzip_insee, \n",
    "            len_inpi_except, \n",
    "            len_insee_except, \n",
    "            REDUCE(\n",
    "              zip_with(\n",
    "                list_weights_inpi, \n",
    "                list_weights_insee, \n",
    "                (x, y) -> x * y\n",
    "              ), \n",
    "              CAST(\n",
    "                ROW(0.0) AS ROW(sum DOUBLE)\n",
    "              ), \n",
    "              (s, x) -> CAST(\n",
    "                ROW(x + s.sum) AS ROW(sum DOUBLE)\n",
    "              ), \n",
    "              s -> s.sum\n",
    "            ) / (\n",
    "              SQRT(\n",
    "                REDUCE(\n",
    "                  transform(\n",
    "                    list_weights_inpi, \n",
    "                    (x) -> POW(x, 2)\n",
    "                  ), \n",
    "                  CAST(\n",
    "                    ROW(0.0) AS ROW(sum DOUBLE)\n",
    "                  ), \n",
    "                  (s, x) -> CAST(\n",
    "                    ROW(x + s.sum) AS ROW(sum DOUBLE)\n",
    "                  ), \n",
    "                  s -> s.sum\n",
    "                )\n",
    "              ) * SQRT(\n",
    "                REDUCE(\n",
    "                  transform(\n",
    "                    list_weights_insee, \n",
    "                    (x) -> POW(x, 2)\n",
    "                  ), \n",
    "                  CAST(\n",
    "                    ROW(0.0) AS ROW(sum DOUBLE)\n",
    "                  ), \n",
    "                  (s, x) -> CAST(\n",
    "                    ROW(x + s.sum) AS ROW(sum DOUBLE)\n",
    "                  ), \n",
    "                  s -> s.sum\n",
    "                )\n",
    "              )\n",
    "            ) AS cosine_distance \n",
    "          FROM \n",
    "            list_weights_insee_inpi\n",
    "        )\n",
    "    ) \n",
    "    SELECT \n",
    "      row_id, \n",
    "      dataset.index_id, \n",
    "      inpi_except, \n",
    "      insee_except, \n",
    "      unzip_inpi, \n",
    "      unzip_insee, \n",
    "      max_cosine_distance,\n",
    "      CASE WHEN max_cosine_distance >= .6 THEN 'True' ELSE 'False' END AS test_distance_costine,\n",
    "      test as key_except_to_test,\n",
    "      levenshtein_distance(unzip_inpi, unzip_insee) AS levenshtein_distance,\n",
    "      CASE WHEN levenshtein_distance(unzip_inpi, unzip_insee) <=1  THEN 'True' ELSE 'False' END AS test_levhenstein\n",
    "    \n",
    "    FROM \n",
    "      dataset \n",
    "      LEFT JOIN (\n",
    "        SELECT \n",
    "          distance.index_id, \n",
    "          unzip_inpi, \n",
    "          unzip_insee, \n",
    "          max_cosine_distance \n",
    "        FROM \n",
    "          distance \n",
    "          RIGHT JOIN (\n",
    "            SELECT \n",
    "              index_id, \n",
    "              MAX(cosine_distance) as max_cosine_distance \n",
    "            FROM \n",
    "              distance \n",
    "            GROUP BY \n",
    "              index_id\n",
    "          ) as tb_max_distance ON distance.index_id = tb_max_distance.index_id \n",
    "          AND distance.cosine_distance = tb_max_distance.max_cosine_distance\n",
    "      ) as tb_max_distance_lookup ON dataset.index_id = tb_max_distance_lookup.index_id\n",
    "  )\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "output = s3.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output=s3_output,\n",
    "  filename = None, ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breve analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution Distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT approx_percentile(\n",
    "  max_cosine_distance, ARRAY[\n",
    "    0.25,\n",
    "    0.50,\n",
    "    0.60,\n",
    "    0.70,\n",
    "    0.75,\n",
    "    0.80,\n",
    "    0.85,\n",
    "    0.95,\n",
    "    0.99]\n",
    "  )\n",
    "  FROM ets_inpi_distance_max_word2vec \n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output=s3_output,\n",
    "  filename = 'distance_cosine', ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\" \n",
    "SELECT test_distance_costine, COUNT(*)\n",
    "FROM ets_inpi_distance_max_word2vec \n",
    "GROUP BY test_distance_costine\n",
    "\"\"\"\n",
    "\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output=s3_output,\n",
    "  filename = 'test_distance_cosine', ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT approx_percentile(\n",
    "  levenshtein_distance, ARRAY[\n",
    "    0.25,\n",
    "    0.50,\n",
    "    0.60,\n",
    "    0.70,\n",
    "    0.75,\n",
    "    0.80,\n",
    "    0.85,\n",
    "    0.95,\n",
    "    0.99]\n",
    "  )\n",
    "  FROM ets_inpi_distance_max_word2vec \n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output=s3_output,\n",
    "  filename = 'levenshtein_distance', ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\" \n",
    "SELECT test_levhenstein, COUNT(*)\n",
    "FROM ets_inpi_distance_max_word2vec \n",
    "GROUP BY test_levhenstein\n",
    "\"\"\"\n",
    "\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output=s3_output,\n",
    "  filename = 'test_levhenstein', ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create table ajout distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "CREATE TABLE inpi.ets_inpi_insee_cases_distance\n",
    "WITH (\n",
    "  format='PARQUET'\n",
    ") AS\n",
    "SELECT \n",
    "  rank, \n",
    "  ets_inpi_insee_cases.row_id, \n",
    "  ets_inpi_insee_cases.index_id, \n",
    "  sequence_id, \n",
    "  siren, \n",
    "  siret,\n",
    "  list_inpi, \n",
    "  lenght_list_inpi, \n",
    "  list_insee, \n",
    "  lenght_list_insee, \n",
    "  ets_inpi_insee_cases.inpi_except, \n",
    "  ets_inpi_insee_cases.insee_except, \n",
    "  intersection, \n",
    "  union_, \n",
    "  pct_intersection, \n",
    "  len_inpi_except, \n",
    "  len_insee_except, \n",
    "  status_cas,\n",
    "  unzip_inpi,\n",
    "  unzip_insee,\n",
    "  max_cosine_distance,\n",
    "  test_distance_costine,\n",
    "  levenshtein_distance,\n",
    "  test_levhenstein, \n",
    "  count_initial_insee, \n",
    "  count_inpi_siren_siret, \n",
    "  count_inpi_siren_sequence, \n",
    "  count_inpi_sequence_siret, \n",
    "  count_inpi_sequence_stat_cas_siret, \n",
    "  count_inpi_index_id_siret, \n",
    "  count_inpi_index_id_stat_cas_siret, \n",
    "  count_inpi_index_id_stat_cas, \n",
    "  index_id_duplicate, \n",
    "  test_sequence_siret, \n",
    "  test_index_siret, \n",
    "  test_siren_insee_siren_inpi, \n",
    "  test_sequence_siret_many_cas, \n",
    "  list_numero_voie_matching_inpi, \n",
    "  list_numero_voie_matching_insee, \n",
    "  intersection_numero_voie, \n",
    "  union_numero_voie, \n",
    "  test_list_num_voie, \n",
    "  datecreationetablissement, \n",
    "  date_debut_activite, \n",
    "  test_date, \n",
    "  etatadministratifetablissement, \n",
    "  status_admin, \n",
    "  test_status_admin, \n",
    "  etablissementsiege, \n",
    "  status_ets, \n",
    "  test_siege, \n",
    "  codecommuneetablissement, \n",
    "  code_commune, \n",
    "  test_code_commune, \n",
    "  codepostaletablissement, \n",
    "  code_postal_matching, \n",
    "  numerovoieetablissement, \n",
    "  numero_voie_matching, \n",
    "  test_numero_voie, \n",
    "  typevoieetablissement, \n",
    "  type_voie_matching, \n",
    "  test_type_voie, \n",
    "  test_adresse_cas_1_3_4, \n",
    "  index_id_dup_has_cas_1_3_4, \n",
    "  test_duplicates_is_in_cas_1_3_4, \n",
    "  enseigne, \n",
    "  enseigne1etablissement, \n",
    "  enseigne2etablissement, \n",
    "  enseigne3etablissement, \n",
    "  test_enseigne,\n",
    "  key_except_to_test\n",
    "FROM \n",
    "  ets_inpi_insee_cases\n",
    "LEFT JOIN\n",
    "ets_inpi_distance_max_word2vec \n",
    "ON ets_inpi_insee_cases.row_id = ets_inpi_distance_max_word2vec.row_id\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = s3.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output=s3_output,\n",
    "  filename = None, ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, shutil, urllib, ipykernel, json\n",
    "from pathlib import Path\n",
    "from notebook import notebookapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_report(extension = \"html\"):\n",
    "    \"\"\"\n",
    "    Create a report from the current notebook and save it in the \n",
    "    Report folder (Parent-> child directory)\n",
    "    \n",
    "    1. Exctract the current notbook name\n",
    "    2. Convert the Notebook \n",
    "    3. Move the newly created report\n",
    "    \n",
    "    Args:\n",
    "    extension: string. Can be \"html\", \"pdf\", \"md\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ### Get notebook name\n",
    "    connection_file = os.path.basename(ipykernel.get_connection_file())\n",
    "    kernel_id = connection_file.split('-', 1)[0].split('.')[0]\n",
    "\n",
    "    for srv in notebookapp.list_running_servers():\n",
    "        try:\n",
    "            if srv['token']=='' and not srv['password']:  \n",
    "                req = urllib.request.urlopen(srv['url']+'api/sessions')\n",
    "            else:\n",
    "                req = urllib.request.urlopen(srv['url']+ \\\n",
    "                                             'api/sessions?token=' + \\\n",
    "                                             srv['token'])\n",
    "            sessions = json.load(req)\n",
    "            notebookname = sessions[0]['name']\n",
    "        except:\n",
    "            pass  \n",
    "    \n",
    "    sep = '.'\n",
    "    path = os.getcwd()\n",
    "    #parent_path = str(Path(path).parent)\n",
    "    \n",
    "    ### Path report\n",
    "    #path_report = \"{}/Reports\".format(parent_path)\n",
    "    #path_report = \"{}/Reports\".format(path)\n",
    "    \n",
    "    ### Path destination\n",
    "    name_no_extension = notebookname.split(sep, 1)[0]\n",
    "    source_to_move = name_no_extension +'.{}'.format(extension)\n",
    "    dest = os.path.join(path,'Reports', source_to_move)\n",
    "    \n",
    "    ### Generate notebook\n",
    "    os.system('jupyter nbconvert --no-input --to {} {}'.format(\n",
    "    extension,notebookname))\n",
    "    \n",
    "    ### Move notebook to report folder\n",
    "    #time.sleep(5)\n",
    "    shutil.move(source_to_move, dest)\n",
    "    print(\"Report Available at this adress:\\n {}\".format(dest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_report(extension = \"html\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nteract": {
   "version": "0.24.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
