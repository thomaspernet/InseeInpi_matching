{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siretisation: Version 2 \n",
    "\n",
    "## Algorithme\n",
    "\n",
    "### origin\n",
    "\n",
    "Comme la taille de la donnée est trop élevée, il faut prendre un sous échantillon pour faire la siretisation. Le sous échantillonage se fait avec l'origine. \n",
    "\n",
    "Input:\n",
    "- INSEE:\n",
    "    - Athena: `insee_final_sql` \n",
    "- INPI:\n",
    "    - Athena: `ets_final_sql` \n",
    "\n",
    "Output:\n",
    "\n",
    "- ets_siretise\n",
    "    * Table siretisée: [INPI/TC_1/04_table_siretisee](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/TC_1/04_table_siretisee/?region=eu-west-3) \n",
    "\n",
    "- ets_non_siretise\n",
    "    * Table non siretisée: [INPI/TC_1/05_table_non_siretisee](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/TC_1/05_table_non_siretisee/?region=eu-west-3)\n",
    "\n",
    "- ets_rules\n",
    "    * Table règles: [INPI/TC_1/06_table_regles/ETS](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/TC_1/06_table_regles/ETS/?region=eu-west-3&tab=overview)\n",
    "\n",
    "- logs ETS: [INPI/TC_1/04_table_siretisee/ETS_logs](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/TC_1/04_table_siretisee/ETS_logs/?region=eu-west-3&tab=overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os, re\n",
    "#os.chdir('../')\n",
    "#from inpi_insee import siretisation\n",
    "import json, os, re\n",
    "from dask.diagnostics import ProgressBar\n",
    "from dask.multiprocessing import get\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "pbar = ProgressBar()\n",
    "pbar.register()\n",
    "current_dir = os.getcwd()\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "pd.set_option('display.max_columns', None)\n",
    "#param = {\n",
    "    #'insee': 'data/input/INSEE/InitialPartielEVTNEW/insee_1557220_InitialPartielEVTNEW.csv' ### PP\n",
    "#    'insee': 'data/input/INSEE/InitialPartielEVTNEW/insee_9368683_InitialPartielEVTNEW.csv'  ### ETS\n",
    "    #'insee': 'data/input/INSEE/NEW/insee_1745311_NEW.csv' ### ETS\n",
    "#}\n",
    "# 4824158 SIREN a trouver!\n",
    "#al_siret = siretisation.siretisation_inpi(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from itertools import compress, product\n",
    "\n",
    "#def combinations(items):\n",
    "#    return ( set(compress(items,mask)) for \n",
    "#            mask in product(*[[0,1]]*len(items)))\n",
    "\n",
    "#all_list = ['ncc',\n",
    "#             'Code_Postal','Code_Commune',\n",
    "#             'INSEE','digit_inpi']\n",
    "#test = list(combinations(items = all_list))[1:]\n",
    "#sort_list = sorted(test[1:], key=lambda k: len(k), reverse=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_inpi = ['ville_matching','code_postal_matching','code_commune','voie_matching','numero_voie_matching',\n",
    "             'date_début_activité', 'status_admin', 'status_ets']\n",
    "\n",
    "list_insee = ['ville_matching',\n",
    "            'codepostaletablissement', 'codecommuneetablissement',\n",
    "            'typevoieetablissement','numerovoieetablissement',\n",
    "             'datecreationetablissement', 'etatadministratifetablissement', 'etablissementsiege']\n",
    "\n",
    "sort_list = [\n",
    " {'ville_matching', 'code_postal_matching', 'code_commune', 'voie_matching', 'numero_voie_matching',\n",
    "  'date_début_activité', 'status_admin', 'status_ets'},\n",
    "    \n",
    " {'ville_matching', 'code_postal_matching', 'code_commune', 'voie_matching',\n",
    "  'date_début_activité', 'status_admin', 'status_ets'},\n",
    "    \n",
    " {'ville_matching', 'code_postal_matching', 'code_commune', 'numero_voie_matching',\n",
    " 'date_début_activité', 'status_admin', 'status_ets'},\n",
    "    \n",
    " {'ville_matching', 'code_postal_matching', 'code_commune','date_début_activité', 'status_admin', 'status_ets'},   \n",
    " {'ville_matching', 'code_postal_matching','date_début_activité', 'status_admin', 'status_ets'},\n",
    "    \n",
    " {'ville_matching', 'date_début_activité', 'status_admin', 'status_ets'},\n",
    "    \n",
    " {'code_postal_matching', 'date_début_activité', 'status_admin', 'status_ets'},\n",
    "    \n",
    " {'code_commune', 'date_début_activité', 'status_admin', 'status_ets'},\n",
    "]\n",
    "len(sort_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_possibilities = []\n",
    "for i in sort_list:\n",
    "    left =[]\n",
    "    right = []\n",
    "    for j in i:\n",
    "        left.append(j)\n",
    "        right.append(list_insee[list_inpi.index(j)])\n",
    "    left.insert(0,'siren')\n",
    "    right.insert(0,'siren')\n",
    "    \n",
    "    dic_ = {\n",
    "    'match':{\n",
    "        'inpi':left,\n",
    "        'insee':right,\n",
    "    }\n",
    "}\n",
    "    list_possibilities.append(dic_)\n",
    "list_possibilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indiquer le fichiers a siretiser. Si pas en local, le télécharger depuis le S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsPy.aws_authorization import aws_connector\n",
    "from awsPy.aws_athena import service_athena\n",
    "from awsPy.aws_s3 import service_s3\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os, shutil\n",
    "bucket = 'calfdata'\n",
    "path = os.getcwd()\n",
    "parent_path = str(Path(path).parent)\n",
    "path_cred = r\"{}/credential_AWS.json\".format(parent_path)\n",
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                       region = 'eu-west-3')\n",
    "client= con.client_boto()\n",
    "s3 = service_s3.connect_S3(client = client,\n",
    "                      bucket = 'calfdata') \n",
    "athena = service_athena.connect_athena(client = client,\n",
    "                      bucket = 'calfdata') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recuperation ETS INPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_a_cree = False\n",
    "if csv_cree:\n",
    "    query = \"\"\"\n",
    "  SELECT \n",
    "  index_id, \n",
    "  siren, \n",
    "  code_greffe, \n",
    "  nom_greffe, \n",
    "  numero_gestion, \n",
    "  id_etablissement, \n",
    "  status, \n",
    "  origin, \n",
    "  date_greffe, \n",
    "  file_timestamp, \n",
    "  libelle_evt, \n",
    "  last_libele_evt, \n",
    "  status_admin, \n",
    "  type, \n",
    "  status_ets, \n",
    "  adress_reconstituee_inpi, \n",
    "  adress_regex_inpi, \n",
    "  adress_distance_inpi, \n",
    "  numero_voie_matching, \n",
    "  voie_clean, \n",
    "  voie_matching, \n",
    "  code_postal_matching, \n",
    "  ville_matching , \n",
    "  code_commune, \n",
    "  enseigne, \n",
    "  \"date_début_activité\", \n",
    "  csv_source \n",
    "FROM \n",
    "  ets_final_sql \n",
    "WHERE \n",
    "  status != 'IGNORE'\n",
    "\"\"\"\n",
    "    \n",
    "    output = athena.run_query(\n",
    "        query=query,\n",
    "        database='inpi',\n",
    "        s3_output='INPI/sql_output'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if csv_cree:\n",
    "    table = 'ets_final_sql'\n",
    "    source_key = \"{}/{}.csv\".format(\n",
    "                            'INPI/sql_output',\n",
    "                            output['QueryExecutionId']\n",
    "                                   )\n",
    "\n",
    "    destination_key = \"{}/{}.csv\".format(\n",
    "                            'INPI/TC_1/02_preparation_donnee/ETS_SQL',\n",
    "                            table\n",
    "                        )\n",
    "    results = s3.copy_object_s3(\n",
    "                            source_key = source_key,\n",
    "                            destination_key = destination_key,\n",
    "                            remove = False\n",
    "                        )\n",
    "    \n",
    "    s3.download_file(\n",
    "    key= destination_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'ets_final_sql'\n",
    "shutil.move(\"{}.csv\".format(filename),\n",
    "            \"data/input/INPI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recuperation ETS INSEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_cree = True\n",
    "if csv_cree:\n",
    "    query = \"\"\"SELECT \n",
    "  count_initial_insee, \n",
    "  insee_final_sql.siren, \n",
    "  siret, \n",
    "  datecreationetablissement, \n",
    "  etablissementsiege, \n",
    "  etatadministratifetablissement, \n",
    "  codepostaletablissement, \n",
    "  codecommuneetablissement, \n",
    "  libellecommuneetablissement, \n",
    "  ville_matching, \n",
    "  libellevoieetablissement, \n",
    "  complementadresseetablissement, \n",
    "  numerovoieetablissement, \n",
    "  indicerepetitionetablissement, \n",
    "  typevoieetablissement, \n",
    "  adress_reconstituee_insee, \n",
    "  enseigne1etablissement, \n",
    "  enseigne2etablissement, \n",
    "  enseigne3etablissement\n",
    "FROM \n",
    "  insee_final_sql \n",
    "  INNER JOIN (\n",
    "    select \n",
    "      distinct(siren) \n",
    "    FROM \n",
    "      ets_final_sql\n",
    "  ) as ets ON insee_final_sql.siren = ets.siren\n",
    "\"\"\"\n",
    "    output = athena.run_query(\n",
    "        query=query,\n",
    "        database='inpi',\n",
    "        s3_output='INPI/sql_output'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if csv_cree:\n",
    "    table = 'insee_final_sql'\n",
    "    source_key = \"{}/{}.csv\".format(\n",
    "                            'INPI/sql_output',\n",
    "                            output['QueryExecutionId']\n",
    "                                   )\n",
    "\n",
    "    destination_key = \"{}/{}.csv\".format(\n",
    "                            'INPI/TC_1/02_preparation_donnee/INSEE_SQL',\n",
    "                            table\n",
    "                        )\n",
    "    results = s3.copy_object_s3(\n",
    "                            source_key = source_key,\n",
    "                            destination_key = destination_key,\n",
    "                            remove = False\n",
    "                        )\n",
    "    \n",
    "    s3.download_file(\n",
    "    key= destination_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.move(\"insee_final_sql.csv\",\n",
    "            \"../data/input/INSEE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut prendre l'`origin` et `filename` que l'on souhaite sitetiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "files = glob.glob('data/logs/InitialPartielEVTNEW/*')\n",
    "for f in files:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##origin = \"InitialPartielEVTNEW\"\n",
    "##filename = \"ets_preparation_python_lib1\" ####ETS\n",
    "#filename = \"inpi_initial_partiel_evt_new_ets_status_final_InitialPartielEVTNEW\"\n",
    "#origin = \"NEW\"\n",
    "#filename = \"inpi_initial_partiel_evt_new_ets_status_final_NEW\"\n",
    "#### make dir\n",
    "#parent_dir = 'data/output/'\n",
    "#parent_dir_1 = 'data/input/INPI/special_treatment/'\n",
    "#parent_dir_2 = 'data/logs/'\n",
    "\n",
    "#for d in [parent_dir,parent_dir_1,parent_dir_2]:\n",
    "#    path = os.path.join(d, origin) \n",
    "#    try:\n",
    "#        os.mkdir(path) \n",
    "#    except:\n",
    "#        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parametres et fonctions\n",
    "\n",
    "- `split_duplication`: Split un dataframe si l'index (la variable, pas l'index) contient des doublons\n",
    "- `find_regex`: Performe une recherche regex entre deux colonnes\n",
    "- `jackard_distance`: Calcul l'indice de dissimilarité entre deux colonnes\n",
    "- `edit_distance`: Calcul le nombre de modification a faire pour obtenir la même séquence\n",
    "- `import_dask`: Charge csv en Dask DataFrame pour clusteriser les calculs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import sidetable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_dask(file, usecols = None, dtype=None, parse_dates = False):\n",
    "        \"\"\"\n",
    "        Import un fichier gzip ou csv en format Dask\n",
    "\n",
    "        Deja dans preparation data\n",
    "\n",
    "        Args:Merge\n",
    "        - file: String, Path pour localiser le fichier, incluant le nom et\n",
    "        l'extension\n",
    "        - usecols: List: les noms des colonnes a importer. Par defaut, None\n",
    "        - dtype: Dictionary: La clé indique le nom de la variable, la valeur\n",
    "        indique le type de la variable\n",
    "        - parse_dates: bool or list of int or names or list of lists or dict,\n",
    "         default False\n",
    "        \"\"\"\n",
    "        #extension = os.path.splitext(file)[1]\n",
    "        if usecols == None:\n",
    "            low_memory = False\n",
    "        else:\n",
    "            low_memory = True\n",
    "        dd_df = dd.read_csv(file, usecols = usecols, dtype = dtype,\n",
    "        blocksize=None, low_memory = low_memory,parse_dates = parse_dates)\n",
    "\n",
    "        return dd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_duplication(df):\n",
    "        \"\"\"\n",
    "        Split un dataframe si l'index (la variable, pas l'index) contient des\n",
    "        doublons.\n",
    "\n",
    "        L'idée est de distinguer les doublons resultants du merge avec l'INSEE\n",
    "\n",
    "        Args:\n",
    "        - df: Pandas dataframe contenant au moins une variable \"index\"\n",
    "\n",
    "        Returns:\n",
    "        - Un Dictionary avec:\n",
    "            - not_duplication: Dataframe ne contenant pas les doublons\n",
    "            - duplication: Dataframe contenant les doublons\n",
    "            - report_dup: Une Serie avec le nombres de doublons\n",
    "        \"\"\"\n",
    "        if 'count_duplicates_' in df.columns:\n",
    "            df = df.drop(columns = 'count_duplicates_')\n",
    "\n",
    "        df = df.merge(\n",
    "            (df\n",
    "                .groupby('index_id')['index_id']\n",
    "                .count()\n",
    "                .rename('count_duplicates_')\n",
    "                .reset_index()\n",
    "                )\n",
    "                )\n",
    "        \n",
    "        dic_ = {\n",
    "            'not_duplication':df[df['count_duplicates_'].isin([1])],\n",
    "            'duplication' : df[~df['count_duplicates_'].isin([1])],\n",
    "            'report_dup':df[\n",
    "            ~df['count_duplicates_'].isin([1])\n",
    "            ]['count_duplicates_'].value_counts()\n",
    "            }\n",
    "\n",
    "        return dic_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_regex(regex, test_str):\n",
    "        \"\"\"\n",
    "        Performe une recherche regex entre deux colonnes.\n",
    "\n",
    "        Args:\n",
    "        - regex: string: charactère contenant la formule regex\n",
    "        - test_str: String: String contenant le regex a trouver\n",
    "\n",
    "        Return:\n",
    "        Boolean, True si le regex est trouvé, sinon False\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            matches = re.search(regex, test_str)\n",
    "            if matches:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        except:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jackard_distance(inpi, insee):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    try:\n",
    "        w1 = set(inpi)\n",
    "        w2 = set(insee)\n",
    "        return nltk.jaccard_distance(w1, w2)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_distance(inpi, insee):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        return nltk.edit_distance(inpi, insee)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpi_col = [\"index_id\",\n",
    "            \"siren\",\n",
    "            \"code_greffe\",\n",
    "            \"nom_greffe\",\n",
    "            \"numero_gestion\",\n",
    "            \"id_etablissement\",\n",
    "            \"status\",\n",
    "            \"origin\",\n",
    "            \"date_greffe\",\n",
    "            \"file_timestamp\",\n",
    "            \"libelle_evt\",\n",
    "            \"last_libele_evt\", \n",
    "            \"status_admin\",\n",
    "            \"type\",\n",
    "            \"status_ets\",\n",
    "            \"adress_reconstituee_inpi\",\n",
    "            \"adress_regex_inpi\",\n",
    "            \"adress_distance_inpi\",\n",
    "            \"numero_voie_matching\",\n",
    "            \"voie_clean\",\n",
    "            \"voie_matching\",\n",
    "            \"code_postal_matching\",\n",
    "            \"ville_matching\",\n",
    "            \"code_commune\",\n",
    "            \"enseigne\",\n",
    "            \"date_début_activité\",\n",
    "            \"csv_source\"\n",
    "            ]\n",
    "\n",
    "#['count_initial_inpi', 'list_digit_inpi', 'pays', 'len_digit_address_inpi', 'nom_commercial', 'ville']\n",
    "\n",
    "inpi_dtype = {\n",
    "    \"index_id\":\"object\",\n",
    "\"siren\":\"object\",\n",
    "\"code_greffe\":\"object\",\n",
    "\"nom_greffe\":\"object\",\n",
    "\"numero_gestion\":\"object\",\n",
    "\"id_etablissement\":\"object\",\n",
    "\"status\":\"object\",\n",
    "\"origin\":\"object\",\n",
    "\"date_greffe\":\"object\",\n",
    "\"file_timestamp\":\"object\",\n",
    "\"libelle_evt\":\"object\",\n",
    "\"last_libele_evt\":\"object\", \n",
    "\"status_admin\":\"object\",\n",
    "\"type\":\"object\",\n",
    "\"status_ets\":\"object\",\n",
    "\"adress_reconstituee_inpi\":\"object\",\n",
    "\"adress_regex_inpi\":\"object\",\n",
    "\"adress_distance_inpi\":\"object\",\n",
    "\"numero_voie_matching\":\"object\",\n",
    "\"voie_clean\":\"object\",\n",
    "\"voie_matching\":\"object\",\n",
    "\"code_postal_matching\":\"object\",\n",
    "\"ville_matching\":\"object\",\n",
    "\"code_commune\":\"object\",\n",
    "\"enseigne\":\"object\",\n",
    "\"date_début_activité\":\"object\",\n",
    "\"csv_source\":\"object\"\n",
    "}\n",
    "\n",
    "insee_col = [\n",
    "    \"count_initial_insee\",\n",
    "    \"siren\",\n",
    "    \"siret\",\n",
    "    \"datecreationetablissement\",\n",
    "    \"etablissementsiege\",\n",
    "    \"etatadministratifetablissement\",\n",
    "    \"codepostaletablissement\",\n",
    "    \"codecommuneetablissement\",\n",
    "    \"libellecommuneetablissement\",\n",
    "    \"ville_matching\",\n",
    "    \"libellevoieetablissement\",\n",
    "    \"complementadresseetablissement\",\n",
    "    \"numerovoieetablissement\",\n",
    "    \"indicerepetitionetablissement\",\n",
    "    \"typevoieetablissement\",\n",
    "    \"adress_reconstituee_insee\",\n",
    "    \"enseigne1etablissement\",\n",
    "    \"enseigne2etablissement\",\n",
    "    \"enseigne3etablissement\" \n",
    "    \n",
    "]\n",
    "\n",
    "insee_dtype = {\n",
    "             \"count_initial_insee\":\"object\",\n",
    "\"siren\":\"object\",\n",
    "\"siret\":\"object\",\n",
    "\"datecreationetablissement\":\"object\",\n",
    "\"etablissementsiege\":\"object\",\n",
    "\"etatadministratifetablissement\":\"object\",\n",
    "\"codepostaletablissement\":\"object\",\n",
    "\"codecommuneetablissement\":\"object\",\n",
    "\"libellecommuneetablissement\":\"object\",\n",
    "\"ville_matching\":\"object\",\n",
    "\"libellevoieetablissement\":\"object\",\n",
    "\"complementadresseetablissement\":\"object\",\n",
    "\"numerovoieetablissement\":\"object\",\n",
    "\"indicerepetitionetablissement\":\"object\",\n",
    "\"typevoieetablissement\":\"object\",\n",
    "\"adress_reconstituee_insee\":\"object\",\n",
    "\"enseigne1etablissement\":\"object\",\n",
    "\"enseigne2etablissement\":\"object\",\n",
    "\"enseigne3etablissement\" :\"object\"\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reindex = ['index_id',\n",
    " 'siren',\n",
    " 'siret',\n",
    " 'count_initial_insee',\n",
    " 'date_début_activité',\n",
    " 'datecreationetablissement',\n",
    " 'code_greffe',\n",
    " 'nom_greffe',\n",
    " 'numero_gestion',\n",
    " 'id_etablissement',\n",
    " 'status',\n",
    " 'origin',\n",
    " 'date_greffe',\n",
    " 'file_timestamp',\n",
    " 'libelle_evt',\n",
    " 'last_libele_evt',\n",
    " 'status_admin',\n",
    " 'etatadministratifetablissement',\n",
    " 'type',\n",
    " 'status_ets',\n",
    " 'etablissementsiege',\n",
    " 'numero_voie_matching',\n",
    " 'numerovoieetablissement',\n",
    " 'voie_clean',\n",
    " 'voie_matching',\n",
    " 'typevoieetablissement',\n",
    " 'libellevoieetablissement',\n",
    " 'complementadresseetablissement',\n",
    " 'adress_reconstituee_inpi',\n",
    " 'adress_reconstituee_insee',\n",
    " 'adress_regex_inpi',\n",
    " 'adress_distance_inpi',\n",
    " 'test_address_regex',\n",
    " 'jacquard',\n",
    " 'edit',\n",
    " 'min_jacquard',\n",
    " 'min_edit',\n",
    " 'test_jacquard_adress',\n",
    " 'test_edit_adress',\n",
    " 'test_distance_diff',\n",
    " 'code_postal_matching',\n",
    " 'codepostaletablissement',\n",
    " 'ville_matching',\n",
    " 'libellecommuneetablissement',\n",
    " 'code_commune',\n",
    " 'codecommuneetablissement',\n",
    " 'enseigne',\n",
    " 'enseigne1etablissement',\n",
    " 'enseigne2etablissement',\n",
    " 'enseigne3etablissement',\n",
    " 'jacquard_enseigne1',\n",
    " 'jacquard_enseigne2',\n",
    " 'jacquard_enseigne3',\n",
    " 'edit_enseigne1',\n",
    " 'edit_enseigne2',\n",
    " 'edit_enseigne3',\n",
    " 'min_jacquard_enseigne1',\n",
    " 'min_jacquard_enseigne2',\n",
    " 'min_jacquard_enseigne3',\n",
    " 'min_edit_enseigne1',\n",
    " 'min_edit_enseigne2',\n",
    " 'min_edit_enseigne3',\n",
    " 'test_enseigne_insee',\n",
    " 'test_enseigne_jacquard',\n",
    " 'test_enseigne_edit',\n",
    " 'csv_source',\n",
    " 'indicerepetitionetablissement'\n",
    " ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processus de siretisation\n",
    "\n",
    "## Variables utilisées pour les tests\n",
    "\n",
    "Lors du processus de siretisation, nous avons besoin de créer de nouvelles variables, qui seront, faites lors de la préparation de la donnée. \n",
    "\n",
    "Les nouvelles variables sont les suivantes:\n",
    "\n",
    "- `adress_regex_inpi`: Concatenation des champs de l'adresse, suppression des espaces, des articles et des numéros et ajout de `(?:^|(?<= ))(` et `)(?:(?= )|$)`\n",
    "- `adress_distance_inpi`: Concatenation des champs de l'adresse, suppression des espaces et des articles\n",
    "- `adress_reconstituee_insee`: Reconstitution de l'adresse à l'INSEE en utilisant le numéro de voie `numeroVoieEtablissement`, le type de voie non abbrégé, `voie_clean`, l'adresse `libelleVoieEtablissement` et le `complementAdresseEtablissement` et suppression des articles\n",
    "- `enseigne`\n",
    "- `enseigne1etablissement`\n",
    "- `enseigne2etablissement`\n",
    "- `enseigne3etablissement`\n",
    "\n",
    "\n",
    "### variables nécéssaires aux tests\n",
    "\n",
    "Les variables ci dessous sont des nouvelles variables résultant du merge entre les deux tables\n",
    "\n",
    "- `test_address_regex`: \n",
    "    - Si un des mots contenus dans la variable `adress_regex_inpi` est présente dans la variable `adress_reconstituee_insee` alors True\n",
    "- `jacquard`:\n",
    "    - Calcul de la distance (dissimilarité) entre `adress_distance_inpi` et `adress_reconstituee_insee`\n",
    "- `edit`:\n",
    "    - Calcul de la distance (Levhenstein) entre `adress_distance_inpi` et `adress_reconstituee_insee`\n",
    "- `jacquard_enseigne1`:\n",
    "    - Jaccard distance entre `enseigne` et `enseigne1Etablissement1`\n",
    "- `jacquard_enseigne2`:\n",
    "    - Jaccard distance entre `enseigne` et `enseigne1Etablissement2`\n",
    "- `jacquard_enseigne3`:\n",
    "    - Jaccard distance entre `enseigne` et `enseigne1Etablissement3`\n",
    "- `edit_enseigne1`:\n",
    "    - Edit distance entre `enseigne` et `enseigne1Etablissement1`\n",
    "- `edit_enseigne2`:\n",
    "    - Edit distance entre `enseigne` et `enseigne1Etablissement2`\n",
    "- `edit_enseigne3`:\n",
    "    - Edit distance entre `enseigne` et `enseigne1Etablissement3`\n",
    "- `min_jacquard`:\n",
    "    - Jaccard distance minimum sur l'adresse pour la séquance `siren`, `code_greffe`, `nom_greffe`, `numero_gestion`, `id_etablissement`\n",
    "- `min_edit`:\n",
    "    - Edit distance minimum sur l'adresse pour la séquance `siren`, `code_greffe`, `nom_greffe`, `numero_gestion`, `id_etablissement`\n",
    "- `min_jacquard_enseigne1`:\n",
    "    - Jaccard distance minimum sur l'enseigne 1 pour la séquance `siren`, `code_greffe`, `nom_greffe`, `numero_gestion`, `id_etablissement`\n",
    "- `min_jacquard_enseigne2`:\n",
    "    - Jaccard distance minimum sur l'enseigne 2 pour la séquance `siren`, `code_greffe`, `nom_greffe`, `numero_gestion`, `id_etablissement`\n",
    "- `min_jacquard_enseigne3`:\n",
    "    - Jaccard distance minimum sur l'enseigne 3 pour la séquance `siren`, `code_greffe`, `nom_greffe`, `numero_gestion`, `id_etablissement`\n",
    "- `min_edit_enseigne1`:\n",
    "    - Edit distance minimum sur l'enseigne 1 pour la séquance `siren`, `code_greffe`, `nom_greffe`, `numero_gestion`, `id_etablissement`\n",
    "- `min_edit_enseigne2`:\n",
    "    - Edit distance minimum sur l'enseigne 2 pour la séquance `siren`, `code_greffe`, `nom_greffe`, `numero_gestion`, `id_etablissement`\n",
    "- `min_edit_enseigne3`:\n",
    "    - Edit distance minimum sur l'enseigne 3 pour la séquance `siren`, `code_greffe`, `nom_greffe`, `numero_gestion`, `id_etablissement`\n",
    "\n",
    "\n",
    "### Variables Tests \n",
    "\n",
    "- test_jacquard_adress:\n",
    "    - Si `jacquard` est égale à `min_jacquard` alors, True\n",
    "- test_edit_adress:\n",
    "    - Si `edit` est égale à `min_edit` alors, True\n",
    "- test_distance_diff:\n",
    "    - Vérification des désacords entre distance de Jaccard et Levhenstein\n",
    "        - Si (`test_jacquard_adress` est True et `test_edit_adress` est False) OU (`test_jacquard_adress` est False et `test_edit_adress` est True), alors True\n",
    "- test_enseigne_insee:\n",
    "    - Si `enseigne1Etablissement1` et `enseigne1Etablissement2` et `enseigne1Etablissement3` ne sont pas renseignées, alors True\n",
    "- test_enseigne_jacquard: \n",
    "    - Si l'enseigne à l'INSEE ou à l'INPI est renseignée et que la distance de jaccard est égale à 0, alors True\n",
    "- test_enseigne_edit:\n",
    "    - Si l'enseigne à l'INSEE ou à l'INPI est renseignée et que la distance de Levensthein est égale à 0, alors True\n",
    "    \n",
    "# Processus\n",
    "\n",
    "## Etape 1: Merge\n",
    "\n",
    "Dans un premier temps, nous allons merger la table de l'INSEE et de l'INPI sur un ensemble de variable très contraignante: \n",
    "\n",
    "- `siren`\n",
    "- `status_ets`: Etablissement ouvert/fermé\n",
    "- `numero_voie_matching`: Numéro de voie\n",
    "- `voie_matching`:  Type de voie\n",
    "- `ncc`: ville\n",
    "- `code_commune`:  Code commune\n",
    "- `status_admin`: Type d'entreprise\n",
    "- `date_début_activité`: Date de création de l'établissement\n",
    "- `code_postal_matching`: Code postal\n",
    "\n",
    "L'idée principale est de trouver le siret d'une séquence avec le plus d'exactitude pour ensuite appliquer le siret à la séquence. Cette technique permet d'être plus sur sur l'historique. En effet, l'INSEE donne le dernier état connu, il peut donc avoir des différences avec les valeurs historisées de l'INPI, surtout sur le type ou l'enseigne.\n",
    "\n",
    "## Etape 2: Création variables tests\n",
    "\n",
    "Dans cette étape, nous allons créer toutes les variables de test, comme évoqué précédement, a savoir sur l'adresse et l'enseigne.\n",
    "\n",
    "## Etape 3: Dedoublonnage\n",
    "\n",
    "Cette étape permet de dédoublonner les lignes matchées via la variable `index`. En effet, il est possible d'avoir des doublons lorsque l'entreprise à plusieurs établissements dans la même adresse. C'est le cas pour les sièges et principals.\n",
    "\n",
    "On va appliquer le filtre sur l'ensemble de la table matchée, puis compter le nombre de siret par séquence. Si le nombre de siret est égal à 1, c'est un bon match, sinon, il y a encore des doublons même après le filtre. Nous allons appliquer un deuxième filtre sur les doublons puis concatener avec les séquences ayant 1 siret. Dès lors, on applique la fonction `split_duplication` pour séparer les doublons des valeurs uniques. Si il y a encore des doublons, il n'y a pas suffisamment d'information pour distinguer le bon siret. Il faudra prendre plus de précaution avec des séquences\n",
    "\n",
    "Les règles sons les suivantes:\n",
    "\n",
    "### Filtre 1\n",
    "\n",
    "- Si `test_address_regex` est égal a True, ET `test_jacquard_adress` est égal à True, ET `test_edit_adress` est égal à True, ET `test_enseigne_edit` est égal a True OU `test_enseigne_jacquard` est égal a True OU `count_initial_insee` est égal à 1\n",
    "\n",
    "### Filtre 2\n",
    "\n",
    "Le filtre deux ne s'applique que sur les lignes dont la séquence a plus de deux sirets. Le filtre est le suivant \n",
    "\n",
    "- Si `jacquard` est egal 0 et `edit` est egal a 0, alors on garde. Autrement dit, on ne garde que les lignes dont l'adresse est correcte dans les deux cas. On pourrait potentiellement lever la contrainte.\n",
    "\n",
    "\n",
    "## Etape 4: Récupération sequence dans table INPI\n",
    "\n",
    "Dans cette étape, nous allons utiliser les siret que nous venons de récupérer et les matcher avec la table de l'INPI. Cela évite de refaire tous les tests sur des séquences dont on a déjà récupérer le siret.\n",
    "\n",
    "Tout d'abord, nous devons récupérer les siret sur la séquence `siren`, `code_greffe`, `nom_greffe`, `numero_gestion`, et `id_etablissement`. Attention, il faut enlever les doublons du aux valeurs historiques, puis on merge avec la table de l'INSEE. \n",
    "\n",
    "## Etape 5: Concatenation des sequences\n",
    "\n",
    "Maintenant que nous avons réussi a récuperer les siret dans la table INPI depuis les valeurs connues lors de nos tests, nous pouvons concatener les deux tables et ne prendre que les colonnes d'origines.\n",
    "\n",
    "Il faut tout de même refaire la fonction `split_duplication` pour enlever les siret multiples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_possibilities[0]['match']['inpi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_possibilities[0]['match']['insee']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siretisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')\n",
    "#os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, values in enumerate(list_possibilities[:1]):\n",
    "    df_ets = 'data/input/INPI/ets_final_sql_{0}.csv'.format(key)\n",
    "    #print(df_ets)\n",
    "    \n",
    "    ## Etape 1\n",
    "    inpi = import_dask(file=df_ets,\n",
    "                                usecols=inpi_col,\n",
    "                                dtype=inpi_dtype,\n",
    "                                parse_dates=False)\n",
    "       \n",
    "    insee = import_dask(\n",
    "        file= 'data/input/INSEE/insee_final_sql.csv',\n",
    "        usecols=insee_col,\n",
    "        dtype=insee_dtype\n",
    ")\n",
    "    \n",
    "    temp = (inpi\n",
    "            .merge(insee,\n",
    "                          how='inner',\n",
    "                   left_on = ['siren','code_postal_matching'],\n",
    "                   right_on = ['siren','codepostaletablissement']\n",
    "                   \n",
    "                          #left_on=list_possibilities[0]['match']['inpi'],\n",
    "                          #right_on= list_possibilities[0]['match']['insee']\n",
    "                  )\n",
    "           ).compute()\n",
    "    \n",
    "    ### Etape 2\n",
    "    ## Test 1: address\n",
    "    df_2 = dd.from_pandas(temp, npartitions=10)\n",
    "    df_2['test_address_regex'] = df_2.map_partitions(\n",
    "                lambda df:\n",
    "                    df.apply(lambda x:\n",
    "                        find_regex(\n",
    "                         x['adress_regex_inpi'],\n",
    "                         x['adress_reconstituee_insee']), axis=1)\n",
    "                         ).compute()\n",
    "\n",
    "    df_2['jacquard'] = df_2.map_partitions(\n",
    "                lambda df:\n",
    "                    df.apply(lambda x:\n",
    "                        jackard_distance(\n",
    "                         x['adress_distance_inpi'],\n",
    "                         x['adress_reconstituee_insee']), axis=1)\n",
    "                         ).compute()\n",
    "\n",
    "    df_2['edit'] = df_2.map_partitions(\n",
    "                lambda df:\n",
    "                    df.apply(lambda x:\n",
    "                        edit_distance(\n",
    "                         x['adress_distance_inpi'],\n",
    "                         x['adress_reconstituee_insee']), axis=1)\n",
    "                         ).compute()\n",
    "    \n",
    "    df_2['jacquard_enseigne1'] = df_2.map_partitions(\n",
    "            lambda df:\n",
    "                df.apply(lambda x:\n",
    "                    jackard_distance(\n",
    "                     x['enseigne'],\n",
    "                     x['enseigne1etablissement']), axis=1)\n",
    "                     ).compute()\n",
    "    df_2['jacquard_enseigne2'] = df_2.map_partitions(\n",
    "                lambda df:\n",
    "                    df.apply(lambda x:\n",
    "                        jackard_distance(\n",
    "                         x['enseigne'],\n",
    "                         x['enseigne2etablissement']), axis=1)\n",
    "                         ).compute()\n",
    "    df_2['jacquard_enseigne3'] = df_2.map_partitions(\n",
    "                lambda df:\n",
    "                    df.apply(lambda x:\n",
    "                        jackard_distance(\n",
    "                         x['enseigne'],\n",
    "                         x['enseigne3etablissement']), axis=1)\n",
    "                         ).compute()\n",
    "\n",
    "    df_2['edit_enseigne1'] = df_2.map_partitions(\n",
    "                lambda df:\n",
    "                    df.apply(lambda x:\n",
    "                        edit_distance(\n",
    "                         x['enseigne'],\n",
    "                         x['enseigne1etablissement']), axis=1)\n",
    "                         ).compute()\n",
    "    df_2['edit_enseigne2'] = df_2.map_partitions(\n",
    "                lambda df:\n",
    "                    df.apply(lambda x:\n",
    "                        edit_distance(\n",
    "                         x['enseigne'],\n",
    "                         x['enseigne2etablissement']), axis=1)\n",
    "                         ).compute()\n",
    "    df_2['edit_enseigne3'] = df_2.map_partitions(\n",
    "                lambda df:\n",
    "                    df.apply(lambda x:\n",
    "                        edit_distance(\n",
    "                         x['enseigne'],\n",
    "                         x['enseigne3etablissement']), axis=1)\n",
    "                         ).compute()\n",
    "\n",
    "    df_2 = df_2.compute()\n",
    "\n",
    "    df_2 = df_2.assign(\n",
    "        min_jacquard = lambda x:\n",
    "        x.groupby(['siren', 'code_greffe', 'nom_greffe', 'numero_gestion',\n",
    "                   'id_etablissement'])['jacquard'].transform('min'),\n",
    "        min_edit = lambda x:\n",
    "        x.groupby(['siren', 'code_greffe', 'nom_greffe', 'numero_gestion',\n",
    "                   'id_etablissement'])['edit'].transform('min'))\n",
    "\n",
    "    df_2 = df_2.assign(\n",
    "        test_jacquard_adress = lambda x: np.where(\n",
    "            x['jacquard'] == x['min_jacquard'],\n",
    "            True, False\n",
    "        ),\n",
    "        test_edit_adress = lambda x: np.where(\n",
    "            x['edit'] == x['min_edit'],\n",
    "            True, False\n",
    "        ),\n",
    "        test_distance_diff = lambda x:\n",
    "        np.where(\n",
    "            np.logical_or(\n",
    "                np.logical_and(\n",
    "                x['jacquard'] == x['min_jacquard'],\n",
    "                x['edit'] != x['min_edit']\n",
    "                ),\n",
    "                np.logical_and(\n",
    "                x['jacquard'] != x['min_jacquard'],\n",
    "                x['edit'] == x['min_edit']\n",
    "                )\n",
    "            ),\n",
    "            True, False\n",
    "\n",
    "        ),\n",
    "\n",
    "        min_jacquard_enseigne1 = lambda x:\n",
    "        x.groupby(['siren', 'code_greffe', 'nom_greffe', 'numero_gestion',\n",
    "                   'id_etablissement'])['jacquard_enseigne1'].transform('min'),\n",
    "\n",
    "        min_jacquard_enseigne2 = lambda x:\n",
    "        x.groupby(['siren', 'code_greffe', 'nom_greffe', 'numero_gestion',\n",
    "                   'id_etablissement'])['jacquard_enseigne2'].transform('min'),\n",
    "\n",
    "        min_jacquard_enseigne3 = lambda x:\n",
    "        x.groupby(['siren', 'code_greffe', 'nom_greffe', 'numero_gestion',\n",
    "                   'id_etablissement'])['jacquard_enseigne3'].transform('min'),\n",
    "\n",
    "        min_edit_enseigne1 = lambda x:\n",
    "        x.groupby(['siren', 'code_greffe', 'nom_greffe', 'numero_gestion',\n",
    "                   'id_etablissement'])['edit_enseigne1'].transform('min'),\n",
    "\n",
    "        min_edit_enseigne2 = lambda x:\n",
    "        x.groupby(['siren', 'code_greffe', 'nom_greffe', 'numero_gestion',\n",
    "                   'id_etablissement'])['edit_enseigne2'].transform('min'),\n",
    "\n",
    "        min_edit_enseigne3 = lambda x:\n",
    "        x.groupby(['siren', 'code_greffe', 'nom_greffe', 'numero_gestion',\n",
    "                   'id_etablissement'])['edit_enseigne3'].transform('min')\n",
    "    )\n",
    "\n",
    "    #df_2 = df_2.drop(columns =  ['test_enseigne_insee','test_enseigne_edit',\n",
    "    #                                               'test_enseigne_jacquard'])\n",
    "\n",
    "    df_2.loc[\n",
    "        (df_2['enseigne1etablissement'].isin([np.nan]))\n",
    "        &(df_2['enseigne2etablissement'].isin([np.nan]))\n",
    "        &(df_2['enseigne3etablissement'].isin([np.nan])),\n",
    "        'test_enseigne_insee'\n",
    "    ] = True\n",
    "\n",
    "    df_2.loc[\n",
    "            (df_2['test_enseigne_insee'] != True),\n",
    "            'test_enseigne_insee'] = False\n",
    "\n",
    "    df_2.loc[\n",
    "        (df_2['enseigne'].isin([np.nan]))\n",
    "        |(df_2['test_enseigne_insee'].isin([True]))\n",
    "        |(df_2['jacquard_enseigne1'] == 0)\n",
    "        |(df_2['jacquard_enseigne2'] == 0)\n",
    "        |(df_2['jacquard_enseigne3'] == 0),\n",
    "        'test_enseigne_jacquard'\n",
    "    ] = True\n",
    "\n",
    "    df_2.loc[\n",
    "        (df_2['enseigne'].isin([np.nan]))\n",
    "        |(df_2['test_enseigne_insee'].isin([True]))\n",
    "        |(df_2['edit_enseigne1'] == 0)\n",
    "        |(df_2['edit_enseigne2'] == 0)\n",
    "        |(df_2['edit_enseigne3'] == 0),\n",
    "        'test_enseigne_edit'\n",
    "    ] = True\n",
    "\n",
    "    df_2.loc[\n",
    "            (df_2['test_enseigne_edit'] != True),\n",
    "            'test_enseigne_edit'] = False\n",
    "\n",
    "    df_2.loc[\n",
    "            (df_2['test_enseigne_jacquard'] != True),\n",
    "            'test_enseigne_jacquard'] = False\n",
    "    \n",
    "    df_2 = df_2.reindex(columns = reindex)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.loc[lambda x: x['test_address_regex'].isin([False])].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est assez simple de voir que le merge a abouti a la création de doublon "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons appliquer des règles de gestion sur les combinaisons matchées (les `both`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 2: Création variables tests\n",
    "\n",
    "Dans cette étape, nous allons créer toutes les variables de test, comme évoqué précédement, a savoir sur l'adresse et l'enseigne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.to_csv('temp.csv', index= False)#.loc[lambda x: x['siren'].isin(['400534020'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Nombre de duplicate\n",
    "df_2.shape[0] -df_2['index_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.stb.freq(['test_address_regex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.stb.freq(['test_jacquard_adress'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.stb.freq(['test_edit_adress'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.stb.freq(['test_distance_diff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.stb.freq(['test_enseigne_jacquard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.stb.freq(['test_enseigne_edit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 3: Dedoublonnage\n",
    "\n",
    "Cette étape permet de dédoublonner les lignes matchées via la variable `index`. En effet, il est possible d'avoir des doublons lorsque l'entreprise à plusieurs établissements dans la même adresse. C'est le cas pour les sièges et principals.\n",
    "\n",
    "On va appliquer le filtre sur l'ensemble de la table matchée, puis compter le nombre de siret par séquence. Si le nombre de siret est égal à 1, c'est un bon match, sinon, il y a encore des doublons même après le filtre. Nous allons appliquer un deuxième filtre sur les doublons puis concatener avec les séquences ayant 1 siret. Dès lors, on applique la fonction `split_duplication` pour séparer les doublons des valeurs uniques. Si il y a encore des doublons, il n'y a pas suffisamment d'information pour distinguer le bon siret. Il faudra prendre plus de précaution avec des séquences\n",
    "\n",
    "Les règles sons les suivantes:\n",
    "\n",
    "### Filtre 1\n",
    "\n",
    "- Si `test_regex_adress` est égal a True, ET `test_jacquard_adress` est égal à True, ET `test_edit_adress` est égal à True, ET `test_enseigne_edit` est égal a True OU `test_enseigne_jacquard` est égal a True OU `count_initial_insee` est égal à 1\n",
    "\n",
    "### Filtre 2\n",
    "\n",
    "Le filtre deux ne s'applique que sur les lignes dont la séquence a plus de deux sirets. Le filtre est le suivant \n",
    "\n",
    "- Si `jacquard` est egal 0 et `edit` est egal a 0, alors on garde. Autrement dit, on ne garde que les lignes dont l'adresse est correcte dans les deux cas. On pourrait potentiellement lever la contrainte.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_2 = pd.read_csv('temp.csv', dtype = {'siren': 'O',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = ['siren', 'code_greffe', 'nom_greffe', 'numero_gestion', 'id_etablissement']\n",
    "df_3 = (df_2.loc[\n",
    "    \n",
    "    lambda x: \n",
    "    (x['test_address_regex'].isin([True]))\n",
    "    &\n",
    "    (x['test_jacquard_adress'].isin([True]))\n",
    "    &\n",
    "    (x['test_edit_adress'].isin([True]))\n",
    "    &\n",
    "    (x['test_enseigne_edit'].isin([True]))\n",
    "    |\n",
    "    (x['test_enseigne_jacquard'].isin([True]))\n",
    "    |\n",
    "    (x['count_initial_insee'].isin([1])) ### 1 seul etb a l'INSEE\n",
    "]\n",
    "          .assign(\n",
    "    total_siret = lambda x: \n",
    "    x.groupby(sequence)['siret'].transform('nunique')\n",
    ")\n",
    "          #.reindex(columns = reindex)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3.stb.freq(['total_siret'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, nous allons faire une dédoublonnage sur les séquences avec plusieurs siret, puis on concatene avec les lignes ayant qu'un seul siret par séquence. Finalement, on applique la fonction `split_duplication` pour écarter les doublons restant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3.loc[lambda x: \n",
    "         (x['total_siret'] > 1)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3_bis = split_duplication(pd.concat(\n",
    "    [df_3.loc[lambda x: \n",
    "         (x['total_siret'] > 1)\n",
    "        &\n",
    "         (\n",
    "         (x['jacquard'] == 0) \n",
    "         |\n",
    "         (x['edit'] == 0) \n",
    "         )]\n",
    "        ,\n",
    "     df_3.loc[lambda x: \n",
    "         (x['total_siret'] == 1)\n",
    "             ]\n",
    "    ]\n",
    "     )\n",
    "                             \n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_2.loc[lambda x:x['test_distance_diff'].isin([True])].head()\n",
    "# test_distance_diff  A REVOIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3_bis['not_duplication'] = df_3_bis['not_duplication'].sort_values(by = [\n",
    "     'siren','code_greffe', 'nom_greffe', 'numero_gestion', 'id_etablissement', 'date_greffe'\n",
    " ])\n",
    "df_3_bis['not_duplication'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3_bis['duplication'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 4: Récupération sequence dans table INPI\n",
    "\n",
    "Dans cette étape, nous allons utiliser les siret que nous venons de récupérer et les matcher avec la table de l'INPI. Cela évite de refaire tous les tests sur des séquences dont on a déjà récupérer le siret.\n",
    "\n",
    "Tout d'abord, nous devons récupérer les siret sur la séquence `siren`, `code_greffe`, `nom_greffe`, `numero_gestion`, et `id_etablissement`. Attention, il faut enlever les doublons du aux valeurs historiques, puis on merge avec la table de l'INSEE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_siret = ['siren', 'siret', 'code_greffe', 'nom_greffe', 'numero_gestion', 'id_etablissement']\n",
    "df_3_bis['not_duplication'][seq_siret].drop_duplicates() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recupération de l'historique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seq = ['siren','code_greffe', 'nom_greffe', 'numero_gestion', 'id_etablissement']\n",
    "#columns_to_keep = ['siren', 'siret', 'code_greffe', 'nom_greffe', 'numero_gestion',\n",
    "#       'id_etablissement', 'total_siret', 'origin', 'file_timestamp',\n",
    "#       'date_greffe', 'libelle_evt', 'last_libele_evt', 'type', 'adress_new',\n",
    "#       'adresse_new_clean_reg', 'voie_matching', 'numero_voie_matching',\n",
    "#       'code_postal_matching', 'ncc', 'code_commune', 'enseigne',\n",
    "#       'date_début_activité', 'index_id', 'status_admin', 'status_ets', '_merge']\n",
    "\n",
    "df_match_2 = (\n",
    "    (df_3_bis['not_duplication'][seq_siret + ['total_siret']]\n",
    " .drop_duplicates()  \n",
    "    )\n",
    " .merge(inpi.compute().loc[lambda x: \n",
    "                                 ~x['index_id'].isin(df_3_bis['not_duplication']['index_id'])],\n",
    "                           on = sequence, how = 'left', indicator = True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_match_2.stb.freq(['_merge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_match_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 5: Concatenation des sequences\n",
    "\n",
    "Maintenant que nous avons réussi a récuperer les siret dans la table INPI depuis les valeurs connues lors de nos tests, nous pouvons concatener les deux tables et ne prendre que les colonnes d'origines.\n",
    "\n",
    "Il faut tout de même refaire la fonction `split_duplication` pour enlever les siret multiples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = (pd.concat(\n",
    "    [\n",
    "        df_match_2.loc[lambda x: x['_merge'].isin(['both'])],\n",
    "        df_3_bis['not_duplication']#.reindex(columns  = columns_to_keep)\n",
    "        \n",
    "    ])\n",
    " .sort_values(by = [\n",
    "     'siren','code_greffe', 'nom_greffe', 'numero_gestion', 'id_etablissement', 'date_greffe'\n",
    " ])\n",
    " #.loc[lambda x: x['total_siret'] == 1]            \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sequence trouvée\n",
    "df_final_no_duplicate = split_duplication(\n",
    "                df_final)['not_duplication']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_no_duplicate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_duplicate = split_duplication(\n",
    "                df_final)['duplication']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_duplicate.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 6: Ecarte les séquences trouvées de la table INPI\n",
    "\n",
    "La dernière étape consiste a enlever les index des séquences siretisées de la table de l'INPI. On va sauvegarder la nouvelle table de l'INPI mais aussi, la table que a servi a trouver le siret, et la table siretiser.\n",
    "\n",
    "On sauvegarde aussi un table de log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a_trouver = inpi.compute().loc[lambda x: ~x['index_id'].isin(df_final['index_id'].values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'Nombre de lignes': df_final_no_duplicate.shape[0],\n",
    "'detail': {\n",
    "    'siren':df_final['siren'].nunique(),\n",
    "    'siret':{\n",
    "        'unique':df_final_no_duplicate['siret'].nunique(),\n",
    "        'multiple':df_final_duplicate['siret'].nunique(),\n",
    "        'siren_multiple':df_final_duplicate['siren'].nunique()\n",
    "    },\n",
    "    'merge_step_4': df_match_2.stb.freq(['_merge']).to_dict()\n",
    "},\n",
    "'reste a trouver':{\n",
    "    'siren': df_a_trouver['siren'].nunique(),\n",
    "    'size': df_a_trouver.shape[0],\n",
    "    'sequence':df_a_trouver.groupby(sequence)['siren'].nunique().sum()\n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10648545\n",
    "9245473 + 1403065 + 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_a_trouver.groupby(sequence)['siren'].nunique().sum()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
