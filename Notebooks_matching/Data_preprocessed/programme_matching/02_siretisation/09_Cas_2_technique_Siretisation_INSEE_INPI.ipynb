{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cas 2 Test nombre lignes siretise\n",
    "\n",
    "Copy paste from Coda to fill the information\n",
    "\n",
    "## Objective(s)\n",
    "\n",
    "- Lors de [l’US 7: Test nombre lignes siretise avec nouvelles regles de gestion](https://coda.io/d/CreditAgricole_dCtnoqIftTn/US-07-ETS-version-3_su0VF), une batterie d’analyse a été réalisé ce qui a permit de mettre en évidence des règles de gestion pour discriminer les lignes qui ont des doublons, mais aussi ce qui n’en n’ont pas. Effectivement, certaines lignes sans doublons n’ ont pas nécessairement une relation (i.e. l’adresse n’est pas celle du siret). \n",
    "\n",
    "  - Nous avons pu distinguer 4 grandes catégories pour séparer les lignes. Le tableau ci dessous récapitule les possibilités\n",
    "\n",
    "![00_ensemble_cas_siretisation.jpeg](https://codahosted.io/docs/CtnoqIftTn/blobs/bl-vmJJOeJ__6/455e18296e4076d4b2ec1dbcb5f1364af4b7824c8e96262f78095b67a3db4fd10422ee1adc6e06779452eeb0a47003a6f4a1e29995c559aa9531a8e6499db04a0e089b7e173ed187259fc4aa4a799935f8ead28ed9dfe555a307bdac2eb840e4991b963c)\n",
    "\n",
    "  - Dans cette US, nous allons traiter du cas ou il y a des doublon, et la relation entre l’adresse INSEE et INPI appartient au cas de figure 1,3 ou 4. Pour filtrer la table avec les lignes qui satisfont ses deux critères, il faut utiliser les variables:\n",
    "\n",
    "   - index_id_duplicate\n",
    "   - test_adresse_cas_1_3_4\n",
    "\n",
    "   * Les variables à utiliser pour les deux sont:\n",
    "    * test_siege\n",
    "    * test_enseigne\n",
    "    * test_sequence_siret\n",
    "  * Des que les lignes sont écartées, il faut réaliser un document json avec les informations sur la siretisation, qui indique les informations suivantes:\n",
    "    *  Nom csv : LOGS_CAS_2.json\n",
    "    * IN S3: RESULTATS_SIRETISATION/LOGS_ANALYSE\n",
    "    \n",
    "```    \n",
    "dic_ = {\n",
    "    \n",
    "    'CAS': 'CAS_2',\n",
    "    'count_all_index':total_index.values[0][0],\n",
    "    'count_filtered_in': total_index_filtre.values[0][0],\n",
    "    'count_filtered_out':total_index_found.values[0][0],\n",
    "    'percentage_found': total_index_found.values[0][0]/total_index.values[0][0]\n",
    "}\n",
    "```\n",
    "\n",
    "  * Un csv avec le siret par index sera créé. Le CSV doit contenir les informations suivantes:\n",
    "    *  Nom csv : RESULTAT_CAS_2.csv\n",
    "    * IN S3: RESULTATS_SIRETISATION/TABLES_SIRET\n",
    "      * index_id, \n",
    "      * sequence_id, \n",
    "      * siren, \n",
    "      * siret\n",
    "      * test_sequence_siret,\n",
    "      * test_index_siret, \n",
    "      *  test_list_num_voie, \n",
    "      *  test_date, \n",
    "      *  test_status_admin,\n",
    "      *  test_siege\n",
    "      *  test_code_commune\n",
    "      *  test_adresse_cas_1_3_4, \n",
    "      *  test_duplicates_is_in_cas_1_3_4\n",
    "      *  test_enseigne  \n",
    "\n",
    "  - Il faut aussi filtrer les lignes qui sont à éjecter du test test_duplicates_is_in_cas_1_3_4 a savoir les TO_REMOVE \n",
    "\n",
    "  - Include DataStudio (tick if yes): false\n",
    "\n",
    "## Metadata \n",
    "\n",
    "- Metadata parameters are available here: [Ressources_suDYJ#_luZqd](http://Ressources_suDYJ#_luZqd)\n",
    "\n",
    "- Task type:\n",
    "    - Jupyter Notebook\n",
    "- Users: :\n",
    "    - [Thomas Pernet](mailto:t.pernetcoudrier@gmail.com)\n",
    "- Watchers:\n",
    "\n",
    "  - [Thomas Pernet](mailto:t.pernetcoudrier@gmail.com)\n",
    "- Estimated Log points:\n",
    " - One being a simple task, 15 a very difficult one\n",
    "    -  7\n",
    "- Task tag\n",
    "    -  \\#sql-query,#matching,#siretisation,#regle-de-gestion,#cas-1\n",
    "- Toggl Tag\n",
    "   - \\#data-analysis \n",
    "  \n",
    "## Input Cloud Storage [AWS/GCP]\n",
    "\n",
    "If link from the internet, save it to the cloud first\n",
    "\n",
    "### Tables [AWS/BigQuery]\n",
    "\n",
    "- Batch 1:\n",
    "\n",
    "    - Select Provider: Athena\n",
    "\n",
    "    - Select table(s): ets_inpi_insee_cases_filtered\n",
    "\n",
    "      - Select only tables created from the same notebook, else copy/paste selection to add new input tables\n",
    "\n",
    "    - Information:\n",
    "\n",
    "        - Region: \n",
    "\n",
    "          - NameEurope (Paris)\n",
    "          - Code: eu-west-3\n",
    "\n",
    "        - Database: inpi\n",
    "\n",
    "          - Notebook construction file: [08_Cas_1_technique_Siretisation_INSEE_INPI](https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/02_siretisation/08_Cas_1_technique_Siretisation_INSEE_INPI.md)\n",
    "    \n",
    "## Destination Output/Delivery\n",
    "\n",
    "- AWS\n",
    "\n",
    "  - Athena: \n",
    "\n",
    "      - Region: Europe (Paris)\n",
    "      - Database: inpi\n",
    "      - Tables (Add name new table): ets_inpi_insee_cases_filtered\n",
    "      - List new tables\n",
    "      - ets_inpi_insee_cases_filtered\n",
    "- S3(Add new filename to Database: [Ressources](https://coda.io/d/CreditAgricole_dCtnoqIftTn/Ressources_suDYJ))\n",
    "\n",
    "    - Origin: Jupyter notebook\n",
    "\n",
    "    - Bucket: calfdata\n",
    "\n",
    "    - Key: RESULTATS_SIRETISATION/LOGS_ANALYSE\n",
    "\n",
    "    - Filename(s): LOGS_CAS_2.json,RESULTAT_CAS_2.csv\n",
    "        - [RESULTATS_SIRETISATION/TABLES_SIRET/RESULTAT_CAS_2.csv](https://s3.console.aws.amazon.com/s3/buckets/calfdata/RESULTATS_SIRETISATION/TABLES_SIRET)\n",
    "        - [RESULTATS_SIRETISATION/LOGS_ANALYSE/LOGS_CAS_2.json](https://s3.console.aws.amazon.com/s3/buckets/calfdata/RESULTATS_SIRETISATION/LOGS_ANALYSE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Things to know (Steps, Attention points or new flow of information)\n",
    "\n",
    "### Sources of information  (meeting notes, Documentation, Query, URL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connexion serveur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsPy.aws_authorization import aws_connector\n",
    "from awsPy.aws_athena import service_athena\n",
    "from awsPy.aws_s3 import service_s3\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, shutil\n",
    "bucket = 'calfdata'\n",
    "path = os.getcwd()\n",
    "parent_path = str(Path(path).parent)\n",
    "path_cred = r\"{}/credential_AWS.json\".format(parent_path)\n",
    "\n",
    "region = 'eu-west-3'\n",
    "bucket = 'calfdata'\n",
    "s3_output = 'INPI/sql_output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                       region = region)\n",
    "client= con.client_boto()\n",
    "s3 = service_s3.connect_S3(client = client,\n",
    "                      bucket = bucket, verbose = False) \n",
    "#athena = service_athena.connect_athena(client = client,\n",
    "#                      bucket = bucket) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation tables\n",
    "\n",
    "La première étape consiste à créer la table `ets_inpi_insee_cases_filtered` qui filtre la table `ets_inpi_insee_cases` des lignes lorsque la variable `test_duplicates_is_in_cas_1_3_4` est différent de `'TO_REMOVE'`\n",
    "\n",
    "## Steps\n",
    "\n",
    "- Creation de la table  `ets_inpi_insee_cases_filtered`\n",
    "- Compter le nombre d'observations\n",
    "- Brève analyse des résultats des tests\n",
    "- Séparation des lignes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count ligne a trouver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le nombre d'index a trouver sur l'ensemble de la base est de:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT COUNT(*) AS total_index\n",
    "FROM ets_inpi_insee_cases_filtered \n",
    "\"\"\"\n",
    "total_index = s3.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output=s3_output,\n",
    "    filename = 'count_ets_inpi_insee_cases_to_find',\n",
    "    destination_key = None\n",
    "        )\n",
    "total_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le nombre d'observations est de:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT COUNT(*) AS count_rows\n",
    "FROM ets_inpi_insee_cases_filtered \n",
    "WHERE index_id_duplicate = 'True' AND test_adresse_cas_1_3_4 ='True'\n",
    "\"\"\"\n",
    "total_index_filtre = s3.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output=s3_output,\n",
    "    filename = 'cas_1_count_ets_inpi_insee_cases_filtered',\n",
    "    destination_key = None\n",
    "        )\n",
    "total_index_filtre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brève analyse des résultats des tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top = \"\"\"\n",
    "#WITH test AS (\n",
    "#  SELECT test_list_num_voie  AS groups, COUNT(*) as cnt_test_list_num_voie\n",
    "#FROM ets_inpi_insee_cases_filtered \n",
    "#WHERE index_id_duplicate = 'False' AND test_adresse_cas_1_3_4 ='True'\n",
    "#GROUP BY test_list_num_voie\n",
    "#  )\n",
    "#\"\"\"\n",
    "\n",
    "top_1 = \"SELECT groups_true_false_null.groups, \"\n",
    "top_2 = \"\"\n",
    "middle_2 = \" FROM groups_true_false_null \"\n",
    "\n",
    "bottom_1 = \"\"\n",
    "bottom_2 = \"\"\"\n",
    "\n",
    "-- {0}\n",
    "\n",
    "LEFT JOIN (\n",
    "    SELECT {0}  AS groups, COUNT(*) as cnt_{0}\n",
    "    FROM ets_inpi_insee_cases_filtered \n",
    "    WHERE index_id_duplicate = 'True' AND test_adresse_cas_1_3_4 ='True'\n",
    "    GROUP BY {0}\n",
    "    ) AS tb_{0}\n",
    "  ON groups_true_false_null.groups = tb_{0}.groups\n",
    "\"\"\"\n",
    "\n",
    "tests = [\n",
    "    \"test_sequence_siret\",\n",
    "    \"test_index_siret\",\n",
    "    \"test_siren_insee_siren_inpi\",\n",
    "    \"test_sequence_siret_many_cas\",\n",
    "    'test_list_num_voie',\n",
    "    \"test_siege\",\n",
    "    \"test_date\",\n",
    "    \"test_status_admin\",\n",
    "    \"test_code_commune\",\n",
    "    \"test_type_voie\",\n",
    "    \"test_enseigne\"] \n",
    "\n",
    "for i, test in enumerate(tests):\n",
    "    var = 'cnt_{}'.format(test)\n",
    "    if i == len(tests) -1:\n",
    "        top_2 += '{}'.format(var)\n",
    "    else:\n",
    "        top_2+='{},'.format(var)\n",
    "        \n",
    "    bottom_1+= bottom_2.format(test)  \n",
    "\n",
    "query = top_1 + top_2 + middle_2 + bottom_1\n",
    "tb = s3.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output=s3_output,\n",
    "    filename = 'cas_2_results_tests',\n",
    "    destination_key = 'ANALYSE_POST_SIRETISATION'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `test_sequence_siret`: Pertinence faible\n",
    "    - `count_inpi_sequence_siret = 1 THEN ‘True’ ELSE ‘False``\n",
    "    - Si la variable est ‘True’ alors, il n’y a pas de duplicate pour une séquence\n",
    "- `test_index_siret`: Pertinence faible\n",
    "    - `count_inpi_index_id_stat_cas_siret = 1 THEN 'True' ELSE 'False'`\n",
    "    - Si la variable est true, alors, il n’y a qu”un seul cas de figure par index \n",
    "- `test_siren_insee_siren_inpi`: Pertinence elevée\n",
    "    - `count_initial_insee = count_inpi_siren_siret THEN 'True' ELSE 'False'`\n",
    "    - Si la variable est ‘True’ alors tous les établissements ont été trouvé\n",
    "- `test_sequence_siret_many_cas`: Pertinence faible\n",
    "    - `count_inpi_sequence_siret = count_inpi_sequence_stat_cas_siret THEN 'True' ELSE 'False`\n",
    "    - test si la séquence appartient a plusieurs cas\n",
    "- `test_date`: Pertinence moyenne\n",
    "    - `WHEN datecreationetablissement = date_debut_activite THEN 'True' WHEN datecreationetablissement IS NULL \n",
    "        OR date_debut_activite IS NULL THEN 'NULL' --WHEN datecreationetablissement = '' \n",
    "        ELSE 'False'``\n",
    "    - Test si la date de création de l'établissement est égale à la date de création. \n",
    "- `test_status_admin`: Pertinence moyenne\n",
    "    - `WHEN etatadministratifetablissement = status_admin THEN 'True' WHEN etatadministratifetablissement IS NULL \n",
    "        OR status_admin IS NULL THEN 'NULL' WHEN etatadministratifetablissement = '' \n",
    "        OR status_admin = '' THEN 'NULL' ELSE 'False'``\n",
    "    - Test si l'établissement est fermé ou non. Pas radié mais fermé. L'INSEE n'indique pas les radiations, et le fichier ETS de l'INPI n'indique pas les radiations et n'indique pas les fermetures resultants de radiation. Pour cela il faut construire la variable via la table PM ou PP.\n",
    "- `test_siege`: Pertinence elevée\n",
    "    - `etablissementsiege = status_ets THEN 'True' WHEN etablissementsiege IS NULL \n",
    "        OR status_ets IS NULL THEN 'NULL' WHEN etablissementsiege = '' \n",
    "        OR status_ets = '' THEN 'NULL' ELSE 'False'``\n",
    "    - Test si le siret est un siège ou non. \n",
    "- `test_code_commune`: Pertinence faible\n",
    "    - `codecommuneetablissement = code_commune THEN 'True' WHEN codecommuneetablissement IS NULL \n",
    "        OR code_commune IS NULL THEN 'NULL' WHEN codecommuneetablissement = '' \n",
    "        OR code_commune = '' THEN 'NULL' ELSE 'False'``\n",
    "    - Test si le code commune est identique entre l'INPI et l'INSEE. Pas suffisament fiable\n",
    "- `test_type_voie`: Pertinence faible\n",
    "    - `numerovoieetablissement = numero_voie_matching THEN 'True' WHEN numerovoieetablissement IS NULL \n",
    "        OR numero_voie_matching IS NULL THEN 'NULL' WHEN numerovoieetablissement = '' \n",
    "        OR numero_voie_matching = '' THEN 'NULL' ELSE 'False'`\n",
    "    - Test si le type de voie est identique entre les deux variables. Methode d'extraction que nous avons utilisé n'est pas suffisement pertinente\n",
    "- `test_enseigne`: Pertinence moyenne\n",
    "    - `WHEN cardinality(test) = 0 THEN 'NULL' WHEN enseigne = '' THEN 'NULL' WHEN temp_test_enseigne = TRUE THEN 'True' ELSE 'False'`\n",
    "    - Test si l'enseigne est identique entre les variables. Aucun retraitement si ce n'est mise en majuscule et exclusion des accents. Ne regardepas les fautes d'orthographe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tb.assign(\n",
    "    groups = lambda x: x['groups'].fillna('NULL'),\n",
    ")\n",
    " .replace({'groups':{True:'True', False:'False'}})\n",
    " .set_index('groups').T\n",
    " .assign(total_row = lambda x : x.sum(axis = 1))\n",
    " #.columns\n",
    " .sort_values(by = ['True'], ascending = False)\n",
    " .style\n",
    " .format(\"{:,.0f}\")\n",
    " .bar(subset= ['False'],color='#d65f5f')\n",
    " .bar(subset= ['True'],color='#008000')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons voir dans le tableau précédent qu'il y a 104,051 lignes qui ont plusieurs siret pour la même séquence. Une des explication possible est un déménagement de l'établissement sans fermeture coté greffe. En effet, un déménagement entraine automatiquement une nouvelle attribution de siret coté INSEE. Concernant l'INPI, le déménagement déclenche un événement de modification, et non une suppression. Dès lors, l'INSEE va indiquer la première adresse comme fermée alors que l'INPI va considérer les deux adresses comme étant ouvertes. \n",
    "\n",
    "Une autre explication peut être le changement de nom de la rue. \n",
    "\n",
    "Si dessous, deux exemples de cas de figure avec plusieurs adresses pour une même séquence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\" \n",
    "SELECT * \n",
    "FROM ets_final_sql  \n",
    "WHERE sequence_id = 4721741\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output=s3_output,\n",
    "    filename = 'exemple_changement_adresse_4721741',\n",
    "    destination_key = None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\" \n",
    "SELECT * \n",
    "FROM ets_final_sql  \n",
    "WHERE sequence_id = 2235206\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output=s3_output,\n",
    "    filename = 'exemple_changement_adresse_2235206',\n",
    "    destination_key = None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le tableau indique aussi 64,790 lignes avec des enseignes différentes. C'est le cas lorsque l'enseigne n'est pas écrit de manière identique coté INPI et coté INSEE. Par exemple, l'INPI indique `&` alors que l'INSEE écrit `ET`.  Une autre explication réside dans le changement d'enseigne. L'INSEE informe du dernier état connu de l'enseigne, alors que l'INPI fournit toutes les modifications. L'enseigne peut dès lors être différente pour les lignes historiques. Prenons l'exemple ci dessous, l'établissement a modifié son enseigne au cours du temps, induisant un échec du test, mais la variable `count_inpi_sequence_siret` est égale à 1, donc un seul siret possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT index_id, sequence_id, siren, siret,test_sequence_siret, count_inpi_sequence_siret, enseigne, enseigne1etablissement, enseigne2etablissement, enseigne3etablissement, test_enseigne\n",
    "FROM ets_inpi_insee_cases_filtered \n",
    "WHERE sequence_id = 9133786\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output=s3_output,\n",
    "    filename = 'exemple_changement_enseigne_9133786',\n",
    "    destination_key = None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci dessous, nous vérifions que le résultat des tests en prenant en compte le test `test_sequence_siret` comme référence, a savoir si la séquence a plusieurs siret. Le test impacte 104,051 lignes, comme indiqué dans le tableau ci dessus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query =\"\"\"\n",
    "SELECT test_enseigne,test_sequence_siret, COUNT(test_enseigne) as cnt\n",
    "FROM ets_inpi_insee_cases_filtered \n",
    "WHERE index_id_duplicate = 'True' AND test_adresse_cas_1_3_4 ='True' --AND test_sequence_siret = 'False'\n",
    "GROUP BY test_sequence_siret, test_enseigne\n",
    "\"\"\"\n",
    "tb = s3.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output=s3_output,\n",
    "    filename = 'cas_2_exemple_many_siret_sequence_test_enseigne',\n",
    "    destination_key = 'ANALYSE_POST_SIRETISATION'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a 1894 lignes qui ont la même adresse, mais qui a été modifié au cours du temps, et en plus avec une modification de nom d'enseigne qui n'a pas donné lieu a une fermeture de l'établissement côté INPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tb\n",
    " .set_index(['test_enseigne', 'test_sequence_siret'])\n",
    " .unstack(-1)\n",
    " .assign(\n",
    "     total_row = lambda x : x.sum(axis = 1),\n",
    "     pct_false = lambda x: x[('cnt',False)]/x['total_row'],\n",
    "     pct_true = lambda x: x[('cnt',True)]/x['total_row']\n",
    " )\n",
    " .style\n",
    " .format(\"{0:,.2%}\", subset=[\"pct_false\", \"pct_true\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un autre test peut être de regarder le nombre de séquence avec plusieurs siret, et qui ont des divergences aevc le statut de siège entre l'INSEE et l'INPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query =\"\"\"\n",
    "SELECT test_siege,test_sequence_siret, COUNT(test_siege) as cnt\n",
    "FROM ets_inpi_insee_cases_filtered \n",
    "WHERE index_id_duplicate = 'True' AND test_adresse_cas_1_3_4 ='True'\n",
    "GROUP BY test_sequence_siret, test_siege\n",
    "\"\"\"\n",
    "tb = s3.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output=s3_output,\n",
    "    filename = 'cas_2_exemple_many_siret_sequence_test_siege',\n",
    "    destination_key = 'ANALYSE_POST_SIRETISATION'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque le `test_siege` et `test_sequence_siret` sont tous les deux égals à `False` c'est que l'INPI n'a pas modifié correctement les informations. L'établissement a gardé la même séquence, mais a changé d'adresse. La modification n'a pas donné lieu a de modification du type d'établissement, a savoir siège, principal ou secondaire. Un changement d'adresse peut en même temps être accompagné d'un changement de type. L'INSEE peut par exemple, modifier l'adresse, créer un nouveau siret et modifier le type de siège a secondaire. L'INPI ne va pas faire tous ses changements. L'INPI va créer un événenement de modification de l'adresse, sans changement de type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tb\n",
    " .set_index(['test_siege', 'test_sequence_siret'])\n",
    " .unstack(-1)\n",
    " .assign(\n",
    "     total_row = lambda x : x.sum(axis = 1),\n",
    "     pct_false = lambda x: x[('cnt',False)]/x['total_row'],\n",
    "     pct_true = lambda x: x[('cnt',True)]/x['total_row']\n",
    " )\n",
    " .style\n",
    " .format(\"{0:,.2%}\", subset=[\"pct_false\", \"pct_true\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est le cas pour le fichier ci dessous, la séquence est la même coté INPI, mais à l'INSEE le nouvel établissement est devenu secondaire, or il est resté siège à l'INPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT row_id, index_id, sequence_id, siren, siret, count_inpi_siren_siret, test_sequence_siret, etablissementsiege, status_ets, test_siege\n",
    "FROM ets_inpi_insee_cases_filtered \n",
    "WHERE sequence_id = 4736523\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output=s3_output,\n",
    "    filename = 'cas_1_exemple_erreur_inpi_4736523',\n",
    "    destination_key = None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT index_id, sequence_id, siren, code_greffe, nom_greffe, numero_gestion, id_etablissement, origin, date_greffe, libelle_evt, type\n",
    "FROM ets_final_sql  \n",
    "WHERE sequence_id = 4736523\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output=s3_output,\n",
    "    filename = 'cas_1_exemple_erreur_inpi_4736523',\n",
    "    destination_key = None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT siren, siret, datederniertraitementetablissement, etablissementsiege\n",
    "FROM insee_rawdata_juillet\n",
    "WHERE siren = '512619941'\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output=s3_output,\n",
    "    filename = 'cas_1_exemple_erreur_inpi_4736523',\n",
    "    destination_key = None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export vers S3\n",
    "\n",
    "La liste des séquences avec les siret est maintenant prête, nous pouvons l'exporter dans le S3 afin de reconstruire l'ensemble des siret de la base. \n",
    "\n",
    "A partir du moment ou nous connaissons le siret d'une séquence, il est possible de l'attribuer aux autres lignes. Toutefois, nous avons vu qu'une sequence peut avoir plusieurs siret, de fait, il faut être très attentif lors de la siretisation de nouvelles valeurs.\n",
    "\n",
    "Dans ce fichier csv, nous allons garder les tests suivants:\n",
    "\n",
    "- test_sequence_siret,\n",
    "- test_index_siret, \n",
    "- test_list_num_voie, \n",
    "- test_date, \n",
    "- test_status_admin,\n",
    "- test_siege\n",
    "- test_code_commune\n",
    "- test_adresse_cas_1_3_4, \n",
    "- test_duplicates_is_in_cas_1_3_4\n",
    "- test_enseigne  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT index_id, sequence_id, siren, siret, \n",
    "test_sequence_siret, test_index_siret, test_list_num_voie, test_date, \n",
    "test_status_admin, test_siege, test_code_commune, test_adresse_cas_1_3_4, \n",
    "test_duplicates_is_in_cas_1_3_4, test_enseigne  \n",
    "FROM ets_inpi_insee_cases_filtered \n",
    "WHERE index_id_duplicate = 'True' AND test_adresse_cas_1_3_4 ='True'\n",
    "\"\"\"\n",
    "output = s3.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output=s3_output,\n",
    "    filename = None,\n",
    "    destination_key = None\n",
    "        )\n",
    "\n",
    "results = False\n",
    "filename_ligne = 'RESULTAT_CAS_2.csv'\n",
    "\n",
    "while results != True:\n",
    "    source_key = \"{}/{}.csv\".format(\n",
    "                                'INPI/sql_output',\n",
    "                                output['QueryExecutionId']\n",
    "                                       )\n",
    "    destination_key = \"{}/{}\".format(\n",
    "                                    'RESULTATS_SIRETISATION/TABLES_SIRET',\n",
    "                                    filename_ligne\n",
    "                                )\n",
    "\n",
    "    results = s3.copy_object_s3(\n",
    "                                    source_key = source_key,\n",
    "                                    destination_key = destination_key,\n",
    "                                    remove = True\n",
    "                                )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalement, nous allons créer un fichier json avec un résumé de la siretisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT COUNT(*) \n",
    "FROM ets_inpi_insee_cases_filtered \n",
    "WHERE index_id_duplicate = 'True' AND test_adresse_cas_1_3_4 ='True'\n",
    "\"\"\"\n",
    "total_index_found = s3.run_query(\n",
    "            query=query,\n",
    "            database='inpi',\n",
    "            s3_output=s3_output,\n",
    "    filename = 'cas_2_count_ets_inpi_insee_cases_found',\n",
    "    destination_key = None\n",
    "        )\n",
    "total_index_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_ = {\n",
    "    \n",
    "    'CAS': 'CAS_2',\n",
    "    'count_all_index':total_index.values[0][0],\n",
    "    'count_filtered_in': total_index_filtre.values[0][0],\n",
    "    'count_filtered_out':total_index_found.values[0][0],\n",
    "    'percentage_found': total_index_found.values[0][0]/total_index.values[0][0]\n",
    "}\n",
    "dic_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def convert(o):\n",
    "        if isinstance(o, np.int64): return int(o)  \n",
    "        raise TypeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LOGS_CAS_2.json', 'w') as outfile:\n",
    "    json.dump(dic_, outfile, default=convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.upload_file(file_to_upload = 'LOGS_CAS_2.json',\n",
    "            destination_in_s3 = 'RESULTATS_SIRETISATION/LOGS_ANALYSE')\n",
    "os.remove('LOGS_CAS_2.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, shutil, urllib, ipykernel, json\n",
    "from pathlib import Path\n",
    "from notebook import notebookapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_report(extension = \"html\"):\n",
    "    \"\"\"\n",
    "    Create a report from the current notebook and save it in the \n",
    "    Report folder (Parent-> child directory)\n",
    "    \n",
    "    1. Exctract the current notbook name\n",
    "    2. Convert the Notebook \n",
    "    3. Move the newly created report\n",
    "    \n",
    "    Args:\n",
    "    extension: string. Can be \"html\", \"pdf\", \"md\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ### Get notebook name\n",
    "    connection_file = os.path.basename(ipykernel.get_connection_file())\n",
    "    kernel_id = connection_file.split('-', 1)[0].split('.')[0]\n",
    "\n",
    "    for srv in notebookapp.list_running_servers():\n",
    "        try:\n",
    "            if srv['token']=='' and not srv['password']:  \n",
    "                req = urllib.request.urlopen(srv['url']+'api/sessions')\n",
    "            else:\n",
    "                req = urllib.request.urlopen(srv['url']+ \\\n",
    "                                             'api/sessions?token=' + \\\n",
    "                                             srv['token'])\n",
    "            sessions = json.load(req)\n",
    "            notebookname = sessions[0]['name']\n",
    "        except:\n",
    "            pass  \n",
    "    \n",
    "    sep = '.'\n",
    "    path = os.getcwd()\n",
    "    #parent_path = str(Path(path).parent)\n",
    "    \n",
    "    ### Path report\n",
    "    #path_report = \"{}/Reports\".format(parent_path)\n",
    "    #path_report = \"{}/Reports\".format(path)\n",
    "    \n",
    "    ### Path destination\n",
    "    name_no_extension = notebookname.split(sep, 1)[0]\n",
    "    source_to_move = name_no_extension +'.{}'.format(extension)\n",
    "    dest = os.path.join(path,'Reports', source_to_move)\n",
    "    \n",
    "    ### Generate notebook\n",
    "    os.system('jupyter nbconvert --no-input --to {} {}'.format(\n",
    "    extension,notebookname))\n",
    "    \n",
    "    ### Move notebook to report folder\n",
    "    #time.sleep(5)\n",
    "    shutil.move(source_to_move, dest)\n",
    "    print(\"Report Available at this adress:\\n {}\".format(dest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_report(extension = \"html\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
