{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Programme de Matching\n",
    "\n",
    "Regarder si l'établissement est fermé ou pas\n",
    "\n",
    "## Moteur de recherche TEST\n",
    "\n",
    "* Insee\n",
    "  * http://avis-situation-sirene.insee.fr/IdentificationListeSiret.action\n",
    "* INPI/TC\n",
    "  * https://data.inpi.fr/\n",
    "* Infogreffe\n",
    "  * https://www.infogreffe.fr/\n",
    "\n",
    "# Presentation Algorithme\n",
    "\n",
    "# Class preparation Data\n",
    "\n",
    "Le coude source est disponible [ici](https://github.com/thomaspernet/InseeInpi_matching/blob/dev_thomas/Notebooks_matching/programme_matching/inpi_insee/preparation_data.py) et le notebook pour lancer le programme est disponible [ici](https://github.com/thomaspernet/InseeInpi_matching/blob/dev_thomas/Notebooks_matching/programme_matching/Preparation_data.ipynb)\n",
    "\n",
    "## INSEE\n",
    "\n",
    "Les données sources de l'INSEE proviennent de [Data Gouv](https://www.data.gouv.fr/fr/datasets/base-sirene-des-entreprises-et-de-leurs-etablissements-siren-siret/)\n",
    "\n",
    "- communes_insee:\n",
    "    - Le fichier source pour les communes se trouvent à cette [URL](https://www.insee.fr/fr/information/3720946)\n",
    "    - Le notebook pour reconstituer le csv est disponible a cette [URL](https://github.com/thomaspernet/InseeInpi_matching/blob/dev_thomas/Notebooks_matching/programme_matching/Source_intermediates.ipynb). ⚠️ Repo privé + branche\n",
    "- voie:\n",
    "    - Le fichier source pour les communes se trouvent à cette [URL](https://www.sirene.fr/sirene/public/variable/libelleVoieEtablissement)\n",
    "    - Le notebook pour reconstituer le csv est disponible a cette [URL](https://github.com/thomaspernet/InseeInpi_matching/blob/dev_thomas/Notebooks_matching/programme_matching/Source_intermediates.ipynb). ⚠️ Repo privé + branche\n",
    "- upper_word:\n",
    "    - La liste des upper word (stop word capitalisé) provient de la librarie [NLTK](https://www.nltk.org/) avec un ajout manuel.\n",
    "    \n",
    "## INPI \n",
    "\n",
    "Les données de l'INPI proviennent de ses différents Notebooks:\n",
    "\n",
    "- [inpi_etb](https://github.com/thomaspernet/InseeInpi_matching/blob/dev_thomas/Notebooks_matching/programme_matching/Source_intermediates.ipynb)\n",
    "\n",
    "## Normalisation du fichier INPI.\n",
    "\n",
    "Le fichier INPI doit contenir un seul fichier gz avant d'être ingéré par le programme. Le fichier va être importé dans un format Dask, ce qui permet de paralléliser les calcules et bien sur d'éviter les problèmes de mémoire. \n",
    "\n",
    "La normalisation du fichier de l'INPI se fait en plusieurs étapes:\n",
    "\n",
    "1) Exclusion des observations contenant des NaN pour chacune des variables candidates, à savoir:\n",
    "\n",
    "    - Adresse_Ligne1\n",
    "    - Adresse_Ligne2\n",
    "    - Adresse_Ligne3\n",
    "    - Code_Postal\n",
    "    - Ville\n",
    "    - Code_Commune\n",
    "2) Extraction des SIREN a SIRETISER -> cela evite d'utiliser toute la base INSEE pour la sirétisation. I.e Speedup le process\n",
    "3) Calcule du nombre de SIRET par SIREN via la fonction `nombre_siret_siren`\n",
    "4) Normalisation de la variable commune via la fonction `clean_commune`\n",
    "\n",
    "    - Extraction des digits dans la ville. En effet, certaines communes incluent l'arrondissement dans la variable.\n",
    "    - Extraction des caractères spéciaux et espaces\n",
    "    - Capitalisation du nom de la commune\n",
    "    - Matching avec le fichier commune pour avoir le nom de la commune de l'INSEE.\n",
    "5) Préparation de l'adresse via la fonction `prepare_adress`\n",
    "    - Concatenation des variables `Adresse_Ligne1` + `Adresse_Ligne2` + `Adresse_Ligne3`\n",
    "    \n",
    "    - Normalisation de la variable concatenée -> Extraction des caractères speciaux, espace, digit puis capitalisation\n",
    "    - Extraction de tous les stop words du fichier `upper_word`\n",
    "    - Split de chaque mot restant de l'adresse \n",
    "    - Creation du regex de la forme suivante:  `MOT1$|MOT2$` \n",
    "    - Extration des digits:\n",
    "        - Première variable avec le premier digit\n",
    "        - Seconde variable avec une liste de digit et jointure -> DIGIT1|DIGIT2\n",
    "    - Merge avec le fichier `voie` pour obtenir le type de voie de l'INSEE\n",
    "    - Calcule du nombre de digit dans l'adresse\n",
    "        - Si len inférieure a 2, alors NaN. C'est une variable utlisée pendant le matching des règles spéciales\n",
    "    - Creation d'une variable `index` correspondant à l'index du dataframe. Indispensable\n",
    " \n",
    "Le fichier est sauvegardé en format gz, et dans un table SQL\n",
    "\n",
    "    - inpi_etb_stock_0.gz\n",
    "    - inpi_origine.db\n",
    "    \n",
    "Un appercu de la table est disponible via cette application `App_inpi`.\n",
    "\n",
    "## Normalisation du fichier INSEE\n",
    "\n",
    "Pour l'étape de siretisation, les variables candidates sont les suivantes:\n",
    "\n",
    "- 'siren',\n",
    "- 'siret',\n",
    "- \"etablissementSiege\",\n",
    "- \"etatAdministratifEtablissement\",\n",
    "- \"numeroVoieEtablissement\",\n",
    "- \"indiceRepetitionEtablissement\",\n",
    "- \"typeVoieEtablissement\",\n",
    "- \"libelleVoieEtablissement\",\n",
    "- \"complementAdresseEtablissement\",\n",
    "- \"codeCommuneEtablissement\",\n",
    "- \"libelleCommuneEtablissement\",\n",
    "- \"codePostalEtablissement\",\n",
    "- \"codeCedexEtablissement\",\n",
    "- \"libelleCedexEtablissement\",\n",
    "- \"distributionSpecialeEtablissement\",\n",
    "- \"libelleCommuneEtrangerEtablissement\",\n",
    "- \"codePaysEtrangerEtablissement\",\n",
    "- \"libellePaysEtrangerEtablissement\",\n",
    "- \"dateCreationEtablissement\"\n",
    "\n",
    "Comme pour le fichier de l'INPI, le fichier csv est importé en Dask Dataframe. Les étapes sont les suivantes:\n",
    "\n",
    "1) Filtre les SIREN à sirétiser uniquement\n",
    "\n",
    "2) Filtre la date limite à l'INSEE. Cette étape sert essentiellement pour siretiser les bases de stocks. Cela évite d'utiliser des valeurs \"dans le future\" -> inconnu à l'INPI\n",
    "\n",
    "3) Remplacement des \"-\" par des \" \" dans la variable `libelleCommuneEtablissement`\n",
    "\n",
    "4) Extraction des digits en format liste de la variable `libelleVoieEtablissement`\n",
    "\n",
    "5) Calcule du nombre de SIRET par SIREN\n",
    "\n",
    "6) Calcule du nombre de digit dans la variable `libelleCommuneEtablissement`\n",
    "\n",
    "    - Si len inférieure a 2, alors NaN. C'est une variable utlisée pendant le matching des règles spéciales\n",
    "    \n",
    "Le fichier est sauvegardé en format gz, et dans un table SQL\n",
    "\n",
    "    - insee_2017_SIZE.gz\n",
    "    - App_insee.db\n",
    "    \n",
    "Un appercu de la table est disponible via cette application `App_insee`.\n",
    "\n",
    "# Algorithme Siretisation\n",
    "\n",
    "Le code source est disponible [ici](https://github.com/thomaspernet/InseeInpi_matching/blob/dev_thomas/Notebooks_matching/programme_matching/inpi_insee/siretisation.py) et le notebook pour lancer le programme est disponible [ici](https://github.com/thomaspernet/InseeInpi_matching/blob/dev_thomas/Notebooks_matching/programme_matching/Siretisation.ipynb)\n",
    "\n",
    "L'algorithme de SIRETISATION fonctionne avec l'aidre de trois fonctions:\n",
    "\n",
    "- `step_one`: permet d'écarter les doublons du merge et d'appliquer les premières règles afin de connaitre l'origine de la siretisation\n",
    "- `step_two_assess_test`: détermine l'origine du matching, a savoir la date, adresse, voie, numéro de voie\n",
    "- `step_two_duplication`: permet de récuperer des SIRET sur les doublons émanant du merge avec l'INSEE\n",
    "\n",
    "Dans premier temps, on créer un dictionnaire avec toutes les variables de matching. Toutefois, l'algorithme va utiliser séquentiellement les variables suivantes:\n",
    "\n",
    "```\n",
    " {'ncc', 'Code_Postal', 'Code_Commune', 'INSEE', 'digit_inpi'},\n",
    " {'ncc', 'Code_Postal', 'Code_Commune', 'INSEE'},\n",
    " {'ncc', 'Code_Postal', 'Code_Commune', 'digit_inpi'},\n",
    " {'ncc', 'Code_Postal', 'Code_Commune'},   \n",
    " {'ncc', 'Code_Postal'},\n",
    " {'ncc'},\n",
    " {'Code_Postal'},\n",
    " {'Code_Commune'}\n",
    "```\n",
    "\n",
    "Pour connaitre l'ensemble des variables de matching INSEE/INPI, veuillez vous rendre en [annexe](#annexe).\n",
    "\n",
    "Dans la mesure ou l'algorithme fonctionne de manière séquentielle, et utilise comme input un fichier de l'INPI a siretiser. De fait, après chaque séquence, l'algorithme sauvegarde un fichier gz contenant les siren a trouver. Cette étape de sauvegarde en gz permet de loader le fichier gz en input en Dataframe Dask.\n",
    "\n",
    "## Step One\n",
    "\n",
    "La première étape de la séquence est l'ingestion d'un fichier gz contenant les SIREN a trouver. L'ingestion va se faire en convertissant le dataframe en Dask. L'algorithme tout d'abord utiliser la fonction `step_one` et produit deux dataframes selon si le matching avec l'INSEE a débouté sur des doublons ou non. \n",
    "\n",
    "Les doublons sont générés si pour un même nombre de variables de matching, il existe plusieurs possibilités à l'INSEE. Par exemple, pour un siren, ville, adressse donnée, il y a plusieurs possibilité. Cela constitue un double et il sera traiter ultérieurement, dans la mesure du possible. \n",
    "\n",
    "Les étapes déroulées lors du premier processus est le suivant:\n",
    "\n",
    "```\n",
    "- Test 1: doublon\n",
    "        - non: Save-> `test_1['not_duplication']`\n",
    "        - oui:\n",
    "            - Test 2: Date equal\n",
    "                - oui:\n",
    "                    - Test 2 bis: doublon\n",
    "                        - non: Save-> `test_2_bis['not_duplication']`\n",
    "                        - oui: Save-> `test_2_bis['duplication']`\n",
    "                - non:\n",
    "                    - Test 3: Date sup\n",
    "                        - oui:\n",
    "                            - Test 2 bis: doublon\n",
    "                                - non: Save-> `test_3_oui_bis['not_duplication']`\n",
    "                                - oui: Save-> `test_3_oui_bis['duplication']`\n",
    "                        - non: Save-> `test_3_non`\n",
    "```\n",
    "\n",
    "Deux dataframe sont crées, un ne contenant pas de doublon pas les doublons et un deuxième contenant les doublon. L'algorithme va réaliser les tests sur le premier et faire d'avantage de recherche sur le second\n",
    "\n",
    "## step_two_assess_test\n",
    "\n",
    "Le premier dataframe ne contient pas de doublon, il est donc possible de réaliser différents tests afin de mieux déterminer\n",
    "l'origine du matching. Plus précisement, si le matching a pu se faire sur la date, l'adresse, la voie, numéro de voie et le nombre unique d'index. Les règles sont définies ci-dessous.\n",
    "\n",
    "```\n",
    "- Test 1: address libelle\n",
    "            - Si mots dans inpi est contenu dans INSEE, True\n",
    "        - Test 1 bis: address complement\n",
    "            - Si mots dans inpi est contenu dans INSEE, True\n",
    "        - Test 2: Date\n",
    "            - dateCreationEtablissement >= Date_Début_Activité OR\n",
    "            Date_Début_Activité = NaN OR (nombre SIREN a l'INSEE = 1 AND nombre\n",
    "            SIREN des variables de matching = 1), True\n",
    "        - Test 3: siege\n",
    "            - Type = ['SEP', 'SIE'] AND siege = true, True\n",
    "        - Test 4: voie\n",
    "            - Type voie INPI = Type voie INSEE, True\n",
    "        - Test 5: numero voie\n",
    "            - Numero voie INPI = Numero voie INSEE, True\n",
    "```\n",
    "\n",
    "Un premier fichier gz est enregistré contenant les \"pure matches\"\n",
    "\n",
    "## step_two_duplication\n",
    "\n",
    "Les second dataframe contient les doublons obtenus après le matching avec l'INSEE. L'algorithme va travailler sur différentes variables de manière séquencielle pour tenter de trouver les bons siret. Plus précisément, 3 variables qui ont été récemment créé sont utilisées:\n",
    "\n",
    "- test_join_address -> True si la variable test_address_libelle = True (ie mot INPI trouvé dans INSEE) et test_join_address =  True\n",
    "- test_address_libelle ->  True si la variable test_address_libelle = True (ie mot INPI trouvé dans INSEE)\n",
    "- test_address_complement -> True si la variable test_join_address =  True\n",
    "\n",
    "Pour chaque séquence, on réalise les tests suivants:\n",
    "\n",
    "```\n",
    "- Si test_join_address = True:\n",
    "        - Test 1: doublon:\n",
    "            - Oui: append-> `df_not_duplicate`\n",
    "            - Non: Pass\n",
    "            - Exclue les `index` de df_duplication\n",
    "            - then go next\n",
    "        - Si test_address_libelle = True:\n",
    "            - Test 1: doublon:\n",
    "                - Oui: append-> `df_not_duplicate`\n",
    "                - Non: Pass\n",
    "                - Exclue les `index` de df_duplication\n",
    "                - then go next\n",
    "        - Si test_address_complement = True:\n",
    "            - Test 1: doublon:\n",
    "                - Oui: append-> `df_not_duplicate`\n",
    "                - Non: Pass\n",
    "                - Exclue les `index` de df_duplication\n",
    " ```\n",
    " \n",
    "On peut sauvegarder le `df_not_duplicate` et le restant en tant que `special_treatment`\n",
    "\n",
    "\n",
    "![](https://www.lucidchart.com/publicSegments/view/5a8cb28f-dc42-4708-babd-423962514878/image.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Algorithme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "current_dir = os.getcwd()\n",
    "from inpi_insee import siretisation\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "param = {\n",
    "    'insee': r'data\\input\\INSEE\\insee_2017_7627977.gz'\n",
    "}\n",
    "# 4824158 SIREN a trouver!\n",
    "al_siret = siretisation.siretisation_inpi(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#from itertools import compress, product\n",
    "\n",
    "#def combinations(items):\n",
    "#    return ( set(compress(items,mask)) for \n",
    "#            mask in product(*[[0,1]]*len(items)))\n",
    "\n",
    "#all_list = ['ncc',\n",
    "#             'Code_Postal','Code_Commune',\n",
    "#             'INSEE','digit_inpi']\n",
    "#test = list(combinations(items = all_list))[1:]\n",
    "#sort_list = sorted(test[1:], key=lambda k: len(k), reverse=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "list_inpi = ['ncc','code_postal','code_commune','insee','digit_inpi']\n",
    "list_insee = ['libelleCommuneEtablissement',\n",
    "            'codePostalEtablissement', 'codeCommuneEtablissement',\n",
    "            'typeVoieEtablissement','numeroVoieEtablissement']\n",
    "\n",
    "sort_list = [\n",
    " {'ncc', 'code_postal', 'code_commune', 'insee', 'digit_inpi'},\n",
    " {'ncc', 'code_postal', 'code_commune', 'insee'},\n",
    " {'ncc', 'code_postal', 'code_commune', 'digit_inpi'},\n",
    " {'ncc', 'code_postal', 'code_commune'},   \n",
    " {'ncc', 'code_postal'},\n",
    " {'ncc'},\n",
    " {'code_postal'},\n",
    " {'code_commune'}\n",
    "]\n",
    "len(sort_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "list_possibilities = []\n",
    "for i in sort_list:\n",
    "    left =[]\n",
    "    right = []\n",
    "    for j in i:\n",
    "        left.append(j)\n",
    "        right.append(list_insee[list_inpi.index(j)])\n",
    "    left.insert(0,'siren')\n",
    "    right.insert(0,'siren')\n",
    "    \n",
    "    dic_ = {\n",
    "    'match':{\n",
    "        'inpi':left,\n",
    "        'insee':right,\n",
    "    }\n",
    "}\n",
    "    list_possibilities.append(dic_)\n",
    "list_possibilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indiquer le fichiers a siretiser. Si pas en local, le télécharger depuis le S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'inpi_stock_initial_etb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsPy.aws_authorization import aws_connector\n",
    "from awsPy.aws_s3 import service_s3\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "bucket = 'calfdata'\n",
    "path = os.getcwd()\n",
    "parent_path = str(Path(path).parent)\n",
    "path_cred = r\"{}/programme_matching/credential_AWS.json\".format(parent_path)\n",
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                        region = 'eu-west-3')\n",
    "client= con.client_boto()\n",
    "s3 = service_s3.connect_S3(client = client,\n",
    "                      bucket = 'calfdata') \n",
    "s3.download_file(\n",
    "    key= 'INPI/TC_1/02_preparation_donnee/Stock/ETB/{}_0.gz'.format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "try:\n",
    "    os.remove(r\"data\\input\\INPI\\{}_0.gz'.format(filename))\n",
    "except:\n",
    "    pass\n",
    "shutil.move(r\"{}_0.gz\".format(filename),\n",
    "            r\"data\\input\\INPI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "inpi_col = ['siren',\n",
    "            'index',\n",
    "            'type',\n",
    "            'code_postal',\n",
    "            'ville',\n",
    "            'code_commune',\n",
    "            'pays',\n",
    "            'count_initial_inpi',\n",
    "            'ncc',\n",
    "            'adresse_new_clean_reg',\n",
    "            'adress_new',\n",
    "            'insee',\n",
    "            'date_début_activité',\n",
    "            'digit_inpi',\n",
    "            'len_digit_address_inpi',\n",
    "            'list_digit_inpi'\n",
    "            ]\n",
    "\n",
    "inpi_dtype = {\n",
    "    'siren': 'object',\n",
    "    'index': 'int',\n",
    "    'type': 'object',\n",
    "    'code_postal': 'object',\n",
    "    'ville': 'object',\n",
    "    'code_commune': 'object',\n",
    "    'pays': 'object',\n",
    "    'count_initial_inpi': 'int',\n",
    "    'ncc': 'object',\n",
    "    'adresse_new_clean_reg': 'object',\n",
    "    'adress_new':'object',\n",
    "    'insee': 'object',\n",
    "    'date_début_activité': 'object',\n",
    "    'digit_inpi': 'object',\n",
    "    'len_digit_address_inpi':'object'\n",
    "}\n",
    "\n",
    "for key, values in enumerate(list_possibilities):\n",
    "    df_ets = r'data\\input\\INPI\\{}_{}.gz'.format(filename, key)\n",
    "\n",
    "    inpi = al_siret.import_dask(file=df_ets,\n",
    "                                usecols=inpi_col,\n",
    "                                dtype=inpi_dtype,\n",
    "                                parse_dates=False)\n",
    "\n",
    "    df_no_duplication, df_duplication = al_siret.step_one(\n",
    "        df_input=inpi,\n",
    "        left_on=values['match']['inpi'],\n",
    "        right_on=values['match']['insee']\n",
    "    )\n",
    "\n",
    "    # Step 2: No duplication\n",
    "    pure_match = al_siret.step_two_assess_test(df=df_no_duplication,\n",
    "                                               var_group=values['match']['inpi'])\n",
    "\n",
    "    pure_match.to_csv(r'data\\output\\{}_{}_pure_match.gz'.format(key, filename),\n",
    "                      compression='gzip', index= False)\n",
    "    # Step 2: duplication\n",
    "    df_not_duplicate, sp = al_siret.step_two_duplication(df_duplication,\n",
    "                                                        var_group = \n",
    "                                                         values['match']['inpi'])\n",
    "    \n",
    "    (df_not_duplicate\n",
    "        .to_csv(r'data\\output\\{}_{}_not_duplicate.gz'.format(key, filename),\n",
    "                compression='gzip', index= False))\n",
    "\n",
    "    (sp.to_csv(\n",
    "        r'data\\input\\INPI\\special_treatment\\{}_{}_special_treatment.gz'.format(\n",
    "        key, filename),compression='gzip', index= False))\n",
    "\n",
    "    # Input -> Save for the next loop \n",
    "    inpi.loc[\n",
    "        (~inpi['index'].isin(pure_match['index'].unique()))\n",
    "        & (~inpi['index'].isin(df_not_duplicate['index'].unique()))\n",
    "        & (~inpi['index'].isin(sp['index'].unique()))\n",
    "    ].compute().to_csv(r'data\\input\\INPI\\{}_{}.gz'.format(filename, key+1),\n",
    "                       compression='gzip', index= False)\n",
    "\n",
    "    #### Creation LOG\n",
    "    if key ==0:\n",
    "        total_to_siret_intial = inpi.compute().shape[0]\n",
    "        total_siren_initial = inpi.compute()['siren'].nunique()\n",
    "    \n",
    "    ### Total rows in df inpi to match\n",
    "    total_to_siret_current = inpi.compute().shape[0]\n",
    "    total_siren_current = inpi.compute()['siren'].nunique() # unique siren \n",
    "    \n",
    "    ### DF with no duplication after merge INSEE\n",
    "    total_rows_no_dup = df_no_duplication[\"index\"].nunique()\n",
    "    total_rows_no_dup_unique_siren = df_no_duplication['siren'].nunique()\n",
    "    \n",
    "    ### DF with duplication after merge INSEE\n",
    "    total_rows_dup = df_duplication[\"index\"].nunique() # total duplication\n",
    "    total_rows_dup_unique_siren = df_duplication[\"siren\"].nunique()\n",
    "    \n",
    "    total_rows_dup_matched = df_not_duplicate[\"index\"].nunique() #no duplication\n",
    "    total_rows_dup_matched_unique_siren = df_not_duplicate[\"siren\"].nunique()\n",
    "    \n",
    "    total_rows_dup_not_matched = sp[\"index\"].nunique() # special treatmnent\n",
    "    total_rows_dup_not_matched_unique_siren = sp[\"siren\"].nunique()\n",
    "    \n",
    "    ### compare with initial\n",
    "    total_match_rows_current = total_rows_no_dup + total_rows_dup_matched\n",
    "    perc_total_match_rows_initial = total_match_rows_current / \\\n",
    "    total_to_siret_intial\n",
    "    \n",
    "    total_match_siren_current = total_rows_no_dup_unique_siren + \\\n",
    "    total_rows_dup_matched_unique_siren\n",
    "    \n",
    "    perc_total_match_siren_initial = total_match_siren_current / \\\n",
    "    total_siren_initial \n",
    "    \n",
    "    ### compare with current\n",
    "    perc_total_match_rows_current = total_match_rows_current / \\\n",
    "    total_to_siret_current\n",
    "\n",
    "    perc_total_match_siren_current = total_match_siren_current / \\\n",
    "    total_siren_current\n",
    "    \n",
    "    \n",
    "    dic_ = {\n",
    "        'key':key,\n",
    "        'total_to_siret_intial':total_to_siret_intial,\n",
    "        'total_stotal_siren_initialiren': total_siren_initial,\n",
    "        'total_to_siret_current':total_to_siret_current,\n",
    "        'total_siren_current': total_siren_current,\n",
    "        'total_match_rows_current':total_match_rows_current,\n",
    "        'perc_total_match_rows_initial':perc_total_match_rows_initial,\n",
    "        'total_match_siren_current':total_match_siren_current,\n",
    "        'perc_total_match_siren_initial':perc_total_match_siren_initial,\n",
    "        'perc_total_match_rows_current':perc_total_match_rows_current,\n",
    "        'perc_total_match_siren_current':perc_total_match_siren_current,\n",
    "        'df_no_duplication': {\n",
    "            'nb_index': total_rows_no_dup,\n",
    "            'unique_siren':total_rows_no_dup_unique_siren\n",
    "        },\n",
    "        'df_duplication': {\n",
    "            'nb_index': total_rows_dup,\n",
    "            'unique_siren':total_rows_dup_unique_siren,\n",
    "            'df_not_duplicate_index': {\n",
    "                'nb_index':total_rows_dup_matched,\n",
    "               'unique_siren':total_rows_dup_unique_siren\n",
    "            },\n",
    "            'df_sp_index': {\n",
    "                'nb_index':total_rows_dup_not_matched,\n",
    "               'unique_siren':total_rows_dup_not_matched_unique_siren\n",
    "            }\n",
    "        },\n",
    "        'check': total_to_siret_current -\n",
    "        total_rows_no_dup +\n",
    "        total_rows_dup\n",
    "    }\n",
    "\n",
    "    with open(r'data\\logs\\{}_{}_logs.json'.format(key,filename), 'w') as f:\n",
    "        json.dump(dic_, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Special Treatment\n",
    "\n",
    "exemple:\n",
    "\n",
    "- 752085324\n",
    "- 342122546: libelle type dans l'adresse complementaire a l'insee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Recuperation via list_digit_ INPI/INSEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "for i in range(0,8):\n",
    "    test = pd.read_csv(\n",
    "        r'data\\input\\INPI\\special_treatment\\{}_special_treatment.gz'.format(i),\n",
    "                   low_memory=False)\n",
    "    print((test\n",
    " .loc[lambda x:\n",
    "      (x['list_digit_inpi'] == x['list_digit_insee'])\n",
    "     ]\n",
    "     )['siret'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\n",
    "        r'data\\input\\INPI\\special_treatment\\{}_special_treatment.gz'.format(1),\n",
    "                   low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "test['siren'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "97*97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "test.loc[lambda x: \n",
    "         x['siren'].isin(['752085324'])\n",
    "        & (x['list_digit_inpi'] == x['list_digit_insee'])\n",
    "        ][['list_digit_inpi', 'list_digit_insee', 'siret']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#.assign(special_digit = lambda x:x['libelleVoieEtablissement'].str.findall(r\"(\\d+)\").apply(\n",
    "#        lambda x:'&'.join([i for i in x])))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
