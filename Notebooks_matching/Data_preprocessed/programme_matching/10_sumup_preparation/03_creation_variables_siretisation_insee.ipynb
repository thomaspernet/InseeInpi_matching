{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKJUtoJnDIUZ",
    "lines_to_next_cell": 0
   },
   "source": [
    "# Création table INSEE transformée contenant les nouvelles variables permettant la siretisation\n",
    "\n",
    "# Objective(s)\n",
    "\n",
    "*  Creation de la table INSEE avec les data de juillet\n",
    "* Création des variables pour faire les tests de siretisation\n",
    "* Please, update the Source URL by clicking on the button after the information have been pasted\n",
    "  * US 03 Creation Variables data INPI et INSEE Modify rows\n",
    "  * Delete tables and Github related to the US: Delete rows\n",
    "\n",
    "# Metadata\n",
    "\n",
    "* Epic: Epic 6\n",
    "* US: US 4\n",
    "* Date Begin: 9/28/2020\n",
    "* Duration Task: 1\n",
    "* Description: Creation des variables qui vont servir a réaliser les tests pour la siretisation\n",
    "* Step type: Transform table\n",
    "* Status: Active\n",
    "  * Change Status task: Active\n",
    "  * Update table: Modify rows\n",
    "* Source URL: US 03 Creation Variables data INPI et INSEE\n",
    "* Task type: Jupyter Notebook\n",
    "* Users: Thomas Pernet\n",
    "* Watchers: Thomas Pernet\n",
    "* User Account: https://937882855452.signin.aws.amazon.com/console\n",
    "* Estimated Log points: 10\n",
    "* Task tag: #athena,#lookup-table,#sql,#data-preparation,#insee\n",
    "* Toggl Tag: #documentation\n",
    "\n",
    "# Input Cloud Storage [AWS/GCP]\n",
    "\n",
    "## Table/file\n",
    "\n",
    "* Origin: \n",
    "* Athena\n",
    "* Name: \n",
    "* ets_insee_raw_juillet\n",
    "* Github: \n",
    "  * \n",
    "\n",
    "# Destination Output/Delivery\n",
    "\n",
    "## Table/file\n",
    "\n",
    "* Origin: \n",
    "* Athena\n",
    "* Name:\n",
    "* ets_insee_transformed\n",
    "* GitHub:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "h4Umnv0nDIUa",
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "from awsPy.aws_authorization import aws_connector\n",
    "from awsPy.aws_s3 import service_s3\n",
    "from awsPy.aws_glue import service_glue\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os, shutil, json\n",
    "\n",
    "path = os.getcwd()\n",
    "parent_path = str(Path(path).parent)\n",
    "path_cred = r\"{}/credential_AWS.json\".format(parent_path)\n",
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                       region = 'eu-west-3')\n",
    "\n",
    "region = 'eu-west-3'\n",
    "bucket = 'calfdata'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "tTZePIXWDIUf",
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                       region = region)\n",
    "client= con.client_boto()\n",
    "s3 = service_s3.connect_S3(client = client,\n",
    "                      bucket = bucket, verbose = True) \n",
    "glue = service_glue.connect_glue(client = client) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "er-S8VWyDIUi"
   },
   "outputs": [],
   "source": [
    "pandas_setting = True\n",
    "if pandas_setting:\n",
    "    cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bea5ESvKDIUm"
   },
   "source": [
    "# Etape création table tansformée INSEE\n",
    "\n",
    "La préparation de la table transformée de l'INSEE se fait en deux étapes. La première étape consiste bien sur à intégrer dans la base de donnée, la table brute de l'INSEE. Nous utiliserons la table datant de juillet 2020, pour correspondre avec celle de l'équipe Datum.\n",
    "\n",
    "Dans un second temps, nous allons 6 variables, qui sont résumés dans le tableau ci dessous\n",
    "\n",
    "| Tables | Variables                          | Commentaire                                                                                                                                                                                                        | Bullet_inputs                                                                                                                 | Bullet_point_regex                                     | Inputs                                                                                                                        | US_md                                                          | query_md_gitlab                                                                                                                                                                                                                                                              | Pattern_regex                                          |\n",
    "|--------|------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------|\n",
    "| INSEE  | voie_clean                         | Extraction du type de voie contenu dans l’adresse. Variable type voie nom complet. Exemple, l'INSEE indique CH, pour chemin, il faut donc indiquer CHEMIN. Besoin table externe (type_voie) pour créer la variable |                                                                                                                               |                                                        |                                                                                                                               | [2953](https://tree.taiga.io/project/olivierlubet-air/us/2953) | [etape-1-pr%C3%A9paration-voie_clean](https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/04_ETS_add_variables_insee.md#etape-1-pr%C3%A9paration-voie_clean)                     |                                                        |\n",
    "| INSEE  | indiceRepetitionEtablissement_full | Récupération du nom complet des indices de répétion; par exemple B devient BIS, T devient TER                                                                                                                      | indiceRepetitionEtablissement                                                                                                 | Regles_speciales                                       | indiceRepetitionEtablissement                                                                                                 | [2953](https://tree.taiga.io/project/olivierlubet-air/us/2953) | []()                                                                                                                                                                                                                                                                         | Regles_speciales                                       |\n",
    "| INSEE  | adresse_reconstituee_insee         | Concatenation des champs de l'adresse et suppression des espace                                                                                                                                                    | numeroVoieEtablissement indiceRepetitionEtablissement_full voie_clean libelleVoieEtablissement complementAdresseEtablissement | debut/fin espace espace Upper                          | numeroVoieEtablissement,indiceRepetitionEtablissement_full,voie_clean,libelleVoieEtablissement,complementAdresseEtablissement | [2954](https://tree.taiga.io/project/olivierlubet-air/us/2954) | [etape-2-preparation-adress_reconstituee_insee](https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/04_ETS_add_variables_insee.md#etape-2-preparation-adress_reconstituee_insee) | debut/fin espace,espace,Upper                          |\n",
    "| INSEE  | adresse_distance_insee             | Concatenation des champs de l'adresse, suppression des espaces et des articles. Utilisé pour calculer le score permettant de distinguer la similarité/dissimilarité entre deux adresses (INPI vs INSEE)            | numeroVoieEtablissement indiceRepetitionEtablissement_full voie_clean libelleVoieEtablissement complementAdresseEtablissement | article digit debut/fin espace espace Upper            | numeroVoieEtablissement,indiceRepetitionEtablissement_full,voie_clean,libelleVoieEtablissement,complementAdresseEtablissement | [3004](https://tree.taiga.io/project/olivierlubet-air/us/3004) | [etape-3-adresse_distance_insee](https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/04_ETS_add_variables_insee.md#etape-3-adresse_distance_insee)                               | article,digit,debut/fin espace,espace,Upper            |\n",
    "| INSEE  | list_numero_voie_matching_insee    | Liste contenant tous les numéros de l'adresse dans l'INSEE                                                                                                                                                         | numeroVoieEtablissement indiceRepetitionEtablissement_full voie_clean libelleVoieEtablissement complementAdresseEtablissement | article digit debut/fin espace                         | numeroVoieEtablissement,indiceRepetitionEtablissement_full,voie_clean,libelleVoieEtablissement,complementAdresseEtablissement | [3004](https://tree.taiga.io/project/olivierlubet-air/us/3004) | [etape-4-creation-liste-num%C3%A9ro-de-voie](https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/04_ETS_add_variables_insee.md#etape-4-creation-liste-num%C3%A9ro-de-voie)       | article,digit,debut/fin espace                         |\n",
    "| INSEE  | ville_matching                     | Nettoyage regex de la ville et suppression des espaces. La même logique de nettoyage est appliquée coté INPI                                                                                                       | libelleCommuneEtablissement                                                                                                   | article digit debut/fin espace espace Regles_speciales | libelleCommuneEtablissement                                                                                                   | [2954](https://tree.taiga.io/project/olivierlubet-air/us/2954) | [etape-2-cr%C3%A9ation-ville_matching](https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/04_ETS_add_variables_insee.md#etape-2-cr%C3%A9ation-ville_matching)                   | article,digit,debut/fin espace,espace,Regles_speciales |\n",
    "| INSEE  | count_initial_insee                | Compte du nombre de siret (établissement) par siren (entreprise)                                                                                                                                                   | siren                                                                                                                         |                                                        | siren                                                                                                                         | [2955](https://tree.taiga.io/project/olivierlubet-air/us/2955) | [etape-5-count_initial_insee](https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/04_ETS_add_variables_insee.md#etape-5-count_initial_insee)                                     |                                                        |\n",
    "    \n",
    "## Prepare `TABLE.CREATION` parameters\n",
    "    \n",
    "Le fichier config JSON contient déjà les étapes de préparation de l'INPI. Nous allons continuer d'ajouter les queries a éxécuter dans le JSON afin d'avoir un processus complet contenu dans un seul est même fichier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U-PscJ-YDIUm"
   },
   "outputs": [],
   "source": [
    "### If chinese characters, set  ensure_ascii=False\n",
    "s3.download_file(key = 'DATA/ETL/parameters_ETL.json')\n",
    "with open('parameters_ETL.json', 'r') as fp:\n",
    "    parameters = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S482exQ8DIUp"
   },
   "outputs": [],
   "source": [
    "parameters['TABLES']['CREATION']['ALL_SCHEMA'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxyFXf8iDIUs"
   },
   "source": [
    "## 2. Prepare `TABLES.CREATION`\n",
    "\n",
    "This part usually starts with raw/transformed data in S3. The typical architecture in the S3 is:\n",
    "\n",
    "- `DATA/RAW_DATA` or `DATA/UNZIP_DATA_APPEND_ALL` or `DATA/TRANSFORMED`. One of our rule is, if the user needs to create a table from a CSV/JSON (raw or transformed), then the query should be written in the key `TABLES.CREATION` and the notebook in the folder `01_prepare_tables`\n",
    "\n",
    "One or more notebooks in the folder `01_prepare_tables` are used to create the raw tables. Please, use the notebook named `XX_template_table_creation_AWS` to create table using the key `TABLES.CREATION`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_0NnpDD-DIUt"
   },
   "outputs": [],
   "source": [
    "table_raw_insee = [{\n",
    "    \"database\": \"ets_insee\",\n",
    "    \"name\": \"ets_insee_raw_juillet\",\n",
    "    \"output_id\": \"\",\n",
    "    \"separator\": \",\",\n",
    "    \"s3URI\": \"s3://calfdata/INSEE/00_rawData/ETS_01_07_2020\",\n",
    "    \"schema\": [\n",
    "        {'Name': 'siren', 'Type': 'string', 'Comment': ''},\n",
    " {'Name': 'nic', 'Type': 'string', 'Comment': ''},\n",
    " {'Name': 'siret', 'Type': 'string', 'Comment': ''},\n",
    " {'Name': 'statutdiffusionetablissement', 'Type': 'string', 'Comment': ''},\n",
    " {'Name': 'datecreationetablissement', 'Type': 'string', 'Comment': ''},\n",
    " {'Name': 'trancheeffectifsetablissement', 'Type': 'string', 'Comment': ''},\n",
    " {'Name': 'anneeeffectifsetablissement', 'Type': 'string', 'Comment': ''},\n",
    " {'Name': 'activiteprincipaleregistremetiersetablissement',\n",
    "  'Type': 'string',\n",
    "  'Comment': ''},\n",
    " {'Name': 'datederniertraitementetablissement',\n",
    "  'Type': 'string',\n",
    "  'Comment': ''},\n",
    " {'Name': 'etablissementsiege', 'Type': 'string', 'Comment': ''},\n",
    " {'Name': 'nombreperiodesetablissement', 'Type': 'string', 'Comment': ''},\n",
    " {'Name': 'complementadresseetablissement', 'Type': 'string', 'Comment': ''},\n",
    " {'Name': 'numerovoieetablissement', 'Type': 'string', 'Comment': ''},\n",
    " {'Name': 'indicerepetitionetablissement', 'Type': 'string', 'Comment': ''},\n",
    " {'Name': 'typevoieetablissement', 'Type': 'string', 'Comment': ''},\n",
    " {'Name': 'libellevoieetablissement', 'Type': 'string', 'Comment': ''},\n",
    " {'Name': 'codepostaletablissement', 'Type': 'string', 'Comment': ''},\n",
    " {'Name': 'libellecommuneetablissement', 'Type': 'string', 'Comment': ''},\n",
    " {'Name': 'libellecommuneetrangeretablissement',\n",
    "  'Type': 'string',\n",
    "  'Comment': ''},\n",
    " {'Name': 'distributionspecialeetablissement',\n",
    "  'Type': 'string',\n",
    "  'Comment': ''},\n",
    " {'Name': 'codecommuneetablissement', 'Type': 'string', 'Comment': ''},\n",
    " {'Name': 'codecedexetablissement', 'Type': 'string', 'Comment': ''},\n",
    " {'Name': 'libellecedexetablissement', 'Type': 'string', 'Comment': ''},\n",
    " {'Name': 'codepaysetrangeretablissement', 'Type': 'string', 'Comment': ''},\n",
    " {'Name': 'libellepaysetrangeretablissement', 'Type': 'string', 'Comment': ''},\n",
    " {'Name': 'complementadresse2etablissement', 'Type': 'string', 'Comment': ''},\n",
    " {'Name': 'numerovoie2etablissement', 'Type': 'string', 'Comment': ''},\n",
    " {'Name': 'indicerepetition2etablissement', 'Type': 'string', 'Comment': ''},\n",
    " {'Name': 'typevoie2etablissement', 'Type': 'string', 'Comment': ''},\n",
    " {'Name': 'libellevoie2etablissement', 'Type': 'string', 'Comment': ''},\n",
    " {'Name': 'codepostal2etablissement', 'Type': 'string', 'Comment': ''},\n",
    " {'Name': 'libellecommune2etablissement', 'Type': 'string', 'Comment': ''},\n",
    " {'Name': 'libellecommuneetranger2etablissement',\n",
    "  'Type': 'string',\n",
    "  'Comment': ''},\n",
    " {'Name': 'distributionspeciale2etablissement',\n",
    "  'Type': 'string',\n",
    "  'Comment': ''},\n",
    " {'Name': 'codecommune2etablissement', 'Type': 'string', 'Comment': ''},\n",
    " {'Name': 'codecedex2etablissement', 'Type': 'string', 'Comment': ''},\n",
    " {'Name': 'libellecedex2etablissement', 'Type': 'string', 'Comment': ''},\n",
    " {'Name': 'codepaysetranger2etablissement', 'Type': 'string', 'Comment': ''},\n",
    " {'Name': 'libellepaysetranger2etablissement',\n",
    "  'Type': 'string',\n",
    "  'Comment': ''},\n",
    " {'Name': 'datedebut', 'Type': 'string', 'Comment': ''},\n",
    " {'Name': 'etatadministratifetablissement', 'Type': 'string', 'Comment': ''},\n",
    " {'Name': 'enseigne1etablissement', 'Type': 'string', 'Comment': ''},\n",
    " {'Name': 'enseigne2etablissement', 'Type': 'string', 'Comment': ''},\n",
    " {'Name': 'enseigne3etablissement', 'Type': 'string', 'Comment': ''},\n",
    " {'Name': 'denominationusuelleetablissement', 'Type': 'string', 'Comment': ''},\n",
    " {'Name': 'activiteprincipaleetablissement', 'Type': 'string', 'Comment': ''},\n",
    " {'Name': 'nomenclatureactiviteprincipaleetablissement',\n",
    "  'Type': 'string',\n",
    "  'Comment': ''},\n",
    " {'Name': 'caractereemployeuretablissement', 'Type': 'string', 'Comment': ''}\n",
    "    ]\n",
    "}\n",
    "]\n",
    "#len(parameters['TABLES']['CREATION']['ALL_SCHEMA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filte = \"42dcc900-50a8-4ab5-83e2-9778ca6fea72.csv\"\n",
    "#list_ = []\n",
    "#for i in list(pd.read_csv(filte).columns):\n",
    "#    list_.append(\n",
    "#    {'Name': i, 'Type': 'string', 'Comment': ''}\n",
    "#    )\n",
    "#list_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwcsoOPCDIUx"
   },
   "source": [
    "To remove an item from the list, use `pop` with the index to remove. Exemple `parameters['TABLES']['CREATION']['ALL_SCHEMA'].pop(6)` will remove the 5th item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZV3cHbKLDIUx"
   },
   "outputs": [],
   "source": [
    "to_remove = False\n",
    "if to_remove:\n",
    "    parameters['TABLES']['CREATION']['ALL_SCHEMA'].pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "auvdEPNLDIU1"
   },
   "outputs": [],
   "source": [
    "parameters['TABLES']['CREATION']['ALL_SCHEMA'].extend(table_raw_insee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-mmdbMCoDIU4"
   },
   "outputs": [],
   "source": [
    "parameters['TABLES']['CREATION']['ALL_SCHEMA'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IqqQ6sbPDIU6"
   },
   "outputs": [],
   "source": [
    "json_filename ='parameters_ETL.json'\n",
    "json_file = json.dumps(parameters)\n",
    "f = open(json_filename,\"w\")\n",
    "f.write(json_file)\n",
    "f.close()\n",
    "s3.upload_file(json_filename, 'DATA/ETL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XgTAunRjDIU9"
   },
   "outputs": [],
   "source": [
    "s3.download_file(key = 'DATA/ETL/parameters_ETL.json')\n",
    "with open('parameters_ETL.json', 'r') as fp:\n",
    "    parameters = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1QKN8xVDIVA"
   },
   "source": [
    "Move `parameters_ETL.json` to the parent folder `01_prepare_tables`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fCwL6Ou5DIVF"
   },
   "outputs": [],
   "source": [
    "s3_output = parameters['GLOBAL']['QUERIES_OUTPUT']\n",
    "db = parameters['GLOBAL']['DATABASE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SiOrUmG2DIVI"
   },
   "outputs": [],
   "source": [
    "for key, value in parameters[\"TABLES\"][\"CREATION\"].items():\n",
    "    if key == \"ALL_SCHEMA\":\n",
    "        for table_info in value:\n",
    "            if table_info['name'] in ['ets_insee_raw_juillet']:\n",
    "\n",
    "                # CREATE QUERY\n",
    "\n",
    "                ### Create top/bottom query\n",
    "                table_top = parameters[\"TABLES\"][\"CREATION\"][\"template\"][\"top\"].format(\n",
    "                            table_info[\"database\"], table_info[\"name\"]\n",
    "                        )\n",
    "                table_bottom = parameters[\"TABLES\"][\"CREATION\"][\"template\"][\n",
    "                            \"bottom_OpenCSVSerde\"\n",
    "                        ].format(table_info[\"separator\"], table_info[\"s3URI\"])\n",
    "\n",
    "                ### Create middle\n",
    "                table_middle = \"\"\n",
    "                nb_var = len(table_info[\"schema\"])\n",
    "                for i, val in enumerate(table_info[\"schema\"]):\n",
    "                    if i == nb_var - 1:\n",
    "                        table_middle += parameters[\"TABLES\"][\"CREATION\"][\"template\"][\n",
    "                                    \"middle\"\n",
    "                                ].format(val['Name'], val['Type'], \")\")\n",
    "                    else:\n",
    "                        table_middle += parameters[\"TABLES\"][\"CREATION\"][\"template\"][\n",
    "                                    \"middle\"\n",
    "                                ].format(val['Name'], val['Type'], \",\")\n",
    "\n",
    "                query = table_top + table_middle + table_bottom\n",
    "\n",
    "                ## DROP IF EXIST\n",
    "\n",
    "                s3.run_query(\n",
    "                                query=\"DROP TABLE {}\".format(table_info[\"name\"]),\n",
    "                                database=db,\n",
    "                                s3_output=s3_output\n",
    "                        )\n",
    "\n",
    "                ## RUN QUERY\n",
    "                output = s3.run_query(\n",
    "                            query=query,\n",
    "                            database=table_info[\"database\"],\n",
    "                            s3_output=s3_output,\n",
    "                            filename=None,  ## Add filename to print dataframe\n",
    "                            destination_key=None,  ### Add destination key if need to copy output\n",
    "                        )\n",
    "\n",
    "                    ## SAVE QUERY ID\n",
    "                table_info['output_id'] = output['QueryID']\n",
    "\n",
    "                         ### UPDATE CATALOG\n",
    "                #glue.update_schema_table(\n",
    "                #            database=table_info[\"database\"],\n",
    "                #            table=table_info[\"name\"],\n",
    "                #            schema=table_info[\"schema\"],\n",
    "                #        )\n",
    "\n",
    "                print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation table transformée\n",
    "\n",
    "La tale tranformée contient 6 variables supplémentaires qui vont être utilisées pour la réalisation des tests. Les 6 variables sont les suivantes:\n",
    "\n",
    "* `voie_clean` \n",
    "    - Ajout de la variable non abbrégée du type de voie. Exemple, l'INSEE indique CH, pour chemin, il faut donc indiquer CHEMIN\n",
    "* `count_initial_insee`\n",
    "    - Compte du nombre de siret (établissement) par siren (entreprise).\n",
    "* ville_matching \n",
    "    - Nettoyage de la ville de l'INSEE (`libelleCommuneEtablissement`) de la même manière que l'INPI\n",
    "* adress_reconstituee_insee:\n",
    "    - Reconstitution de l'adresse à l'INSEE en utilisant le numéro de voie `numeroVoieEtablissement`, le type de voie non abbrégé, `voie_clean`, l'adresse `libelleVoieEtablissement`  et le `complementAdresseEtablissement` et suppression des articles\n",
    "* adresse_distance_insee\n",
    "* list_enseigne:\n",
    "    - Concatenation de:\n",
    "        - `enseigne1etablissement`\n",
    "        - `enseigne2etablissement`\n",
    "        - `enseigne3etablissement`\n",
    "\n",
    "Pour créer le pattern regex, on utilise une liste de type de voie disponible dans le Gitlab et à l'INSEE, que nous avons ensuite modifié manuellement. \n",
    "\n",
    "- Input\n",
    "    - CSV: [TypeVoie.csv](https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/data/input/Parameters/typeVoieEtablissement.csv)\n",
    "        - CSV dans S3: [Parameters/upper_stop.csv](https://s3.console.aws.amazon.com/s3/buckets/calfdata/Parameters/TYPE_VOIE/)\n",
    "        - A créer en table\n",
    "   - Athena: type_voie\n",
    "       - CSV dans S3: [Parameters/type_voie.csv](https://s3.console.aws.amazon.com/s3/buckets/calfdata/Parameters/TYPE_VOIE_SQL/)\n",
    "- Code Python: [Exemple Input 1](https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/05_redaction_US/04_prep_voie_num_2697.md#exemple-input-1)\n",
    "\n",
    "Pour rappel, nous sommes a l'étape 8 de la préparation des données\n",
    "\n",
    "Le nettoyage des variables de l'adresse suive le schema suivant:\n",
    "\n",
    "| Table | Variables                 | Article | Digit | Debut/fin espace | Espace | Accent | Upper |\n",
    "|-------|---------------------------|---------|-------|------------------|--------|--------|-------|\n",
    "| INSEE  | adresse_distance_insee     | X       | X     | X                | X      | X      | X     |\n",
    "| INSEE  | adresse_reconstituee_insee |         |       | X                | X      | X      | X     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_8 = {\n",
    "   \"STEPS_8\":{\n",
    "      \"name\":\"Creation des variables pour la réalisation des tests pour la siretisation\",\n",
    "      \"execution\":[\n",
    "         {\n",
    "            \"database\":\"ets_insee\",\n",
    "            \"name\":\"ets_insee_transformed\",\n",
    "            \"output_id\":\"\",\n",
    "            \"query\":{\n",
    "               \"top\":\" WITH remove_empty_siret AS ( SELECT siren, siret, dateCreationEtablissement, etablissementSiege, etatAdministratifEtablissement, complementAdresseEtablissement, numeroVoieEtablissement, indiceRepetitionEtablissement, CASE WHEN indiceRepetitionEtablissement = 'B' THEN 'BIS' WHEN indiceRepetitionEtablissement = 'T' THEN 'TER' WHEN indiceRepetitionEtablissement = 'Q' THEN 'QUATER' WHEN indiceRepetitionEtablissement = 'C' THEN 'QUINQUIES' ELSE indiceRepetitionEtablissement END as indiceRepetitionEtablissement_full, typeVoieEtablissement, libelleVoieEtablissement, codePostalEtablissement, libelleCommuneEtablissement, libelleCommuneEtrangerEtablissement, distributionSpecialeEtablissement, codeCommuneEtablissement, codeCedexEtablissement, libelleCedexEtablissement, codePaysEtrangerEtablissement, libellePaysEtrangerEtablissement, enseigne1Etablissement, enseigne2Etablissement, enseigne3Etablissement, array_remove( array_distinct( SPLIT( concat( enseigne1etablissement, ',', enseigne2etablissement, ',', enseigne3etablissement ), ',' ) ), '' ) as list_enseigne FROM ets_insee.ets_insee_raw_juillet ) \",\n",
    "                \"middle\":\" SELECT * FROM ( WITH concat_adress AS( SELECT siren, siret, dateCreationEtablissement, etablissementSiege, etatAdministratifEtablissement, codePostalEtablissement, codeCommuneEtablissement, libelleCommuneEtablissement, ville_matching, numeroVoieEtablissement, array_distinct( regexp_extract_all( REGEXP_REPLACE( REGEXP_REPLACE( REGEXP_REPLACE( CONCAT( COALESCE(numeroVoieEtablissement, ''), ' ', COALESCE( indiceRepetitionEtablissement_full, '' ), ' ', COALESCE(voie_clean, ''), ' ', COALESCE(libelleVoieEtablissement, ''), ' ', COALESCE( complementAdresseEtablissement, '' ) ), '[^\\w\\s]| +', ' ' ), '(?:^|(?<= ))(AU|AUX|AVEC|CE|CES|DANS|DE|DES|DU|ELLE|EN|ET|EUX|IL|ILS|LA|LE|LES)(?:(?= )|$)', '' ), '\\s+\\s+', ' ' ), '[0-9]+' ) ) AS list_numero_voie_matching_insee, typeVoieEtablissement, voie_clean, libelleVoieEtablissement, complementAdresseEtablissement, indiceRepetitionEtablissement_full, REGEXP_REPLACE( NORMALIZE( UPPER( trim( REGEXP_REPLACE( REGEXP_REPLACE( REGEXP_REPLACE( CONCAT( COALESCE(numeroVoieEtablissement, ''), ' ', COALESCE( indiceRepetitionEtablissement_full, '' ), ' ', COALESCE(voie_clean, ''), ' ', COALESCE(libelleVoieEtablissement, ''), ' ', COALESCE( complementAdresseEtablissement, '' ) ), '[^\\w\\s]| +', ' ' ), '\\s\\s+', ' ' ), '^\\s+|\\s+$', '' ) ) ), NFD ), '\\pM', '' ) AS adresse_reconstituee_insee, REGEXP_REPLACE( NORMALIZE( UPPER( REGEXP_REPLACE( trim( REGEXP_REPLACE( REGEXP_REPLACE( CONCAT( COALESCE(numeroVoieEtablissement, ''), ' ', COALESCE( indiceRepetitionEtablissement_full, '' ), ' ', COALESCE(voie_clean, ''), ' ', COALESCE(libelleVoieEtablissement, ''), ' ', COALESCE( complementAdresseEtablissement, '' ) ), '[^\\w\\s]|\\d+| +', ' ' ), '(?:^|(?<= ))(AU|AUX|AVEC|CE|CES|DANS|DE|DES|DU|ELLE|EN|ET|EUX|IL|ILS|LA|LE|LES)(?:(?= )|$)', '' ) ), '\\s+\\s+', ' ' ) ), NFD ), '\\pM', '' ) AS adresse_distance_insee, enseigne1Etablissement, enseigne2Etablissement, enseigne3Etablissement, list_enseigne FROM ( SELECT siren, siret, dateCreationEtablissement, etablissementSiege, etatAdministratifEtablissement, codePostalEtablissement, codeCommuneEtablissement, libelleCommuneEtablissement, REGEXP_REPLACE( REGEXP_REPLACE( REGEXP_REPLACE( REGEXP_REPLACE( REGEXP_REPLACE( REGEXP_REPLACE( REGEXP_REPLACE( libelleCommuneEtablissement, '^\\d+\\s|\\s\\d+\\s|\\s\\d+$', '' ), '^LA\\s+|^LES\\s+|^LE\\s+|\\\\(.*\\\\)|^L(ES|A|E) | L(ES|A|E) | L(ES|A|E)$|CEDEX | CEDEX | CEDEX|^E[R*] | E[R*] | E[R*]$', '' ), '^STE | STE | STE$|^STES | STES | STES', 'SAINTE' ), '^ST | ST | ST$', 'SAINT' ), 'S/|^S | S | S$', 'SUR' ), '/S', 'SOUS' ), '[^\\w\\s]|\\([^()]*\\)|ER ARRONDISSEMENT|E ARRONDISSEMENT|\" \\\n",
    "\"|^SUR$|CEDEX|[0-9]+|\\s+', '' ) as ville_matching, libelleVoieEtablissement, complementAdresseEtablissement, numeroVoieEtablissement, indiceRepetitionEtablissement_full, typeVoieEtablissement, enseigne1Etablissement, enseigne2Etablissement, enseigne3Etablissement, list_enseigne FROM remove_empty_siret ) LEFT JOIN inpi.type_voie ON typevoieetablissement = type_voie.voie_matching ) \",\n",
    "                \"bottom\":\" SELECT count_initial_insee, concat_adress.siren, siret, dateCreationEtablissement, etablissementSiege, etatAdministratifEtablissement, codePostalEtablissement, codeCommuneEtablissement, libelleCommuneEtablissement, ville_matching, libelleVoieEtablissement, complementAdresseEtablissement, numeroVoieEtablissement, CASE WHEN cardinality( list_numero_voie_matching_insee ) = 0 THEN NULL ELSE list_numero_voie_matching_insee END as list_numero_voie_matching_insee, indiceRepetitionEtablissement_full, typeVoieEtablissement, voie_clean, adresse_reconstituee_insee, adresse_distance_insee, enseigne1Etablissement, enseigne2Etablissement, enseigne3Etablissement, CASE WHEN cardinality(list_enseigne) = 0 THEN NULL ELSE list_enseigne END AS list_enseigne FROM concat_adress LEFT JOIN ( SELECT siren, COUNT(siren) as count_initial_insee FROM concat_adress GROUP BY siren ) as count_siren ON concat_adress.siren = count_siren.siren ) \" }\n",
    "         }\n",
    "      ],\n",
    "       \"schema\":[\n",
    "               {\n",
    "                  \"Name\":\"\",\n",
    "                  \"Type\":\"\",\n",
    "                  \"Comment\":\"\"\n",
    "               }\n",
    "            ]\n",
    "   }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = True\n",
    "if to_remove:\n",
    "    parameters['TABLES']['PREPARATION']['ALL_SCHEMA'].pop(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters['TABLES']['PREPARATION']['ALL_SCHEMA'].append(step_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters['TABLES']['PREPARATION']['ALL_SCHEMA'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_filename ='parameters_ETL.json'\n",
    "json_file = json.dumps(parameters)\n",
    "f = open(json_filename,\"w\")\n",
    "f.write(json_file)\n",
    "f.close()\n",
    "s3.upload_file(json_filename, 'DATA/ETL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in parameters[\"TABLES\"][\"PREPARATION\"].items():\n",
    "    if key == \"ALL_SCHEMA\":\n",
    "        ### LOOP STEPS\n",
    "        for i, steps in enumerate(value):\n",
    "            step_name = \"STEPS_{}\".format(i)\n",
    "            if step_name in ['STEPS_8']:\n",
    "\n",
    "                ### LOOP EXECUTION WITHIN STEP\n",
    "                for j, step_n in enumerate(steps[step_name][\"execution\"]):\n",
    "\n",
    "                    ### DROP IF EXIST\n",
    "                    s3.run_query(\n",
    "                        query=\"DROP TABLE {}.{}\".format(step_n[\"database\"], step_n[\"name\"]),\n",
    "                        database=db,\n",
    "                        s3_output=s3_output,\n",
    "                    )\n",
    "\n",
    "                    ### CREATE TOP\n",
    "                    table_top = parameters[\"TABLES\"][\"PREPARATION\"][\"template\"][\n",
    "                        \"top\"\n",
    "                    ].format(step_n[\"database\"], step_n[\"name\"],)\n",
    "\n",
    "                    ### COMPILE QUERY\n",
    "                    query = (\n",
    "                        table_top\n",
    "                        + step_n[\"query\"][\"top\"]\n",
    "                        + step_n[\"query\"][\"middle\"]\n",
    "                        + step_n[\"query\"][\"bottom\"]\n",
    "                    )\n",
    "                    output = s3.run_query(\n",
    "                        query=query,\n",
    "                        database=db,\n",
    "                        s3_output=s3_output,\n",
    "                        filename=None,  ## Add filename to print dataframe\n",
    "                        destination_key=None,  ### Add destination key if need to copy output\n",
    "                    )\n",
    "\n",
    "                    ## SAVE QUERY ID\n",
    "                    step_n[\"output_id\"] = output[\"QueryID\"]\n",
    "\n",
    "                    ### UPDATE CATALOG\n",
    "                    #glue.update_schema_table(\n",
    "                    #    database=step_n[\"database\"],\n",
    "                    #    table=step_n[\"name\"],\n",
    "                    #    schema=steps[step_name][\"schema\"],\n",
    "                    #)\n",
    "\n",
    "                    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8K6P4GUMDIVN"
   },
   "source": [
    "# Analytics\n",
    "\n",
    "The cells below execute the job in the key `ANALYSIS`. You need to change the `primary_key` and `secondary_key`.\n",
    "\n",
    "Il n'est pas possible de récupérer le schema de Glue avec Boto3 sous windows. Nous devons récuperer le schéma manuellement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "\t\"StorageDescriptor\": {\n",
    "\t\t\"Columns\":  [\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"count_initial_insee\",\n",
    "\t\t\t\t\t\"Type\": \"bigint\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"siren\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"siret\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"datecreationetablissement\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"etablissementsiege\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"etatadministratifetablissement\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"codepostaletablissement\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"codecommuneetablissement\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"libellecommuneetablissement\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"ville_matching\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"libellevoieetablissement\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"complementadresseetablissement\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"numerovoieetablissement\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"list_numero_voie_matching_insee\",\n",
    "\t\t\t\t\t\"Type\": \"array<string>\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"indicerepetitionetablissement_full\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"typevoieetablissement\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"voie_clean\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"adresse_reconstituee_insee\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"adresse_distance_insee\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"enseigne1etablissement\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"enseigne2etablissement\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"enseigne3etablissement\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"list_enseigne\",\n",
    "\t\t\t\t\t\"Type\": \"array<string>\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t}\n",
    "\t\t\t],\n",
    "\t\t\"location\": \"s3://calfdata/SQL_OUTPUT_ATHENA/tables/51d2765a-0852-4a0a-9333-943ee7e66f5d/\",\n",
    "\t\t\"inputFormat\": \"org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\",\n",
    "\t\t\"outputFormat\": \"org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\",\n",
    "\t\t\"compressed\": \"false\",\n",
    "\t\t\"numBuckets\": \"0\",\n",
    "\t\t\"SerDeInfo\": {\n",
    "\t\t\t\"name\": \"ets_insee_transformed\",\n",
    "\t\t\t\"serializationLib\": \"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\",\n",
    "\t\t\t\"parameters\": {}\n",
    "\t\t},\n",
    "\t\t\"bucketCols\": [],\n",
    "\t\t\"sortCols\": [],\n",
    "\t\t\"parameters\": {},\n",
    "\t\t\"SkewedInfo\": {},\n",
    "\t\t\"storedAsSubDirectories\": \"false\"\n",
    "\t},\n",
    "\t\"parameters\": {\n",
    "\t\t\"EXTERNAL\": \"TRUE\",\n",
    "\t\t\"has_encrypted_data\": \"false\"\n",
    "\t}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvOS6SftDIVO"
   },
   "source": [
    "## Count missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fV8oF3BxDIVP"
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "today = date.today().strftime('%Y%M%d')\n",
    "today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#table_info[\"name\"] = \"ets_insee_transformed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B4PV0taoDIVS"
   },
   "outputs": [],
   "source": [
    "table_top = parameters[\"ANALYSIS\"][\"COUNT_MISSING\"][\"top\"]\n",
    "table_middle = \"\"\n",
    "table_bottom = parameters[\"ANALYSIS\"][\"COUNT_MISSING\"][\"bottom\"].format(\n",
    "    table_info[\"database\"], table_info[\"name\"]\n",
    ")\n",
    "\n",
    "for key, value in enumerate(schema[\"StorageDescriptor\"][\"Columns\"]):\n",
    "    if key == len(schema[\"StorageDescriptor\"][\"Columns\"]) - 1:\n",
    "\n",
    "        table_middle += \"{} \".format(\n",
    "            parameters[\"ANALYSIS\"][\"COUNT_MISSING\"][\"middle\"].format(value[\"Name\"])\n",
    "        )\n",
    "    else:\n",
    "        table_middle += \"{} ,\".format(\n",
    "            parameters[\"ANALYSIS\"][\"COUNT_MISSING\"][\"middle\"].format(value[\"Name\"])\n",
    "        )\n",
    "query = table_top + table_middle + table_bottom\n",
    "output = s3.run_query(\n",
    "    query=query,\n",
    "    database=db,\n",
    "    s3_output=s3_output,\n",
    "    filename=\"count_missing\",  ## Add filename to print dataframe\n",
    "    destination_key=None,  ### Add destination key if need to copy output\n",
    ")\n",
    "display(\n",
    "    output.T.rename(columns={0: \"total_missing\"})\n",
    "    .assign(total_missing_pct=lambda x: x[\"total_missing\"] / x.iloc[0, 0])\n",
    "    .sort_values(by=[\"total_missing\"], ascending=False)\n",
    "    .style.format(\"{0:,.2%}\", subset=[\"total_missing_pct\"])\n",
    "    .bar(subset=\"total_missing_pct\", color=[\"#d65f5f\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ViqZ4ro1DIVU"
   },
   "source": [
    "# Brief description table\n",
    "\n",
    "In this part, we provide a brief summary statistic from the lattest jobs. For the continuous analysis with a primary/secondary key, please add the relevant variables you want to know the count and distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5NRhygTxDIVV"
   },
   "source": [
    "## Categorical Description\n",
    "\n",
    "During the categorical analysis, we wil count the number of observations for a given group and for a pair.\n",
    "\n",
    "### Count obs by group\n",
    "\n",
    "- Index: primary group\n",
    "- nb_obs: Number of observations per primary group value\n",
    "- percentage: Percentage of observation per primary group value over the total number of observations\n",
    "\n",
    "Returns the top 10 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "14pMnRZPDIVV"
   },
   "outputs": [],
   "source": [
    "for field in schema[\"StorageDescriptor\"][\"Columns\"]:\n",
    "    if field[\"Type\"] in [\"string\", \"object\", \"varchar(12)\"]:\n",
    "\n",
    "        print(\"Nb of obs for {}\".format(field[\"Name\"]))\n",
    "\n",
    "        query = parameters[\"ANALYSIS\"][\"CATEGORICAL\"][\"PAIR\"].format(\n",
    "            table_info[\"database\"], table_info[\"name\"], field[\"Name\"]\n",
    "        )\n",
    "        output = s3.run_query(\n",
    "            query=query,\n",
    "            database=db,\n",
    "            s3_output=s3_output,\n",
    "            filename=\"count_categorical_{}\".format(\n",
    "                field[\"Name\"]\n",
    "            ),  ## Add filename to print dataframe\n",
    "            destination_key=None,  ### Add destination key if need to copy output\n",
    "        )\n",
    "\n",
    "        ### Print top 10\n",
    "\n",
    "        display(\n",
    "            (\n",
    "                output.set_index([field[\"Name\"]])\n",
    "                .assign(percentage=lambda x: x[\"nb_obs\"] / x[\"nb_obs\"].sum())\n",
    "                .sort_values(\"percentage\", ascending=False)\n",
    "                .head(10)\n",
    "                .style.format(\"{0:.2%}\", subset=[\"percentage\"])\n",
    "                .bar(subset=[\"percentage\"], color=\"#d65f5f\")\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUrDFOV6DIVo"
   },
   "source": [
    "# Generation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vkHkD2U8DIVp"
   },
   "outputs": [],
   "source": [
    "import os, time, shutil, urllib, ipykernel, json\n",
    "from pathlib import Path\n",
    "from notebook import notebookapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ab11xUncDIVr"
   },
   "outputs": [],
   "source": [
    "def create_report(extension = \"html\", keep_code = False):\n",
    "    \"\"\"\n",
    "    Create a report from the current notebook and save it in the \n",
    "    Report folder (Parent-> child directory)\n",
    "    \n",
    "    1. Exctract the current notbook name\n",
    "    2. Convert the Notebook \n",
    "    3. Move the newly created report\n",
    "    \n",
    "    Args:\n",
    "    extension: string. Can be \"html\", \"pdf\", \"md\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ### Get notebook name\n",
    "    connection_file = os.path.basename(ipykernel.get_connection_file())\n",
    "    kernel_id = connection_file.split('-', 1)[0].split('.')[0]\n",
    "\n",
    "    for srv in notebookapp.list_running_servers():\n",
    "        try:\n",
    "            if srv['token']=='' and not srv['password']:  \n",
    "                req = urllib.request.urlopen(srv['url']+'api/sessions')\n",
    "            else:\n",
    "                req = urllib.request.urlopen(srv['url']+ \\\n",
    "                                             'api/sessions?token=' + \\\n",
    "                                             srv['token'])\n",
    "            sessions = json.load(req)\n",
    "            notebookname = sessions[0]['name']\n",
    "        except:\n",
    "            pass  \n",
    "    \n",
    "    sep = '.'\n",
    "    path = os.getcwd()\n",
    "    #parent_path = str(Path(path).parent)\n",
    "    \n",
    "    ### Path report\n",
    "    #path_report = \"{}/Reports\".format(parent_path)\n",
    "    #path_report = \"{}/Reports\".format(path)\n",
    "    \n",
    "    ### Path destination\n",
    "    name_no_extension = notebookname.split(sep, 1)[0]\n",
    "    source_to_move = name_no_extension +'.{}'.format(extension)\n",
    "    dest = os.path.join(path,'Reports', source_to_move)\n",
    "    \n",
    "    ### Generate notebook\n",
    "    if keep_code:\n",
    "        os.system('jupyter nbconvert --to {} {}'.format(\n",
    "    extension,notebookname))\n",
    "    else:\n",
    "        os.system('jupyter nbconvert --no-input --to {} {}'.format(\n",
    "    extension,notebookname))\n",
    "    \n",
    "    ### Move notebook to report folder\n",
    "    #time.sleep(5)\n",
    "    shutil.move(source_to_move, dest)\n",
    "    print(\"Report Available at this adress:\\n {}\".format(dest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zjf7kUNQDIVu"
   },
   "outputs": [],
   "source": [
    "create_report(extension = \"html\", keep_code = True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "XX_template_table_creation_AWS.ipynb",
   "provenance": []
  },
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nteract": {
   "version": "0.22.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
