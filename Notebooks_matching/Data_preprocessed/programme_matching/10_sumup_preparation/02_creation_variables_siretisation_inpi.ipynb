{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cI5JOtG4DIn2",
    "lines_to_next_cell": 0
   },
   "source": [
    "# Creation table ets_inpi contenant les nouvelles variables permettant la siretisation\n",
    "\n",
    "# Objective(s)\n",
    "\n",
    "*  Création des variables suivantes pour permettre la réalisation des tests avec l’INSEE\n",
    "\n",
    "# Metadata\n",
    "\n",
    "* Epic: Epic 6\n",
    "* US: US 3\n",
    "* Date Begin: 9/28/2020\n",
    "* Duration Task: 1\n",
    "* Description: Creation des variables qui vont servir a réaliser les tests pour la siretisation\n",
    "* Step type: Transform table\n",
    "* Status: Active\n",
    "  * Change Status task: Active\n",
    "  * Update table: Modify rows\n",
    "* Source URL: US 03 Creation Variables data INPI et INSEE\n",
    "* Task type: Jupyter Notebook\n",
    "* Users: Thomas Pernet\n",
    "* Watchers: Thomas Pernet\n",
    "* User Account: https://937882855452.signin.aws.amazon.com/console\n",
    "* Estimated Log points: 10\n",
    "* Task tag: #athena,#lookup-table,#s3,#sql,#data-preparation,#documentation\n",
    "* Toggl Tag: #documentation\n",
    "\n",
    "# Input Cloud Storage [AWS/GCP]\n",
    "\n",
    "## Table/file\n",
    "\n",
    "* Origin: \n",
    "* Athena\n",
    "* Name: \n",
    "* ets_filtre_enrichie_historique\n",
    "* Github: \n",
    "  * https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/10_sumup_preparation/00_filtrage_enrichissement_inpi.md\n",
    "\n",
    "# Destination Output/Delivery\n",
    "\n",
    "## Table/file\n",
    "\n",
    "* Origin: \n",
    "* Athena\n",
    "* Name:\n",
    "* ets_inpi_transformed\n",
    "* GitHub:\n",
    "* https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/10_sumup_preparation/02_creation_variables_siretisation_inpi.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "30Og9fxWDIn2",
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "from awsPy.aws_authorization import aws_connector\n",
    "from awsPy.aws_s3 import service_s3\n",
    "from awsPy.aws_glue import service_glue\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os, shutil, json\n",
    "\n",
    "path = os.getcwd()\n",
    "parent_path = str(Path(path).parent)\n",
    "path_cred = r\"{}/credential_AWS.json\".format(parent_path)\n",
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                       region = 'eu-west-3')\n",
    "\n",
    "region = 'eu-west-3'\n",
    "bucket = 'calfdata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "pBP8v2IBDIn6",
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                       region = region)\n",
    "client= con.client_boto()\n",
    "s3 = service_s3.connect_S3(client = client,\n",
    "                      bucket = bucket, verbose = True) \n",
    "glue = service_glue.connect_glue(client = client) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZsapT38zDIn9"
   },
   "outputs": [],
   "source": [
    "pandas_setting = True\n",
    "if pandas_setting:\n",
    "    cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "L’objectif de la sirétisation est d’attribuer un siret à un établissement appartenant à une entreprise. Le siret est un identifiant unique rattaché à un établissement, et donc à une adresse. Un siret, dès lors, ne peut posséder plusieurs adresses. En cas de création, déménagement ou fermeture d’établissement, un nouveau siret sera attribué. \n",
    "\n",
    "L’INSEE est en charge de la création et attribution du siret. Toutefois, cette information ne figure pas à l’INPI. La manière dont l’INPI distingue des établissements est légèrement différente de l’INSEE. A l’INPI, il faut regarder la séquence siren, code greffe, numéro de gestion et numéro d’établissement pour identifier un établissement d’un autre. \n",
    "\n",
    "La difficulté de la siretisation vient du manque de normalisme entre les deux organismes. L’INSEE affiche davantage de conformité dans la création de la variable qu’à l’INPI. La création de l’adresse du coté de l’INPI est laissée à la seule appréciation du greffe. Il est a noté que des fautes sont possibles à la fois à l’INPI mais aussi à l’INSEE. \n",
    "\n",
    "La seconde difficulté rencontrée est la différence d’état des fichiers entre l’INSEE et l’INPI. L’INSEE fournit chaque mois un stock de donnée, ou dit autrement, donne un état des lieux à l’instant t des entreprises en France. Un établissement peut être ouvert en t-1 mais fermée à date t . Ce changement d’état ne va pas figurer à l’INSEE. Nous allons recevoir uniquement le dernier état connu, a savoir la fermeture. D’un point de vue technique, un siret n’a qu’une seule ligne dans la table de l’INSEE. La table de l’INPI va contenir l’ensemble des données historiques, avant toutes les modifications effectuées sur les établissements. En prenant l’exemple ci dessus, nous connaissons le status de l’établissement lors des deux dates, un premier état ouvert et un second état fermé. Dès lors nous pouvons avoir plusieurs lignes possibles par siret. \n",
    "\n",
    "Lors de notre processus de siretisation, nous allons rapprocher les deux tables en utilisant un score de similarité et des règles de gestion. Le rapprochement  entre les deux tables va se faire via le siren, la ville et le code postal. Il est très probable qu’une entreprise possède plusieurs établissements dans la même ville, ce qui va aboutir à un doublonage des observations. Autrement dit, un même établissement à l’INPI va posséder plusieurs siret. Par exemple, si une entreprise possède 2 établissement dans la même ville, et que ses deux établissements sont aussi présents à l’INPI, alors le rapprochement va déboucher sur 4 lignes (deux lignes par établissements). Il faut multiplier le nombre de création de nouvelles lignes par le nombre d’événements à l’INPI. Si la table de l’INPI à 2 événements pour un établissement, alors cela va créer 4 lignes supplémentaires (deux par événement). \n",
    "\n",
    "L’utilisation du score de similarité (défini ci dessous) et les règles de gestion vont permettre de distinguer les siret aux bonnes adresses. Du fait de la compléxité de certaines adresses, il peut y avoir des lignes qui ne peuvent être siretisé. Lorsque ce cas se présente, nous avons deux méthodes. La méthode une consiste à regarder si la séquence n’a pas été trouvé via une autre ligne. En effet, si nous avons pu trouver le siret d’une séquence via une autre ligne, nous pouvons l’attribuer à l’ensemble des lignes de la même séquence. Nous savons qu’une séquence ne peut pas changer, et qu’une fois le siret trouvé, ce sera toujours le même. Attention, nous avons détecté dans certains cas des séquences avec plusieurs siret car l’INPI a modifié l’adresse d’une séquence sans fermer l’établissement, ce qui n’est pas possible dans les faits. La seconde méthode repose sur le NLP (Natural Language Processing), plus précisément le Word2Vec pour calculer la similarité entre les adresses plus détaillées à l’INPI qu’a l’INSEE. Si un des mots de l’INSEE n’est pas présent dans l’adresse de l’INPI alors que l’adresse est plus détaillée, nous allons calculer un indicateur de similarité entre les mot de l’INSEE  non présents dans l’INPI est ceux de l’INPI. Si une des valeurs est supérieure a un seuil, on peut dire que c’est la bonne adresse. Par exemple, l’INPI peut avoir écrit BD alors que l’INSEE a écrit BOULEVARD . Le modèle va comprendre que les deux mots ont la même signification. \n",
    "\n",
    "## Tableau recapitulatif variables\n",
    "\n",
    "Pour créer les tests pour la siretisation, nous avons besoin de créer de nouvelle variables. Le tableau ci-dessous récapital les variables pour la table de l'INPI.\n",
    "\n",
    "| Tables | Variables                      | Commentaire                                                                                                                                                                                             | Bullet_inputs                                                | US_md                                                          | query_md_gitlab                                                                                                                                                                                                                                                                                        | Pattern_regex                                                       |\n",
    "|--------|--------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------|----------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------|\n",
    "| INPI   | sequence_id                    | ID unique pour la séquence suivante: siren + code greffe + nom greffe + numero gestion +ID établissement                                                                                                | siren code_greffe nom_greffe numero_gestion id_etablissement | [2976](https://tree.taiga.io/project/olivierlubet-air/us/2976) | [create-id-and-id-sequence](https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/01_Athena_concatenate_ETS.md#create-id-and-id-sequence)                                                                    |                                                                     |\n",
    "| INPI   | adresse_reconstituee_inpi      | Concatenation des champs de l'adresse et suppression des espace                                                                                                                                         | adresse_ligne1 adresse_ligne2 adresse_ligne3                 | [2690](https://tree.taiga.io/project/olivierlubet-air/us/2690) | [adress_reconsitituee_inpi](https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/03_ETS_add_variables.md#adress_reconsitituee_inpi)                                                                         | debut/fin espace,espace,accent,Upper                                |\n",
    "| INPI   | adresse_distance_inpi          | Concatenation des champs de l'adresse, suppression des espaces et des articles. Utilisé pour calculer le score permettant de distinguer la similarité/dissimilarité entre deux adresses (INPI vs INSEE) | adresse_ligne1 adresse_ligne2 adresse_ligne3                 | [2949](https://tree.taiga.io/project/olivierlubet-air/us/2949) | [adresse_distance_inpi](https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/03_ETS_add_variables.md#adresse_distance_inpi)                                                                                 | article,digit,debut/fin espace,espace,accent,Upper                  |\n",
    "| INPI   | ville_matching                 | Nettoyage regex de la ville et suppression des espaces. La même logique de nettoyage est appliquée coté INSEE                                                                                           | ville                                                        | [2613](https://tree.taiga.io/project/olivierlubet-air/us/2613) | [etape-1-pr%C3%A9paration-ville_matching](https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/03_ETS_add_variables.md#etape-1-pr%C3%A9paration-ville_matching)                                             | article,digit,debut/fin espace,espace,accent,Upper,Regles_speciales |\n",
    "| INPI   | list_numero_voie_matching_inpi | Liste contenant tous les numéros de l'adresse dans l'INPI                                                                                                                                               | adresse_ligne1 adresse_ligne2 adresse_ligne3                 | [3000](https://tree.taiga.io/project/olivierlubet-air/us/3000) | [etape-5-creation-liste-num%C3%A9ro-de-voie](https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/03_ETS_add_variables.md#etape-5-creation-liste-num%C3%A9ro-de-voie)                                       | digit,debut/fin espace,espace                                       |\n",
    "| INPI   | last_libele_evt                | Extraction du dernier libellé de l'événement connu pour une séquence, et appliquer cette information à l'ensemble de la séquence                                                                        | libelle_evt                                                  | [2950](https://tree.taiga.io/project/olivierlubet-air/us/2950) | [etape-4-cr%C3%A9ation-last_libele_evt-status_admin-status_ets](https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/03_ETS_add_variables.md#etape-4-cr%C3%A9ation-last_libele_evt-status_admin-status_ets) |                                                                     |\n",
    "| INPI   | status_admin                   | Informe du status ouvert/fermé concernant une séquence                                                                                                                                                  | last_libele_evt                                              | [2951](https://tree.taiga.io/project/olivierlubet-air/us/2951) | [etape-4-cr%C3%A9ation-last_libele_evt-status_admin-status_ets](https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/03_ETS_add_variables.md#etape-4-cr%C3%A9ation-last_libele_evt-status_admin-status_ets) | Regles_speciales                                                    |\n",
    "| INPI   | status_ets                     | Informe du type d'établissement (SIE/PRI/SEC) concernant une séquence                                                                                                                                   | type                                                         | [2951](https://tree.taiga.io/project/olivierlubet-air/us/2951) | [etape-4-cr%C3%A9ation-last_libele_evt-status_admin-status_ets](https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/01_preparation/03_ETS_add_variables.md#etape-4-cr%C3%A9ation-last_libele_evt-status_admin-status_ets) | Regles_speciales                                                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation data\n",
    "\n",
    "Il y a 9 variables a construire pour finaliser la table de l'INPI. Parmis les 9 variables, 7 peuvent être créer assez simplement alors que les 3 autres demandent de faire de partionner la data sur une séquence. Une sequence fait référence a à la définition d'un établissement au sens de l'INPI, comme évoqué en introduction de la documentation.\n",
    "\n",
    "**Aucun grouping**\n",
    "\n",
    "* `index_id`: \n",
    "    - Création du numéro de ligne\n",
    "* `enseigne`: \n",
    "    - Mise en majuscule\n",
    "* `ville_matching`:\n",
    "    - Nettoyage regex de la ville et suppression des espaces\n",
    "* `adress_reconstituee_inpi`\n",
    "    - Concatenation des champs de l'adresse et suppression des espaces\n",
    "* `adress_distance_inpi`: \n",
    "    - Concatenation des champs de l'adresse, suppression des espaces et des articles\n",
    "* `list_numero_voie_matching`:\n",
    "    - Liste contenant tous les numéros de l'adresse dans l'INPI\n",
    "* `status_ets`: \n",
    "    - Informe du type d'établissement (SIE/PRI.SEC) concernant une séquence\n",
    "\n",
    "**Grouping**\n",
    "\n",
    "* `sequence_id`:\n",
    "    - Attribution d'un ID unique pour la sequence siren + code greffe + nom greffe + numero gestion +ID établissement. Cela fait référence a la définition d'établissement au sens de l'INPI.\n",
    "* `status_admin`: \n",
    "    - Informe du status ouvert/fermé concernant une séquence\n",
    "* `last_libelle_evt`: \n",
    "    - Extraction du dernier libellé de l'événement connu pour une séquence, et appliquer cette information à l'ensemble de la séquence    \n",
    "    \n",
    "Pour faciliter la construction de la table `ets_inpi_transformed`, nous allons procéder a une étape intermédiaire, a savoir la création de `ets_inpi_transformed_temp`. Cette table intermédiaire va calculer l'index et la séquence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nOhbGckqDIoC"
   },
   "outputs": [],
   "source": [
    "### If chinese characters, set  ensure_ascii=False\n",
    "s3.download_file(key = 'DATA/ETL/parameters_ETL.json')\n",
    "with open('parameters_ETL.json', 'r') as fp:\n",
    "    parameters = json.load(fp)\n",
    "#print(json.dumps(parameters, indent=4, sort_keys=True, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etap 1: Creation séquence\n",
    "\n",
    "Dans cette étape, nous allons créer un index, qui est simplement le numéro de ligne, ainsi qu'un ID établissement unique, qui groupe les variables `siren`, `code_greffe`, `nom_greffe`, `numero_gestion`, `id_etablissement`.\n",
    "\n",
    "Pour rappel, nous en sommes a l'étape 6 dans la pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uVGYsgexDIoF"
   },
   "outputs": [],
   "source": [
    "step_6 = {\n",
    "   \"STEPS_6\":{\n",
    "      \"name\":\"Creation sequence caracterisant un établissement au sens de l'INPI\",\n",
    "      \"execution\":[\n",
    "         {\n",
    "            \"database\":\"ets_inpi\",\n",
    "            \"name\":\"ets_inpi_transformed_temp\",\n",
    "            \"output_id\":\"\",\n",
    "            \"query\":{\n",
    "               \"top\":\" WITH cte AS ( SELECT siren, code_greffe, nom_greffe, numero_gestion, id_etablissement, sequence_id, ROW_NUMBER() OVER ( PARTITION BY sequence_id ORDER BY sequence_id) as rownum FROM ( SELECT siren, code_greffe, nom_greffe, numero_gestion, id_etablissement, DENSE_RANK () OVER( ORDER BY siren, code_greffe, nom_greffe, numero_gestion, id_etablissement) AS sequence_id FROM ets_filtre_enrichie_historique ) ) \",\n",
    "               \"middle\":\" SELECT ROW_NUMBER() OVER () as index_id, sequence_id, ets_filtre_enrichie_historique.siren, ets_filtre_enrichie_historique.code_greffe, ets_filtre_enrichie_historique.nom_greffe, ets_filtre_enrichie_historique.numero_gestion, ets_filtre_enrichie_historique.id_etablissement, status, origin, date_greffe, libelle_evt, type, siege_pm, rcs_registre, adresse_ligne1, adresse_ligne2, adresse_ligne3, code_postal, ville, code_commune, pays, domiciliataire_nom, domiciliataire_siren, domiciliataire_greffe, domiciliataire_complement, siege_domicile_representant, nom_commercial, enseigne, activite_ambulante, activite_saisonniere, activite_non_sedentaire, date_debut_activite, activite, origine_fonds, origine_fonds_info, type_exploitation, csv_source FROM ets_filtre_enrichie_historique INNER JOIN ( \",\n",
    "               \"bottom\":\" SELECT * FROM cte WHERE rownum = 1 ) as no_dup_cte ON ets_filtre_enrichie_historique.siren = no_dup_cte.siren AND ets_filtre_enrichie_historique.code_greffe = no_dup_cte.code_greffe AND ets_filtre_enrichie_historique.nom_greffe = no_dup_cte.nom_greffe AND ets_filtre_enrichie_historique.numero_gestion = no_dup_cte.numero_gestion AND ets_filtre_enrichie_historique.id_etablissement = no_dup_cte.id_etablissement \"\n",
    "            }\n",
    "         }\n",
    "      ],\n",
    "       \"schema\":[\n",
    "               {\n",
    "                  \"Name\":\"\",\n",
    "                  \"Type\":\"\",\n",
    "                  \"Comment\":\"\"\n",
    "               }\n",
    "            ]\n",
    "   }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHp1yZ4-DIoI"
   },
   "source": [
    "To remove an item from the list, use `pop` with the index to remove. Exemple `parameters['TABLES']['PREPARATION']['ALL_SCHEMA'].pop(6)` will remove the 5th item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qwfUq0__DIoJ"
   },
   "outputs": [],
   "source": [
    "to_remove = False\n",
    "if to_remove:\n",
    "    parameters['TABLES']['PREPARATION']['ALL_SCHEMA'].pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b-M0cc5ADIoN"
   },
   "outputs": [],
   "source": [
    "parameters['TABLES']['PREPARATION']['ALL_SCHEMA'].append(step_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters['TABLES']['PREPARATION']['ALL_SCHEMA'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MEw20IRNDIoQ"
   },
   "outputs": [],
   "source": [
    "json_filename ='parameters_ETL.json'\n",
    "json_file = json.dumps(parameters)\n",
    "f = open(json_filename,\"w\")\n",
    "f.write(json_file)\n",
    "f.close()\n",
    "s3.upload_file(json_filename, 'DATA/ETL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDdataIdDIoS"
   },
   "outputs": [],
   "source": [
    "s3.download_file(key = 'DATA/ETL/parameters_ETL.json')\n",
    "with open('parameters_ETL.json', 'r') as fp:\n",
    "    parameters = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWVE6NhlDIoV"
   },
   "source": [
    "Move `parameters_ETL.json` to the parent folder `01_prepare_tables`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EGtMXwPsDIoa"
   },
   "outputs": [],
   "source": [
    "s3_output = parameters['GLOBAL']['QUERIES_OUTPUT']\n",
    "db = parameters['GLOBAL']['DATABASE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut être patient car l'éxécution prend environ 20 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IIfWNKz_DIod"
   },
   "outputs": [],
   "source": [
    "for key, value in parameters[\"TABLES\"][\"PREPARATION\"].items():\n",
    "    if key == \"ALL_SCHEMA\":\n",
    "        ### LOOP STEPS\n",
    "        for i, steps in enumerate(value):\n",
    "            step_name = \"STEPS_{}\".format(i)\n",
    "            if step_name in ['STEPS_6']:\n",
    "\n",
    "                ### LOOP EXECUTION WITHIN STEP\n",
    "                for j, step_n in enumerate(steps[step_name][\"execution\"]):\n",
    "\n",
    "                    ### DROP IF EXIST\n",
    "                    s3.run_query(\n",
    "                        query=\"DROP TABLE {}.{}\".format(step_n[\"database\"], step_n[\"name\"]),\n",
    "                        database=db,\n",
    "                        s3_output=s3_output,\n",
    "                    )\n",
    "\n",
    "                    ### CREATE TOP\n",
    "                    table_top = parameters[\"TABLES\"][\"PREPARATION\"][\"template\"][\n",
    "                        \"top\"\n",
    "                    ].format(step_n[\"database\"], step_n[\"name\"],)\n",
    "\n",
    "                    ### COMPILE QUERY\n",
    "                    query = (\n",
    "                        table_top\n",
    "                        + step_n[\"query\"][\"top\"]\n",
    "                        + step_n[\"query\"][\"middle\"]\n",
    "                        + step_n[\"query\"][\"bottom\"]\n",
    "                    )\n",
    "                    output = s3.run_query(\n",
    "                        query=query,\n",
    "                        database=db,\n",
    "                        s3_output=s3_output,\n",
    "                        filename=None,  ## Add filename to print dataframe\n",
    "                        destination_key=None,  ### Add destination key if need to copy output\n",
    "                    )\n",
    "\n",
    "                    ## SAVE QUERY ID\n",
    "                    step_n[\"output_id\"] = output[\"QueryID\"]\n",
    "\n",
    "                    ### UPDATE CATALOG\n",
    "                    #glue.update_schema_table(\n",
    "                    #    database=step_n[\"database\"],\n",
    "                    #    table=step_n[\"name\"],\n",
    "                    #    schema=steps[step_name][\"schema\"],\n",
    "                    #)\n",
    "\n",
    "                    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etap 2: Creation variables simples et groupings\n",
    "\n",
    "Dès lors que l'index et la sequence ont été créée, nous allons pouvoir calculer les variables de grouping sur ce dernier. De plus, nous allons créer les variables dites \"simples\" au préalable.\n",
    "\n",
    "Le calcul de la variable `status_admin` nécéssite la variable `last_libelle_evt`. En effet, nous avons besoin de connaitre le status le plus récent d'un établissement pour indiquer si un établissement est en activité ou fermé administrativment. Lorsque l'établissement est fermé, il faut l'indiquer sur l'ensemble de la séquence car les valeurs passées doivent être mises a jour.\n",
    "\n",
    "Le nettoyage des variables de l'adresse suive le schema suivant:\n",
    "\n",
    "| Table | Variables                 | Article | Digit | Debut/fin espace | Espace | Accent | Upper |\n",
    "|-------|---------------------------|---------|-------|------------------|--------|--------|-------|\n",
    "| INPI  | adresse_distance_inpi     | X       | X     | X                | X      | X      | X     |\n",
    "| INPI  | adresse_reconstituee_inpi |         |       | X                | X      | X      | X     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_7 = {\n",
    "   \"STEPS_7\":{\n",
    "      \"name\":\"Creation variables simple et le status administratif d'un etablissement\",\n",
    "      \"execution\":[\n",
    "         {\n",
    "            \"database\":\"ets_inpi\",\n",
    "            \"name\":\"ets_inpi_transformed\",\n",
    "            \"output_id\":\"\",\n",
    "            \"query\":{\n",
    "               \"top\":\"WITH create_var AS ( SELECT index_id, ets_inpi_transformed_temp.sequence_id, siren, code_greffe, nom_greffe, numero_gestion, id_etablissement, status, origin, date_greffe, libelle_evt, last_libele_evt, CASE WHEN last_libele_evt = 'Etablissement supprimé' THEN 'F' ELSE 'A' END AS status_admin, type, CASE WHEN type = 'SIE' OR type = 'SEP' THEN 'true' ELSE 'false' END AS status_ets, siege_pm, rcs_registre, adresse_ligne1, adresse_ligne2, adresse_ligne3, REGEXP_REPLACE( trim( REGEXP_REPLACE( REGEXP_REPLACE( NORMALIZE( UPPER( CONCAT( adresse_ligne1, ' ', adresse_ligne2, ' ', adresse_ligne3 ) ), NFD ), '\\pM', '' ), '[^\\w\\s]| +', ' ' ) ), '\\s+\\s+', ' ' ) AS adresse_reconstituee_inpi , regexp_replace( trim( REGEXP_REPLACE( REGEXP_REPLACE( REGEXP_REPLACE( NORMALIZE( UPPER( CONCAT( adresse_ligne1, ' ', adresse_ligne2, ' ', adresse_ligne3 ) ), NFD ), '\\pM', '' ), '[^\\w\\s]|\\d+| +', ' ' ), '(?:^|(?<= ))(AU|AUX|AVEC|CE|CES|DANS|DE|DES|DU|ELLE|EN|ET|EUX|IL|ILS|LA|LE|LES)(?:(?= )|$)', '' ) ), '\\s+\\s+', ' ' ) AS adresse_distance_inpi, array_distinct( regexp_extract_all( trim( REGEXP_REPLACE( REGEXP_REPLACE( NORMALIZE( UPPER( CONCAT( adresse_ligne1, ' ', adresse_ligne2, ' ', adresse_ligne3 ) ), NFD ), '\\pM', '' ), '[^\\w\\s]| +', ' ' ) ), '[0-9]+' ) ) AS list_numero_voie_matching_inpi, code_postal, CASE WHEN code_postal = '' THEN REGEXP_EXTRACT(ville, '\\d{5}') WHEN LENGTH(code_postal) = 5 THEN code_postal ELSE NULL END AS code_postal_matching, ville, REGEXP_REPLACE( REGEXP_REPLACE( REGEXP_REPLACE( REGEXP_REPLACE( REGEXP_REPLACE( REGEXP_REPLACE( REGEXP_REPLACE( REGEXP_REPLACE( REGEXP_REPLACE( NORMALIZE( UPPER(ville), NFD ), '\\pM', '' ), '^\\d+\\s|\\s\\d+\\s|\\s\\d+$', '' ), '^LA\\s+|^LES\\s+|^LE\\s+|\\\\(.*\\\\)|^L(ES|A|E) | L(ES|A|E) | L(ES|A|E)$|CEDEX | CEDEX | CEDEX|^E[R*] | E[R*] | E[R*]$', '' ), '^STE | STE | STE$|^STES | STES | STES', 'SAINTE' ), '^ST | ST | ST$', 'SAINT' ), 'S/|^S | S | S$', 'SUR' ), '/S', 'SOUS' ), '[^\\w\\s]|\\([^()]*\\)|ER ARRONDISSEMENT|E ARRONDISSEMENT|\" \\\n",
    "\"|^SUR$|CEDEX|[0-9]+|\\s+', '' ), 'MARSEILLEE', 'MARSEILLE' ) as ville_matching, code_commune, pays, domiciliataire_nom, domiciliataire_siren, domiciliataire_greffe, domiciliataire_complement, siege_domicile_representant, nom_commercial, UPPER(enseigne) as enseigne, activite_ambulante, activite_saisonniere, activite_non_sedentaire, date_debut_activite, activite, origine_fonds, origine_fonds_info, type_exploitation, csv_source FROM ets_inpi_transformed_temp\",\n",
    "               \"middle\":\" LEFT JOIN ( SELECT ets_inpi_transformed_temp.sequence_id, ets_inpi_transformed_temp.libelle_evt as last_libele_evt, max_date_greffe FROM ets_inpi_transformed_temp INNER JOIN ( SELECT sequence_id, MAX(date_greffe) as max_date_greffe FROM ets_inpi_transformed_temp GROUP BY sequence_id ) AS temp ON temp.sequence_id = ets_inpi_transformed_temp.sequence_id ) as max_date ON max_date.sequence_id = ets_inpi_transformed_temp.sequence_id)\",\n",
    "               \"bottom\":\" SELECT index_id, sequence_id, siren, code_greffe, nom_greffe, numero_gestion, id_etablissement, status, origin, date_greffe, libelle_evt, last_libele_evt, status_admin, type, status_ets, siege_pm, rcs_registre, adresse_ligne1, adresse_ligne2, adresse_ligne3, adresse_reconstituee_inpi , adresse_distance_inpi, CASE WHEN cardinality(list_numero_voie_matching_inpi) = 0 THEN NULL ELSE list_numero_voie_matching_inpi END as list_numero_voie_matching_inpi, code_postal, code_postal_matching, ville_matching, code_commune, pays, domiciliataire_nom, domiciliataire_siren, domiciliataire_greffe, domiciliataire_complement, siege_domicile_representant, nom_commercial, enseigne, activite_ambulante, activite_saisonniere, activite_non_sedentaire, date_debut_activite, activite, origine_fonds, origine_fonds_info, type_exploitation, csv_source FROM create_var \"\n",
    "            }\n",
    "         }\n",
    "      ],\n",
    "      \"schema\":[\n",
    "               {\n",
    "                  \"Name\":\"\",\n",
    "                  \"Type\":\"\",\n",
    "                  \"Comment\":\"\"\n",
    "               }\n",
    "            ]\n",
    "   }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = True\n",
    "if to_remove:\n",
    "    parameters['TABLES']['PREPARATION']['ALL_SCHEMA'].pop(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters['TABLES']['PREPARATION']['ALL_SCHEMA'].append(step_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters['TABLES']['PREPARATION']['ALL_SCHEMA'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in parameters[\"TABLES\"][\"PREPARATION\"].items():\n",
    "    if key == \"ALL_SCHEMA\":\n",
    "        ### LOOP STEPS\n",
    "        for i, steps in enumerate(value):\n",
    "            step_name = \"STEPS_{}\".format(i)\n",
    "            if step_name in ['STEPS_7']:\n",
    "\n",
    "                ### LOOP EXECUTION WITHIN STEP\n",
    "                for j, step_n in enumerate(steps[step_name][\"execution\"]):\n",
    "\n",
    "                    ### DROP IF EXIST\n",
    "                    s3.run_query(\n",
    "                        query=\"DROP TABLE {}.{}\".format(step_n[\"database\"], step_n[\"name\"]),\n",
    "                        database=db,\n",
    "                        s3_output=s3_output,\n",
    "                    )\n",
    "\n",
    "                    ### CREATE TOP\n",
    "                    table_top = parameters[\"TABLES\"][\"PREPARATION\"][\"template\"][\n",
    "                        \"top\"\n",
    "                    ].format(step_n[\"database\"], step_n[\"name\"],)\n",
    "\n",
    "                    ### COMPILE QUERY\n",
    "                    query = (\n",
    "                        table_top\n",
    "                        + step_n[\"query\"][\"top\"]\n",
    "                        + step_n[\"query\"][\"middle\"]\n",
    "                        + step_n[\"query\"][\"bottom\"]\n",
    "                    )\n",
    "                    output = s3.run_query(\n",
    "                        query=query,\n",
    "                        database=db,\n",
    "                        s3_output=s3_output,\n",
    "                        filename=None,  ## Add filename to print dataframe\n",
    "                        destination_key=None,  ### Add destination key if need to copy output\n",
    "                    )\n",
    "\n",
    "                    ## SAVE QUERY ID\n",
    "                    step_n[\"output_id\"] = output[\"QueryID\"]\n",
    "\n",
    "                    ### UPDATE CATALOG\n",
    "                    #glue.update_schema_table(\n",
    "                    #    database=step_n[\"database\"],\n",
    "                    #    table=step_n[\"name\"],\n",
    "                    #    schema=steps[step_name][\"schema\"],\n",
    "                    #)\n",
    "\n",
    "                    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_filename ='parameters_ETL.json'\n",
    "json_file = json.dumps(parameters)\n",
    "f = open(json_filename,\"w\")\n",
    "f.write(json_file)\n",
    "f.close()\n",
    "s3.upload_file(json_filename, 'DATA/ETL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tkXIgOQcDIog"
   },
   "source": [
    "Get the schema of the lattest job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GEDqS67nDIoh"
   },
   "outputs": [],
   "source": [
    "schema = glue.get_table_information(\n",
    "    database = step_n['database'],\n",
    "    table = step_n['name'])['Table']\n",
    "schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvROvxftDIok"
   },
   "source": [
    "# Analytics\n",
    "\n",
    "The cells below execute the job in the key `ANALYSIS`. You need to change the `primary_key` and `secondary_key`.\n",
    "\n",
    "Il n'est pas possible de récupérer le schema de Glue avec Boto3 sous windows. Nous devons récuperer le schéma manuellement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "\t\"StorageDescriptor\": {\n",
    "\t\t\"Columns\": [\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"index_id\",\n",
    "\t\t\t\t\t\"Type\": \"bigint\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"sequence_id\",\n",
    "\t\t\t\t\t\"Type\": \"bigint\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"siren\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"code_greffe\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"nom_greffe\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"numero_gestion\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"id_etablissement\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"status\",\n",
    "\t\t\t\t\t\"Type\": \"varchar(6)\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"origin\",\n",
    "\t\t\t\t\t\"Type\": \"varchar(7)\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"date_greffe\",\n",
    "\t\t\t\t\t\"Type\": \"timestamp\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"libelle_evt\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"last_libele_evt\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"status_admin\",\n",
    "\t\t\t\t\t\"Type\": \"varchar(1)\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"type\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"status_ets\",\n",
    "\t\t\t\t\t\"Type\": \"varchar(5)\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"siege_pm\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"rcs_registre\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"adresse_ligne1\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"adresse_ligne2\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"adresse_ligne3\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"adresse_reconstituee_inpi\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"adresse_distance_inpi\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"list_numero_voie_matching_inpi\",\n",
    "\t\t\t\t\t\"Type\": \"array<string>\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"code_postal\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"code_postal_matching\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"ville_matching\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"code_commune\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"pays\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"domiciliataire_nom\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"domiciliataire_siren\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"domiciliataire_greffe\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"domiciliataire_complement\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"siege_domicile_representant\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"nom_commercial\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"enseigne\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"activite_ambulante\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"activite_saisonniere\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"activite_non_sedentaire\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"date_debut_activite\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"activite\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"origine_fonds\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"origine_fonds_info\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"type_exploitation\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Name\": \"csv_source\",\n",
    "\t\t\t\t\t\"Type\": \"string\",\n",
    "\t\t\t\t\t\"comment\": \"\"\n",
    "\t\t\t\t}\n",
    "\t\t\t],\n",
    "\t\t\"location\": \"s3://calfdata/SQL_OUTPUT_ATHENA/tables/7d26db88-7b1a-4084-9ee3-17f3a59c4f8d/\",\n",
    "\t\t\"inputFormat\": \"org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\",\n",
    "\t\t\"outputFormat\": \"org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\",\n",
    "\t\t\"compressed\": \"false\",\n",
    "\t\t\"numBuckets\": \"0\",\n",
    "\t\t\"SerDeInfo\": {\n",
    "\t\t\t\"name\": \"ets_inpi_transformed\",\n",
    "\t\t\t\"serializationLib\": \"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\",\n",
    "\t\t\t\"parameters\": {}\n",
    "\t\t},\n",
    "\t\t\"bucketCols\": [],\n",
    "\t\t\"sortCols\": [],\n",
    "\t\t\"parameters\": {},\n",
    "\t\t\"SkewedInfo\": {},\n",
    "\t\t\"storedAsSubDirectories\": \"false\"\n",
    "\t},\n",
    "\t\"parameters\": {\n",
    "\t\t\"EXTERNAL\": \"TRUE\",\n",
    "\t\t\"has_encrypted_data\": \"false\"\n",
    "\t}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtEjycmxDIol"
   },
   "source": [
    "## Count missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3xU6B60NDIom"
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "today = date.today().strftime('%Y%M%d')\n",
    "today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ShHZcC-YDIoo"
   },
   "outputs": [],
   "source": [
    "table_top = parameters[\"ANALYSIS\"][\"COUNT_MISSING\"][\"top\"]\n",
    "table_middle = \"\"\n",
    "table_bottom = parameters[\"ANALYSIS\"][\"COUNT_MISSING\"][\"bottom\"].format(\n",
    "    step_n[\"database\"], step_n[\"name\"]\n",
    ")\n",
    "\n",
    "for key, value in enumerate(schema[\"StorageDescriptor\"][\"Columns\"]):\n",
    "    if key == len(schema[\"StorageDescriptor\"][\"Columns\"]) - 1:\n",
    "\n",
    "        table_middle += \"{} \".format(\n",
    "            parameters[\"ANALYSIS\"][\"COUNT_MISSING\"][\"middle\"].format(value[\"Name\"])\n",
    "        )\n",
    "    else:\n",
    "        table_middle += \"{} ,\".format(\n",
    "            parameters[\"ANALYSIS\"][\"COUNT_MISSING\"][\"middle\"].format(value[\"Name\"])\n",
    "        )\n",
    "query = table_top + table_middle + table_bottom\n",
    "output = s3.run_query(\n",
    "    query=query,\n",
    "    database=db,\n",
    "    s3_output=s3_output,\n",
    "    filename=\"count_missing\",  ## Add filename to print dataframe\n",
    "    destination_key=None,  ### Add destination key if need to copy output\n",
    ")\n",
    "display(\n",
    "    output.T.rename(columns={0: \"total_missing\"})\n",
    "    .assign(total_missing_pct=lambda x: x[\"total_missing\"] / x.iloc[0, 0])\n",
    "    .sort_values(by=[\"total_missing\"], ascending=False)\n",
    "    .style.format(\"{0:,.2%}\", subset=[\"total_missing_pct\"])\n",
    "    .bar(subset=\"total_missing_pct\", color=[\"#d65f5f\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-W6g6SFhDIoq"
   },
   "source": [
    "# Brief description table\n",
    "\n",
    "In this part, we provide a brief summary statistic from the lattest jobs. For the continuous analysis with a primary/secondary key, please add the relevant variables you want to know the count and distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QH4FZ3IkDIor"
   },
   "source": [
    "## Categorical Description\n",
    "\n",
    "During the categorical analysis, we wil count the number of observations for a given group and for a pair.\n",
    "\n",
    "### Count obs by group\n",
    "\n",
    "- Index: primary group\n",
    "- nb_obs: Number of observations per primary group value\n",
    "- percentage: Percentage of observation per primary group value over the total number of observations\n",
    "\n",
    "Returns the top 10 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8m9Ja9aDIos"
   },
   "outputs": [],
   "source": [
    "for field in schema[\"StorageDescriptor\"][\"Columns\"]:\n",
    "    if field[\"Type\"] in [\"string\", \"object\", \"varchar(12)\"]:\n",
    "\n",
    "        print(\"Nb of obs for {}\".format(field[\"Name\"]))\n",
    "\n",
    "        query = parameters[\"ANALYSIS\"][\"CATEGORICAL\"][\"PAIR\"].format(\n",
    "            step_n[\"database\"], step_n[\"name\"], field[\"Name\"]\n",
    "        )\n",
    "        output = s3.run_query(\n",
    "            query=query,\n",
    "            database=db,\n",
    "            s3_output=s3_output,\n",
    "            filename=\"count_categorical_{}\".format(\n",
    "                field[\"Name\"]\n",
    "            ),  ## Add filename to print dataframe\n",
    "            destination_key=None,  ### Add destination key if need to copy output\n",
    "        )\n",
    "\n",
    "        ### Print top 10\n",
    "\n",
    "        display(\n",
    "            (\n",
    "                output.set_index([field[\"Name\"]])\n",
    "                .assign(percentage=lambda x: x[\"nb_obs\"] / x[\"nb_obs\"].sum())\n",
    "                .sort_values(\"percentage\", ascending=False)\n",
    "                .head(10)\n",
    "                .style.format(\"{0:.2%}\", subset=[\"percentage\"])\n",
    "                .bar(subset=[\"percentage\"], color=\"#d65f5f\")\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kySMFnjVDIou"
   },
   "source": [
    "### Count obs by two pair\n",
    "\n",
    "You need to pass the primary group in the cell below\n",
    "\n",
    "- Index: primary group\n",
    "- Columns: Secondary key -> All the categorical variables in the dataset\n",
    "- nb_obs: Number of observations per primary group value\n",
    "- Total: Total number of observations per primary group value (sum by row)\n",
    "- percentage: Percentage of observations per primary group value over the total number of observations per primary group value (sum by row)\n",
    "\n",
    "Returns the top 10 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ySqwANvMDIov"
   },
   "outputs": [],
   "source": [
    "primary_key = \"last_libele_evt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zhgodxZRDIox"
   },
   "outputs": [],
   "source": [
    "for field in schema[\"StorageDescriptor\"][\"Columns\"]:\n",
    "    if field[\"Type\"] in [\"string\", \"object\", \"varchar(12)\"]:\n",
    "        if field[\"Name\"] != primary_key:\n",
    "            print(\n",
    "                \"Nb of obs for the primary group {} and {}\".format(\n",
    "                    primary_key, field[\"Name\"]\n",
    "                )\n",
    "            )\n",
    "            query = parameters[\"ANALYSIS\"][\"CATEGORICAL\"][\"MULTI_PAIR\"].format(\n",
    "                step_n[\"database\"], step_n[\"name\"], primary_key, field[\"Name\"]\n",
    "            )\n",
    "\n",
    "            output = s3.run_query(\n",
    "                query=query,\n",
    "                database=db,\n",
    "                s3_output=s3_output,\n",
    "                filename=\"count_categorical_{}_{}\".format(\n",
    "                    primary_key, field[\"Name\"]\n",
    "                ),  # Add filename to print dataframe\n",
    "                destination_key=None,  # Add destination key if need to copy output\n",
    "            )\n",
    "\n",
    "            display(\n",
    "                (\n",
    "                    pd.concat(\n",
    "                        [\n",
    "                            (\n",
    "                                output.loc[\n",
    "                                    lambda x: x[field[\"Name\"]].isin(\n",
    "                                        (\n",
    "                                            output.assign(\n",
    "                                                total_secondary=lambda x: x[\"nb_obs\"]\n",
    "                                                .groupby([x[field[\"Name\"]]])\n",
    "                                                .transform(\"sum\")\n",
    "                                            )\n",
    "                                            .drop_duplicates(\n",
    "                                                subset=\"total_secondary\", keep=\"last\"\n",
    "                                            )\n",
    "                                            .sort_values(\n",
    "                                                by=[\"total_secondary\"], ascending=False\n",
    "                                            )\n",
    "                                            .iloc[:10, 1]\n",
    "                                            .to_list()\n",
    "                                        )\n",
    "                                    )\n",
    "                                ]\n",
    "                                .set_index([primary_key, field[\"Name\"]])\n",
    "                                .unstack([0])\n",
    "                                .fillna(0)\n",
    "                                .assign(total=lambda x: x.sum(axis=1))\n",
    "                                .sort_values(by=[\"total\"])\n",
    "                            ),\n",
    "                            (\n",
    "                                output.loc[\n",
    "                                    lambda x: x[field[\"Name\"]].isin(\n",
    "                                        (\n",
    "                                            output.assign(\n",
    "                                                total_secondary=lambda x: x[\"nb_obs\"]\n",
    "                                                .groupby([x[field[\"Name\"]]])\n",
    "                                                .transform(\"sum\")\n",
    "                                            )\n",
    "                                            .drop_duplicates(\n",
    "                                                subset=\"total_secondary\", keep=\"last\"\n",
    "                                            )\n",
    "                                            .sort_values(\n",
    "                                                by=[\"total_secondary\"], ascending=False\n",
    "                                            )\n",
    "                                            .iloc[:10, 1]\n",
    "                                            .to_list()\n",
    "                                        )\n",
    "                                    )\n",
    "                                ]\n",
    "                                .rename(columns={\"nb_obs\": \"percentage\"})\n",
    "                                .set_index([primary_key, field[\"Name\"]])\n",
    "                                .unstack([0])\n",
    "                                .fillna(0)\n",
    "                                .apply(lambda x: x / x.sum(), axis=1)\n",
    "                            ),\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    )\n",
    "                    .fillna(0)\n",
    "                    # .sort_index(axis=1, level=1)\n",
    "                    .style.format(\"{0:,.2f}\", subset=[\"nb_obs\", \"total\"])\n",
    "                    .bar(subset=[\"total\"], color=\"#d65f5f\")\n",
    "                    .format(\"{0:,.2%}\", subset=(\"percentage\"))\n",
    "                    .background_gradient(\n",
    "                        cmap=sns.light_palette(\"green\", as_cmap=True), subset=(\"nb_obs\")\n",
    "                    )\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJLL-NklDIpB"
   },
   "source": [
    "# Generation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkh77-3nDIpB"
   },
   "outputs": [],
   "source": [
    "import os, time, shutil, urllib, ipykernel, json\n",
    "from pathlib import Path\n",
    "from notebook import notebookapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n8jkfXmdDIpE"
   },
   "outputs": [],
   "source": [
    "def create_report(extension = \"html\", keep_code = False):\n",
    "    \"\"\"\n",
    "    Create a report from the current notebook and save it in the \n",
    "    Report folder (Parent-> child directory)\n",
    "    \n",
    "    1. Exctract the current notbook name\n",
    "    2. Convert the Notebook \n",
    "    3. Move the newly created report\n",
    "    \n",
    "    Args:\n",
    "    extension: string. Can be \"html\", \"pdf\", \"md\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ### Get notebook name\n",
    "    connection_file = os.path.basename(ipykernel.get_connection_file())\n",
    "    kernel_id = connection_file.split('-', 1)[0].split('.')[0]\n",
    "\n",
    "    for srv in notebookapp.list_running_servers():\n",
    "        try:\n",
    "            if srv['token']=='' and not srv['password']:  \n",
    "                req = urllib.request.urlopen(srv['url']+'api/sessions')\n",
    "            else:\n",
    "                req = urllib.request.urlopen(srv['url']+ \\\n",
    "                                             'api/sessions?token=' + \\\n",
    "                                             srv['token'])\n",
    "            sessions = json.load(req)\n",
    "            notebookname = sessions[0]['name']\n",
    "        except:\n",
    "            pass  \n",
    "    \n",
    "    sep = '.'\n",
    "    path = os.getcwd()\n",
    "    #parent_path = str(Path(path).parent)\n",
    "    \n",
    "    ### Path report\n",
    "    #path_report = \"{}/Reports\".format(parent_path)\n",
    "    #path_report = \"{}/Reports\".format(path)\n",
    "    \n",
    "    ### Path destination\n",
    "    name_no_extension = notebookname.split(sep, 1)[0]\n",
    "    source_to_move = name_no_extension +'.{}'.format(extension)\n",
    "    dest = os.path.join(path,'Reports', source_to_move)\n",
    "    \n",
    "    ### Generate notebook\n",
    "    if keep_code:\n",
    "        os.system('jupyter nbconvert --to {} {}'.format(\n",
    "    extension,notebookname))\n",
    "    else:\n",
    "        os.system('jupyter nbconvert --no-input --to {} {}'.format(\n",
    "    extension,notebookname))\n",
    "    \n",
    "    ### Move notebook to report folder\n",
    "    #time.sleep(5)\n",
    "    shutil.move(source_to_move, dest)\n",
    "    print(\"Report Available at this adress:\\n {}\".format(dest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5qEsk80XDIpG"
   },
   "outputs": [],
   "source": [
    "create_report(extension = \"html\", keep_code = True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "XX_template_table_preprocessing_AWS.ipynb",
   "provenance": []
  },
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nteract": {
   "version": "0.22.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
