{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtrage et enrichissement de la donnée de l’INPI\n",
    "\n",
    "Copy paste from Coda to fill the information\n",
    "\n",
    "## Objective(s)\n",
    "\n",
    "Filtrage et enrichissement de la donnée de l’INPI\n",
    "Select the US you just created →Filtrage et enrichissement de la donnée de l’INPI\n",
    "\n",
    "* The ID is ued26xzfy75910v\n",
    "* Add notebook Epic Epic 6 US US 2 Filtrage et enrichissement de la donnée de l’INPI\n",
    "\n",
    "# Objective(s)\n",
    "\n",
    "*  La préparation de la donnée de l’INPI requière plusieurs étapes de filtrate et d’enrichissement de la donnée. Dans cette US, nous allons détailler comment procéder pour préparer la donnée de l’INPI mais aussi mettre en avant les “problèmes” et points d’attention rencontrées.\n",
    "* Le schéma se résume au diagramme ci-dessous\n",
    "\n",
    "![](https://app.lucidchart.com/publicSegments/view/9e73b3ff-1648-4cda-ab7c-204290721629/image.png)\n",
    "\n",
    "\n",
    "# Metadata\n",
    "\n",
    "* Epic: Epic 6\n",
    "* US: US 2\n",
    "* Date Begin: 9/21/2020\n",
    "* Duration Task: 0\n",
    "* Description: Création d’un notebook pour expliquer comment préparer la donnée de l’INPI \n",
    "* Status: Active\n",
    "  * Change Status task: Active\n",
    "  * Update table: Modify rows\n",
    "* Source URL: US 02 Filtrage et enrichissement data INPI\n",
    "* Task type: Jupyter Notebook\n",
    "* Users: Thomas Pernet\n",
    "* Watchers: Thomas Pernet\n",
    "* User Account: https://937882855452.signin.aws.amazon.com/console\n",
    "* Estimated Log points: 10\n",
    "* Task tag: #data-preparation,#documentation,#inpi\n",
    "* Toggl Tag: #documentation\n",
    "\n",
    "\n",
    "# Destination Output/Delivery\n",
    "\n",
    "## Table/file\n",
    "\n",
    "* GitHub:\n",
    "  * https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/10_sumup_preparation/00_filtrage_enrichissement_inpi.md\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connexion serveur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsPy.aws_authorization import aws_connector\n",
    "from awsPy.aws_athena import service_athena\n",
    "from awsPy.aws_s3 import service_s3\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os, shutil\n",
    "from itertools import chain\n",
    "\n",
    "path = os.getcwd()\n",
    "parent_path = str(Path(path).parent)\n",
    "path_cred = r\"{}/credential_AWS.json\".format(parent_path)\n",
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                       region = 'eu-west-3')\n",
    "\n",
    "region = 'eu-west-3'\n",
    "bucket = 'calfdata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                       region = region)\n",
    "client= con.client_boto()\n",
    "s3 = service_s3.connect_S3(client = client,\n",
    "                      bucket = bucket, verbose = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_setting = True\n",
    "if pandas_setting:\n",
    "    cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La donnée de l'INPI \n",
    "\n",
    "Dans l'introduction, nous avons mentionné qu'une transmission de dossier peut être étaller sur plusieurs mois voir année. Il est donc impératif d'intégrer les dossiers de manière séquencielle, et la dernière transmission est prioritaire sur les précédentes. \n",
    "\n",
    "## Plusieurs transmission par date de greffe\n",
    "\n",
    "Il y a deux autres points d'attention qu'il faut prendre en compte. Le premier est en lien étroit avec la manière dont sont transmis les dossiers. Les greffiers peuvent transmettre les informations aux comptes sur plusieurs mois, années, comme indiqué précédement, mais plus bizarement par jour. Effectivement, le même numéro de dossier peut avoir plusieurs transmissions le même jour, ce qui signifie que le CSV peut possséder plusieurs lignes pour un dossier et date donnée. \n",
    "\n",
    "Avoir plusieurs dates de transmission pour un même dossier ne serait pas problématique si chaque ligne contenait l'ensemble des informations contenu dans le schéma de donnée. Les données d'identification sont toujours présentes, mais pour le reste l'INPI ne trasnmet que les variations d'une ligne à l'autre. Le tableau ci dessous est un exemple de ce cas de figure:\n",
    "\n",
    "Le quadruplet Code greffe, 1303, numéro de gestion,\t2003A01166, siren, 450687512, ID Etablissement 3 possède 4 transmission datant du 20170802. \n",
    "\n",
    "Dans l'exemple affiché, nous devons récupérer la dernière ligne (4) car c'est celle qui a été transmise en dernier. Toutefois, il manque l'information sur l'enseigne qui a été communiqué en ligne 2. Ainsi, il est indispensable d'enrichir les informations d'une ligne a l'autre. Il faut garder en tête que la ligne la plus récente prévaut sur la précédente en cas de différence. Finalement, nous ne devons avoir qu'une seule ligne par quadruplet pour une date de greffe donnée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key ='INPI/TC_1/00_RawData/public/IMR_Donnees_Saisies/tc/flux/2017/08/02/1303/112/1303_112_20170802_090910_9_ets_nouveau_modifie_EVT.csv'\n",
    "(\n",
    "    s3.read_df_from_s3(\n",
    "    key = key,\n",
    "                   sep = ';'\n",
    ")\n",
    "    #.sort_values(by = 'Siren')\n",
    "    .loc[lambda  x : \n",
    "         (x['Siren'].isin(['450687512']))\n",
    "        & (x['ID_Etablissement'].isin([3]))\n",
    "        ]\n",
    "    .reset_index()\n",
    "    .head(4)\n",
    "    .T\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le cas de figure que nous venons d'évoquer reste le même lorsque la trasnmission c'est faite a des dates différentes. Autrement dit, le filtrage et l'enrichissement se fait intra jour et intra quadruplet par date de greffe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dossier transmis en tant que partiel\n",
    "\n",
    "La transmission de dossier de la part du greffe vers l'INPI peut dans certains cas contenir des erreurs ou anomalies. L'INPI indique alors la procédure a suivre. \n",
    "Les greffes des Tribunaux de commerce peuvent être amenés à effectuer des corrections sur des dossiers selon deux modes :\n",
    "\n",
    "* Soit sous forme de fichier de flux à traiter selon les règles habituelles d’intégration des mises à jour (corrections mineures),\n",
    "* Soit sous forme de dossier complet retransmis dans le répertoire de stock (ie stocks partiels), à retraiter en annule et remplace (corrections majeures).\n",
    "  * C’est le cas en particulier lorsque il y a incohérence entre des identifiants qui auraient été livrés dans le stock initial et ceux livrés dans le flux (ex. fichiers des établissements, représentants, observations) pour un même dossier (siren/numéro de gestion/code greffe). C’est également le cas de dossiers qui auraient été absents du stock initial et qui seraient retransmis après un délai.\n",
    "  * Dans ce cas, toutes les données qui ont pu être transmises antérieurement via le stock initial ou le flux doivent donc être ignorées (prendre en compte la date de transmission indiquée dans le nom des sous-répertoires du stock et des fichiers cf. description des répertoires de stock TC ci-dessus).\n",
    "  \n",
    "Autrement dit, si la modification est mineure, elle sera disponible dans les événements, sinon, il faudra prendre le CSV le plus récent de la branche stock du FTP, et annuler toutes les lignes précédentes, même si il y a des événements. Un partiel vient corriger et faire une remise a zéro du dossier.\n",
    "\n",
    "Le tableau ci dessous est un exemple de correction de dossier. Le quadruplet Code greffe, 9301, numéro de gestion,\t2019B10958, siren, 878606615, ID Etablissement 1 a connu un transmission de partiel le 20191125 venant corriger les deux précédentes transmissions. La correction corrige l'adresse qui a été mal transmise lors de la création de l'établissement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key1 ='INPI/TC_1/00_RawData/public/IMR_Donnees_Saisies/tc/flux/2019/11/08/9301/1637/9301_1637_20191108_091055_8_ets.csv'\n",
    "key2 ='INPI/TC_1/00_RawData/public/IMR_Donnees_Saisies/tc/flux/2019/11/15/9301/1653/9301_1653_20191115_084921_9_ets_nouveau_modifie_EVT.csv'\n",
    "key3 = 'INPI/TC_1/01_donnee_source/Stock/Stock_Partiel/2019/ETS/9301_S7_20191125_8_ets.csv'\n",
    "(\n",
    "     pd.concat(map(\n",
    "         lambda x: \n",
    "         (s3.read_df_from_s3(x, sep = \";\")\n",
    "          .loc[lambda  x : \n",
    "         (x['Siren'].isin(['878606615']))\n",
    "        & (x['ID_Etablissement'].isin([1]))\n",
    "              ]\n",
    "         )\n",
    "         , [key1, key2, key3]\n",
    "              )\n",
    ")\n",
    "    .T\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrage et enrichissement \n",
    "\n",
    "Dans le point sur **Plusieurs transmission par date de greffe**, nous avons mentionné la nécéssité de filtrer et d'enrichir les lignes les plus récentes avec leur antécédent le plus proche pour ne contenir qu'une seule ligne pour le quadruplet et une date de greffe donnée. La logique d'enrichissment de la donnée doit aussi être éffectuée entre les dates de greffes. Le greffe ne va transmettre que les changements d'information d'une date de greffe a une autre. Les variables d'identification vont bien sur être indiquées. A partir du moment ou un champs a été rempli, et non modifié, nous allons devoir le remplir pour chacun des événements transmit. La seule possibilité ou le remplissage n'a pas lieu d'être est lorsque l'INPI transmet un partiel. Le partiel va corriger et annuler toutes les lignes précédentes.\n",
    "\n",
    "En résumé, chaque transmission pour une date de greffe ne doit posséder qu'une seule ligne. C'est le cas de figure indiqué dans le point **Plusieurs transmission par date de greffe**. Ensuite, un enrichissement de la donnée doit se faire entre les dates de greffes. Par exemple, si un quadruplet possède le schéma suivant: Création, événement 1 et suppression, alors la table finale aura 3 lignes, avec un découlement de l'information entre la création et la suppression. L'une des différences entre la ligne 1 et la ligne 3 est la modification des informations induite par la ligne 2.\n",
    "\n",
    "Les tables ci dessous illuste ce cas de figure.\n",
    "\n",
    "Le tableau 1 regroupe toutes les informations brutes. Le quadruplet code_greffe, 1101, numero_gestion, 2000D00074, siren, 331319582, id_etablissement, 1 a fait l'objet de 5 transmissions de la part du greffe, et contient en tout 8 ligne. En regardant de plus prêt, on peut constater que les dates de greffes 2017-12-18 et 2018-09-12 ont plusieurs transmissions, avec des informations non renseignés. La dernière ligne étant un partiel, elle va annuler tout ce qui s'est passé précédement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key1 ='INPI/TC_1/01_donnee_source/Stock/Stock_Initial/2017/ETS/1101_S1_20170504_8_ets.csv'\n",
    "key2 ='INPI/TC_1/00_RawData/public/IMR_Donnees_Saisies/tc/flux/2017/12/29/1101/162/1101_162_20171229_085906_9_ets_nouveau_modifie_EVT.csv'\n",
    "key3 ='INPI/TC_1/00_RawData/public/IMR_Donnees_Saisies/tc/flux/2017/12/19/1101/155/1101_155_20171219_085917_9_ets_nouveau_modifie_EVT.csv'\n",
    "key4 ='INPI/TC_1/01_donnee_source/Flux/2018/ETS/EVT/1101_167_20180106_201232_9_ets_nouveau_modifie_EVT.csv'\n",
    "key5 ='INPI/TC_1/00_RawData/public/IMR_Donnees_Saisies/tc/flux/2018/08/02/1101/310/1101_310_20180802_070250_9_ets_nouveau_modifie_EVT.csv'\n",
    "key6 ='INPI/TC_1/00_RawData/public/IMR_Donnees_Saisies/tc/flux/2018/09/13/1101/340/1101_340_20180913_055556_9_ets_nouveau_modifie_EVT.csv'\n",
    "key7 ='INPI/TC_1/01_donnee_source/Stock/Stock_Partiel/2019/ETS/1101_S2_20190506_8_ets.csv'\n",
    "\n",
    "(\n",
    "     pd.concat(map(\n",
    "         lambda x: \n",
    "         (s3.read_df_from_s3(x, sep = \";\")\n",
    "          .loc[lambda  x : \n",
    "         (x['Siren'].isin(['331319582']))\n",
    "        & (x['ID_Etablissement'].isin([1]))\n",
    "              ]\n",
    "         )\n",
    "         , [key1, key2, key3, key4, key5, key6, key7\n",
    "           ]\n",
    "              )\n",
    ")\n",
    "    .T\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La préparation de la data de l'INPI va consister a filtrer les lignes avec plusieurs dates de transmission et enrichir les champs manquants. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation artificielle d'établissement\n",
    "\n",
    "Lors de l'introduction sur les entreprises et établissements, nous avons détaillé le type de statut qu'un établissement peut avoir. Une entreprise qui est enregistrée au registre des entreprises doit avoir un siège. Le siège est l'adresse \"juridique\" de l'entreprise. L'entreprise peut posséder un établissement, domicilié à l'adresse ou elle réalise la plupart de son business. Il est très probable que le principal partage la même adresse que le siège. Finalement, tous les établissements en plus du siège et principal sont appelés \"secondaire\". \n",
    "\n",
    "Selon l'INSEE, le siret est l'identifiant permettant de distinguer un établissement d'un autre. L'INPI n'inclut pas le siret dans ses bases de données, mais identifie l'établissement via le quadruplet: siren, numéro de gestion, numéro de dossier et id établissement.\n",
    "\n",
    "Selon l'INSEE, le siret est attribué par le biais de l'adresse. Un établissement ayant pour siège et principal la même adresse va partager le même siren. D'un point de vue data, si le siret est à la fois siège et principal, alors il n'y aura qu'une seule ligne. \n",
    "\n",
    "L'INPI n'a pas la même rigueur que l'INSEE car elle à plus de trois labels possibles pour caractériser un établissement. Il y a \"SIE\", \"PRI\", \"SEP\" et \"SEC\". Le status \"SEP\" indique que l'établissement est à la fois siège et principal. Certains greffes vont utiliser ce label pour caractériser les établissements siège et principal, alors que d'autres vont utiliser \"SIE\" et \"PRI\". Dans les deux cas, l'adresse est identique mais l'identifiant va différer pour le deuxième groupe de greffe. A partir du moment ou le statut est différent, cela va engendrer à la création d'un nouvel ID. \n",
    "\n",
    "Cela pose un problème statistique lorsque nous parlons d'établissement au sens de l'INPI. L'INPI va gonfler artificiellement le nombre d'établissements à cause de la création d'un nouvel identifiant lorsque le greffe va créer deux lignes sur la même adresse.\n",
    "\n",
    "Dans l'exemple ci-dessous, le quadruplet  code_greffe, 7301, numero_gestion, 2001D00111, siren, 437864820, id_etablissement, 1/10 possède deux établissements au sens de l'INPI a cause du double label \"SIE\" et \"PRI\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key ='INPI/TC_1/01_donnee_source/Stock/Stock_Initial/2017/ETS/7301_S1_20170504_8_ets.csv'\n",
    "(\n",
    "    s3.read_df_from_s3(\n",
    "    key = key,\n",
    "                   sep = ';'\n",
    ")\n",
    "    #.sort_values(by = 'Siren')\n",
    "    .loc[lambda  x : \n",
    "         (x['Siren'].isin(['437864820']))\n",
    "        ]\n",
    "    .reset_index()\n",
    "    .T\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le greffe ne va cependant pas renseigner tous les champs lors de la création d'un établissement a double label. C'est souvent le siège ou l'information sur la date de début d'activité et l'activité sont manquantes. La raison est que le siège n'est pas corrélé à l'activité. Une entreprise peut ête légalement active sans forcément avoir un établissement en activité. \n",
    "\n",
    "Dans certains cas, cela peut poser problème si le greffe ne modifie pas les deux \"établissements\". Prenons l'exemple ou le greffe crée un établissement a double label, puis ne modifie que le principal. Nous ne pourrons pas changer les informations du siège, car la clé n'est pas la même (ID établissement différent). Maintenant, l'entrepreneur décide de fermer le principal, le greffe va transmettre la fermeture du principal à l'INPI. Toutefois, selon la définition d'établissement au sens de l'INPI, le siège est encore ouvert, mais le principal est fermé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Pour récapituler, la donnée de l'INPI est dispachée entre deux branches dans le FTP. La branche dite de stock, peut être subdivisée en deux groupes. Le groupe des stocks initiaux, qui regroupe toutes les informations sur les entreprises précédents la date du 04/05/2017. Le groupe des stocks partiel qui inclut toutes les corrections majeures des dossiers. La branche des flux contient toutes les informations relatives aux entreprises passée la date du 04/05/2017. Les informations vont être catégorisées selon si le dossier est une création, modification ou supression d'établissement. Chaque dossier est transmit par l'intermédiaire d'un CSV. Dès lors, une transmission fait référence à un CSV transmit par un greffe, a une date donnée. La date de transmission ne correspond pas à la date de greffe, qui est indiquée dans le CSV. Un dossier est identifié via le quadruplet siren, code greffe, numéro de gestion et id établissement.\n",
    "\n",
    "Le greffe va constituer un dossier a une date de greffe donnée, mais peut transmettre l'information au compte goute. Plus précisément, la transmission a l'INPI peut être faite sur un, deux, trois ou plus de jours, étalé sur plusieurs semaines, mois ou années. La transmission a deux particularités. Premièrement, un dossier peut avoir plusieurs lignes au sein d'un CSV. Le greffe ne va indiquer que les différences entre les lignes, mise a part les champs d'identification. Lorsque cela est le cas, il faut enrichir l'ensemble des champs en prenant la première valeur précédente non vide. En cas de divergence, il faut toujours prioriser la ligne la plus récente. Après avoir enrichi le dossier, il faut garder uniquement la dernière valeur pour n'avoir qu'une seule ligne par date de greffe.\n",
    "\n",
    "La vie d'une entreprise fait qu'un établissement fasse l'objet de plusieurs événements. Par exemple, une modification d'enseigne ou une suppression d'établissement. Le greffe va la encore appliquer la même logique de ne transmettre que les champs qui ont été modifié, avec les informations permettant d'identifier l'établissement. Il faut donc appliquer une deuxième fois l'enrichissement des lignes. Au final, un événement est relayé par une date de greffe via une ou plusieurs transmission. Pour éviter la redondance de l'information, un filtrage est effectué après avoir enrichi la donnée.\n",
    "\n",
    "Finalement, la transmission d'un partiel vient corriger les erreurs de dossiers, rendant caduque toutes les transmissions précédants la date du partiel.\n",
    "\n",
    "L'INPI peut gonfler le nombre d'établissements d'une entreprise lorsque le greffe labelise la même adresse en tant que \"SIE\" et \"PRI\".\n",
    "\n",
    "Pour conclure, la table finale va avoir une seule ligne par quadruplet et date de greffe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Préparation table ETS INPI filtrée et enrichie\n",
    "\n",
    "La préparation de la table des ETS se fait en 3 étapes:\n",
    "\n",
    "1. Création des tables\n",
    "2. Filtrage et enrichissement des flux intra day et intra date de greffe\n",
    "3. Enrichissements des lignes d'un événement a l'autre et filtrage des événements partiels\n",
    "\n",
    "![](https://app.lucidchart.com/publicSegments/view/5c24129a-f50a-4977-97b3-9a62eaa936b7/image.png)\n",
    "\n",
    "La première étape est relativement simple car elle consiste a créer les tables des stocks et des flux. L'arborescence du S3 est la suivante:\n",
    "\n",
    "```\n",
    "01_donnees_source\n",
    "    ├───Flux\n",
    "    │   ├───2017\n",
    "    │   │   ├───ETS\n",
    "    │   │   │   ├───EVT\n",
    "    │   │   │   └───NEW\n",
    "    │   ├───2018\n",
    "    │   │   ├───ETS\n",
    "    │   │   │   ├───EVT\n",
    "    │   │   │   └───NEW\n",
    "    │   └───2019\n",
    "    │   │   ├───ETS\n",
    "    │   │   │   ├───EVT\n",
    "    │   │   │   └───NEW\n",
    "    └───Stock\n",
    "        ├───Stock_initial\n",
    "            ├───2017\n",
    "            │   ├───ETS\n",
    "        └───Stock_partiel\n",
    "            ├───2018\n",
    "            │   ├───ETS\n",
    "            ├───2019\n",
    "            │   ├───ETS\n",
    "            └───2020\n",
    "                ├───ETS\n",
    "```\n",
    "\n",
    "Dans la seconde étape, nous allons concatener les tables des partiels et des flux. De plus nous allons filtrer et enrichir la donnée des flux. L'enrichissement va se faire greffe par greffe car la donnée est trop volumineuse pour etre traité en un seul bloc. La technique que nous avons utilisé n'a pas été optimisé ce qui pousse a faire un traitement brique par brique. Dès que l'enrichissement au niveau du timestamp est fait, il faut répliquer l'opération au niveau de la date de greffe.\n",
    "\n",
    "La troisième est dernière étape est divisée en trois partie. Dans un premier temps, il faut concatener les tables de stock et des flux filtrés et enrichis, ensite il est nécéssaire de filtrer les événements précédents un partiel. Finalement, il faut enrichir la donnée d'un événement a un autre.\n",
    "\n",
    "Un point de rappel sur les règles de gestion appliquées\n",
    "\n",
    "- Une séquence est un classement chronologique pour le quadruplet suivant:\n",
    "    - siren + code greffe + numero gestion + ID établissement pour les Etablissements\n",
    "- Une ligne événement ne modifie que le champs comportant la modification. Les champs non modifiés vont être remplis par la ligne t-1\n",
    "- Une ligne partiel va rendre caduque l'ensemble des séquences précédentes.\n",
    "- Le remplissage doit se faire de deux façons\n",
    "    - une première fois avec la date de transmission (plusieurs informations renseignées pour une meme date de transmission pour une même séquence). La dernière ligne remplie des valeurs précédentes de la séquence -> \n",
    "2. Filtrage et enrichissement des \n",
    "    - une seconde fois en ajoutant les valeurs non renseignées pour cet évènement, en se basant sur les informations des lignes précédentes du triplet (quadruplet pour les Etablissements). Les lignes précédentes ont une date de transmission différente et/ou initial, partiel et création. -> Flux entre les événements "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation json parameters\n",
    "\n",
    "Pour faciliter l'ingestion de données en batch, on prépare un json `parameters` avec les paths où récupérer la data, le nom des tables, les origines, mais aussi un champ pour récupérer l'ID de l'execution dans Athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ = []\n",
    "for i in  [\n",
    "\"type\",\n",
    "\"Siege_PM\",\n",
    "\"RCS_Registre\",\n",
    "\"Adresse_Ligne1\",\n",
    "\"Adresse_Ligne2\",\n",
    "\"Adresse_Ligne3\",\n",
    "\"Code_Postal\",\n",
    "\"Ville\",\n",
    "\"Code_Commune\",\n",
    "\"Pays\",\n",
    "\"Domiciliataire_Nom\",\n",
    "\"Domiciliataire_Siren\",\n",
    "\"Domiciliataire_Greffe\",\n",
    "\"Domiciliataire_Complement\",\n",
    "\"Siege_Domicile_Representant\",\n",
    "\"Nom_Commercial\",\n",
    "\"Enseigne\",\n",
    "\"Activite_Ambulante\",\n",
    "\"Activite_Saisonniere\",\n",
    "\"Activite_Non_Sedentaire\",\n",
    "\"Date_Debut_Activite\",\n",
    "\"Activite\",\n",
    "\"Origine_Fonds\",\n",
    "\"Origine_Fonds_Info\",\n",
    "\"Type_Exploitation\",\n",
    "\"max_partiel\",\n",
    "\"csv_source\"\n",
    "]:\n",
    "    list_.append(i.lower())\n",
    "list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'step_x':{\n",
    "           'query':{\n",
    "               'top': {},\n",
    "               'middle': {}, \n",
    "               'bottom' : {}\n",
    "           },\n",
    "    \"output_id\":[]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters ={\n",
    "   \"global\":{\n",
    "      \"database\":\"ets_inpi\",\n",
    "      \"output\":\"INPI/sql_output\",\n",
    "      \"output_preparation\":\"INPI/sql_output_preparation\",\n",
    "      \"ETS_step4_id\":[\n",
    "         \n",
    "      ],\n",
    "      \"table_final_id\":{\n",
    "         \"ETS\":{\n",
    "            \n",
    "         }\n",
    "      }\n",
    "   },\n",
    "   \"steps\":{\n",
    "       'step_0':{\n",
    "           'query':{\n",
    "               'top':\"CREATE EXTERNAL TABLE IF NOT EXISTS {0}.{1} (\",\n",
    "               'bottom': \"\"\" )\n",
    "     ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
    "    WITH SERDEPROPERTIES (\n",
    "   'separatorChar' = '{0}',\n",
    "   'quoteChar' = '\"'\n",
    "   )\n",
    "     LOCATION '{1}'\n",
    "     TBLPROPERTIES ('has_encrypted_data'='false',\n",
    "              'skip.header.line.count'='1');\"\"\"\n",
    "               \n",
    "           }\n",
    "       },\n",
    "       'step_1':\n",
    "       {\n",
    "           'tables':['ets_partiel', 'ets_flux'],\n",
    "            'partionned': [\n",
    "               \"siren\", \"code_greffe\",\"nom_greffe\",\n",
    "               \"numero_gestion\", \"id_etablissement\", \n",
    "               \"file_timestamp\"\n",
    "           ],\n",
    "           'query':{\n",
    "               'top': \"\"\"\n",
    "                    CREATE TABLE {}.{}\n",
    "                    WITH (\n",
    "                    format='PARQUET'\n",
    "                    ) AS\n",
    "                    WITH append AS (\n",
    "                    SELECT * FROM {}\n",
    "              \"\"\",\n",
    "               'middle':\"\"\"\n",
    "                    UNION \n",
    "                    SELECT * FROM {} \"\"\",\n",
    "               'bottom': \"\"\"\n",
    "            )\n",
    "            SELECT * \n",
    "            FROM append\n",
    "            ORDER BY {}\n",
    "            \"\"\"\n",
    "           },\n",
    "           \"output_id\":[]\n",
    "       },\n",
    "       'step_2':\n",
    "       {\n",
    "           'tables': ['ets_flux_filtre_enrichie_timestamp','ets_flux_filtre_enrichie_date_greffe'],\n",
    "           'partionned': {\n",
    "               'time_stamp':\n",
    "               [\n",
    "               \"siren\", \"code_greffe\",\"nom_greffe\",\n",
    "               \"numero_gestion\", \"id_etablissement\", \n",
    "               \"file_timestamp\"\n",
    "           ],\n",
    "               'date_greffe':\n",
    "               [\n",
    "               \"siren\", \"code_greffe\",\"nom_greffe\",\n",
    "               \"numero_gestion\", \"id_etablissement\", \n",
    "               \"date_greffe\"\n",
    "           ],\n",
    "           },\n",
    "           \"path\":['s3://calfdata/INPI/TC_1/02_preparation_donnee/TEMP_ETS_FLUX',\n",
    "                   's3://calfdata/INPI/TC_1/02_preparation_donnee/TEMP_ETS_FLUX_FILTRE'\n",
    "                  ],\n",
    "           \"separator\":\",\",\n",
    "           'query':{\n",
    "               'top': {\n",
    "                   'first':\"\"\"\n",
    "WITH createID AS (\n",
    "  SELECT \n",
    "   *, \n",
    "    ROW_NUMBER() OVER (\n",
    "      PARTITION BY \n",
    "       {0}\n",
    "    ) As row_ID, \n",
    "    DENSE_RANK () OVER (\n",
    "      ORDER BY \n",
    "        {0}\n",
    "    ) As ID \n",
    "  FROM \n",
    "    {1}.{2} \n",
    "  WHERE {4} = '{3}'  \n",
    ") \n",
    "SELECT \n",
    "  * \n",
    "FROM \n",
    "  (\n",
    "    WITH filled AS (\n",
    "      SELECT \n",
    "        ID, \n",
    "        row_ID, \n",
    "        {0}, \n",
    "\"\"\",\n",
    "                   'second':\"\"\"first_value(\"{0}\") over (partition by ID, \"{0}_partition\" order by \n",
    "ID, row_ID\n",
    " ) as \"{0}\"\n",
    "\"\"\"\n",
    "               },\n",
    "               'middle':{\n",
    "                   'first':\"\"\"FROM \n",
    "        (\n",
    "          SELECT \n",
    "            *, \"\"\",\n",
    "                   'second':\"\"\"sum(case when \"{0}\" = '' then 0 else 1 end) over (partition by ID \n",
    "order by  row_ID) as \"{0}_partition\" \n",
    "\"\"\"\n",
    "               },\n",
    "               'bottom':\"\"\" \n",
    "          FROM \n",
    "            createID \n",
    "          ORDER BY \n",
    "            ID, row_ID ASC\n",
    "        ) \n",
    "      ORDER BY \n",
    "        ID, \n",
    "        row_ID\n",
    "    ) \n",
    "    SELECT\n",
    "    {1},\n",
    "    {0},\n",
    "CASE WHEN siren IS NOT NULL THEN 'EVT' \n",
    "ELSE NULL END as origin\n",
    "    FROM \n",
    "      (\n",
    "        SELECT \n",
    "          *, \n",
    "          ROW_NUMBER() OVER(\n",
    "            PARTITION BY ID \n",
    "            ORDER BY \n",
    "              ID, row_ID DESC\n",
    "          ) AS max_value \n",
    "        FROM \n",
    "          filled\n",
    "      ) AS T \n",
    "    WHERE \n",
    "      max_value = 1\n",
    "  )ORDER BY {1}\n",
    "\"\"\"\n",
    "           },\n",
    "           'to_fill':{\n",
    "           'time_stamp':['libelle_evt',\n",
    " 'date_greffe',\n",
    " 'type',\n",
    " 'siege_pm',\n",
    " 'rcs_registre',\n",
    " 'adresse_ligne1',\n",
    " 'adresse_ligne2',\n",
    " 'adresse_ligne3',\n",
    " 'code_postal',\n",
    " 'ville',\n",
    " 'code_commune',\n",
    " 'pays',\n",
    " 'domiciliataire_nom',\n",
    " 'domiciliataire_siren',\n",
    " 'domiciliataire_greffe',\n",
    " 'domiciliataire_complement',\n",
    " 'siege_domicile_representant',\n",
    " 'nom_commercial',\n",
    " 'enseigne',\n",
    " 'activite_ambulante',\n",
    " 'activite_saisonniere',\n",
    " 'activite_non_sedentaire',\n",
    " 'date_debut_activite',\n",
    " 'activite',\n",
    " 'origine_fonds',\n",
    " 'origine_fonds_info',\n",
    " 'type_exploitation',\n",
    " 'csv_source'],\n",
    "               'date_greffe':[\n",
    "                   'libelle_evt',\n",
    " 'type',\n",
    " 'siege_pm',\n",
    " 'rcs_registre',\n",
    " 'adresse_ligne1',\n",
    " 'adresse_ligne2',\n",
    " 'adresse_ligne3',\n",
    " 'code_postal',\n",
    " 'ville',\n",
    " 'code_commune',\n",
    " 'pays',\n",
    " 'domiciliataire_nom',\n",
    " 'domiciliataire_siren',\n",
    " 'domiciliataire_greffe',\n",
    " 'domiciliataire_complement',\n",
    " 'siege_domicile_representant',\n",
    " 'nom_commercial',\n",
    " 'enseigne',\n",
    " 'activite_ambulante',\n",
    " 'activite_saisonniere',\n",
    " 'activite_non_sedentaire',\n",
    " 'date_debut_activite',\n",
    " 'activite',\n",
    " 'origine_fonds',\n",
    " 'origine_fonds_info',\n",
    " 'type_exploitation',\n",
    " 'csv_source'] },\n",
    "           'year':[2017, 2018,2019],\n",
    "          'code_greffe': [\n",
    "\n",
    "\"1801\",\n",
    "\"7803\",\n",
    "\"0301\",\n",
    "\"1104\",\n",
    "\"8101\",\n",
    "\"4801\",\n",
    "\"6601\",\n",
    "\"2801\",\n",
    "\"2104\",\n",
    "\"1402\",\n",
    "\"1601\",\n",
    "\"0605\",\n",
    "\"8302\",\n",
    "\"7601\",\n",
    "\"5601\",\n",
    "\"2702\",\n",
    "\"7608\",\n",
    "\"2401\",\n",
    "\"6201\",\n",
    "\"8201\",\n",
    "\"5103\",\n",
    "\"1407\",\n",
    "\"5401\",\n",
    "\"7001\",\n",
    "\"1704\",\n",
    "\"5802\",\n",
    "\"6901\",\n",
    "\"6403\",\n",
    "\"4202\",\n",
    "\"0901\",\n",
    "\"7301\",\n",
    "\"3502\",\n",
    "\"4401\",\n",
    "\"5501\",\n",
    "\"9401\",\n",
    "\"1101\",\n",
    "\"7802\",\n",
    "\"4502\",\n",
    "\"8801\",\n",
    "\"1708\",\n",
    "\"3402\",\n",
    "\"7901\",\n",
    "\"2602\",\n",
    "\"2001\",\n",
    "\"6401\",\n",
    "\"0602\",\n",
    "\"3902\",\n",
    "\"5602\",\n",
    "\"3405\",\n",
    "\"6502\",\n",
    "\"4901\",\n",
    "\"8002\",\n",
    "\"5906\",\n",
    "\"8102\",\n",
    "\"0603\",\n",
    "\"0202\",\n",
    "\"7801\",\n",
    "\"4302\",\n",
    "\"7701\",\n",
    "\"2402\",\n",
    "\"5002\",\n",
    "\"6101\",\n",
    "\"5910\",\n",
    "\"1303\",\n",
    "\"3303\",\n",
    "\"0702\",\n",
    "\"8602\",\n",
    "\"9301\",\n",
    "\"4601\",\n",
    "\"5902\",\n",
    "\"1301\",\n",
    "\"6303\",\n",
    "\"4201\",\n",
    "\"6202\",\n",
    "\"3801\",\n",
    "\"6002\",\n",
    "\"7102\",\n",
    "\"2301\",\n",
    "\"5001\",\n",
    "\"5402\",\n",
    "\"9001\",\n",
    "\"7106\",\n",
    "\"2002\",\n",
    "\"4001\",\n",
    "\"3201\",\n",
    "\"4101\",\n",
    "\"0501\",\n",
    "\"3003\",\n",
    "\"1501\",\n",
    "\"4402\",\n",
    "\"8903\",\n",
    "\"0203\",\n",
    "\"5952\",\n",
    "\"2202\",\n",
    "\"1304\",\n",
    "\"3701\",\n",
    "\"8701\",\n",
    "\"3302\",\n",
    "\"3501\",\n",
    "\"0802\",\n",
    "\"6903\",\n",
    "\"5101\",\n",
    "\"4701\",\n",
    "\"3102\",\n",
    "\"1203\",\n",
    "\"3802\",\n",
    "\"2501\",\n",
    "\"8901\",\n",
    "\"2903\",\n",
    "\"2701\",\n",
    "\"6001\",\n",
    "\"5201\",\n",
    "\"8305\",\n",
    "\"1901\",\n",
    "\"7702\",\n",
    "\"8401\",\n",
    "\"7202\",\n",
    "\"7501\",\n",
    "\"9201\",\n",
    "\"8501\",\n",
    "\"7401\",\n",
    "\"7606\",\n",
    "\"0101\",\n",
    "\"7402\",\n",
    "\"0601\",\n",
    "\"8303\",\n",
    "\"0303\",\n",
    "\"5301\",\n",
    "\"3601\",\n",
    "\"1305\",\n",
    "\"4002\",\n",
    "\"2901\",\n",
    "\"1001\",\n",
    "\"0401\"\n",
    "],\n",
    "           \"output_id\":[]\n",
    "       },\n",
    "    'step_3':{\n",
    "        'tables': ['ets_filtre_enrichi_historique_tmp','ets_filtre_enrichie_historique'],\n",
    "        'partionned': {\n",
    "               'date_greffe':\n",
    "               [\n",
    "               \"siren\", \"code_greffe\",\"nom_greffe\",\n",
    "               \"numero_gestion\", \"id_etablissement\", \n",
    "               \"date_greffe\"\n",
    "           ]},\n",
    "        'to_fill':\n",
    "['type',\n",
    " 'siege_pm',\n",
    " 'rcs_registre',\n",
    " 'adresse_ligne1',\n",
    " 'adresse_ligne2',\n",
    " 'adresse_ligne3',\n",
    " 'code_postal',\n",
    " 'ville',\n",
    " 'code_commune',\n",
    " 'pays',\n",
    " 'domiciliataire_nom',\n",
    " 'domiciliataire_siren',\n",
    " 'domiciliataire_greffe',\n",
    " 'domiciliataire_complement',\n",
    " 'siege_domicile_representant',\n",
    " 'nom_commercial',\n",
    " 'enseigne',\n",
    " 'activite_ambulante',\n",
    " 'activite_saisonniere',\n",
    " 'activite_non_sedentaire',\n",
    " 'date_debut_activite',\n",
    " 'activite',\n",
    " 'origine_fonds',\n",
    " 'origine_fonds_info',\n",
    " 'type_exploitation',\n",
    " 'csv_source'],\n",
    "        \n",
    "           'query':{\n",
    "               'preparation':\"\"\"\n",
    "               CREATE TABLE ets_inpi.ets_filtre_enrichi_historique_tmp WITH (format = 'PARQUET') AS \n",
    "WITH concat_ AS (\n",
    "SELECT \n",
    "  siren, \n",
    "  code_greffe, \n",
    "  nom_greffe, \n",
    "  numero_gestion, \n",
    "  id_etablissement, \n",
    "  Coalesce(\n",
    "         try(date_parse(date_greffe, '%Y-%m-%d')),\n",
    "         try(date_parse(date_greffe, '%Y/%m/%d')),\n",
    "         try(date_parse(date_greffe, '%d %M %Y')),\n",
    "         try(date_parse(date_greffe, '%d/%m/%Y')),\n",
    "         try(date_parse(date_greffe, '%d-%m-%Y'))\n",
    "  )\n",
    "  as date_greffe, \n",
    "  libelle_evt,\n",
    "  type, \n",
    "  siege_pm, \n",
    "  rcs_registre, \n",
    "  adresse_ligne1, \n",
    "  adresse_ligne2, \n",
    "  adresse_ligne3, \n",
    "  code_postal, \n",
    "  code_commune, \n",
    "  pays, \n",
    "  domiciliataire_nom, \n",
    "  domiciliataire_siren, \n",
    "  domiciliataire_greffe, \n",
    "  domiciliataire_complement, \n",
    "  siege_domicile_representant, \n",
    "  enseigne, \n",
    "  activite_ambulante, \n",
    "  activite_saisonniere, \n",
    "  activite_non_sedentaire, \n",
    "  date_debut_activite, \n",
    "  activite, \n",
    "  origine_fonds, \n",
    "  origine_fonds_info, \n",
    "  ville,\n",
    "  nom_commercial,\n",
    "  type_exploitation,\n",
    "  csv_source,\n",
    "  'FLUX' AS origin \n",
    "FROM \n",
    "  ets_flux_filtre_enrichie_date_greffe \n",
    "UNION \n",
    "  (\n",
    "    SELECT \n",
    "      siren, \n",
    "      code_greffe, \n",
    "      nom_greffe, \n",
    "      numero_gestion, \n",
    "      id_etablissement, \n",
    "      Coalesce(\n",
    "         try(date_parse(date_greffe, '%Y-%m-%d')),\n",
    "         try(date_parse(date_greffe, '%Y/%m/%d')),\n",
    "         try(date_parse(date_greffe, '%d %M %Y')),\n",
    "         try(date_parse(date_greffe, '%d/%m/%Y')),\n",
    "         try(date_parse(date_greffe, '%d-%m-%Y'))\n",
    "  )\n",
    "  as date_greffe, \n",
    "  libelle_evt,\n",
    "      type, \n",
    "      siege_pm, \n",
    "      rcs_registre, \n",
    "      adresse_ligne1, \n",
    "      adresse_ligne2, \n",
    "      adresse_ligne3, \n",
    "      code_postal, \n",
    "      code_commune, \n",
    "      pays, \n",
    "      domiciliataire_nom, \n",
    "      domiciliataire_siren, \n",
    "      domiciliataire_greffe, \n",
    "      domiciliataire_complement, \n",
    "      siege_domicile_representant, \n",
    "      enseigne, \n",
    "      activite_ambulante, \n",
    "      activite_saisonniere, \n",
    "      activite_non_sedentaire, \n",
    "      date_debut_activite, \n",
    "      activite, \n",
    "      origine_fonds, \n",
    "      origine_fonds_info, \n",
    "      ville,\n",
    "  nom_commercial,\n",
    "  type_exploitation,\n",
    "      csv_source,\n",
    "       'INITIAL' AS origin \n",
    "    FROM \n",
    "      ets_initial\n",
    "  ) \n",
    "UNION \n",
    "  (\n",
    "    SELECT \n",
    "      date_.siren, \n",
    "      date_.code_greffe, \n",
    "      date_.nom_greffe, \n",
    "      date_.numero_gestion, \n",
    "      date_.id_etablissement, \n",
    "      Coalesce(\n",
    "         try(date_parse(date_greffe, '%Y-%m-%d')),\n",
    "         try(date_parse(date_greffe, '%Y/%m/%d')),\n",
    "         try(date_parse(date_greffe, '%d %M %Y')),\n",
    "         try(date_parse(date_greffe, '%d/%m/%Y')),\n",
    "         try(date_parse(date_greffe, '%d-%m-%Y'))\n",
    "  )\n",
    "  as date_greffe, \n",
    "  libelle_evt,\n",
    "      type, \n",
    "      siege_pm, \n",
    "      rcs_registre, \n",
    "      adresse_ligne1, \n",
    "      adresse_ligne2, \n",
    "      adresse_ligne3, \n",
    "      code_postal, \n",
    "      code_commune, \n",
    "      pays, \n",
    "      domiciliataire_nom, \n",
    "      domiciliataire_siren, \n",
    "      domiciliataire_greffe, \n",
    "      domiciliataire_complement, \n",
    "      siege_domicile_representant, \n",
    "      enseigne, \n",
    "      activite_ambulante, \n",
    "      activite_saisonniere, \n",
    "      activite_non_sedentaire, \n",
    "      date_debut_activite, \n",
    "      activite, \n",
    "      origine_fonds, \n",
    "      origine_fonds_info, \n",
    "      ville,\n",
    "  nom_commercial,\n",
    "  type_exploitation,\n",
    "      csv_source, \n",
    "      'PARTIEL' AS origin \n",
    "    FROM \n",
    "      (\n",
    "        SELECT \n",
    "          siren, \n",
    "          code_greffe, \n",
    "          nom_greffe, \n",
    "          numero_gestion, \n",
    "          id_etablissement, \n",
    "          date_greffe, \n",
    "          type, \n",
    "          libelle_evt,\n",
    "          siege_pm, \n",
    "          rcs_registre, \n",
    "          adresse_ligne1, \n",
    "          adresse_ligne2, \n",
    "          adresse_ligne3, \n",
    "          code_postal, \n",
    "          code_commune, \n",
    "          pays, \n",
    "          domiciliataire_nom, \n",
    "          domiciliataire_siren, \n",
    "          domiciliataire_greffe, \n",
    "          domiciliataire_complement, \n",
    "          siege_domicile_representant, \n",
    "          enseigne, \n",
    "          activite_ambulante, \n",
    "          activite_saisonniere, \n",
    "          activite_non_sedentaire, \n",
    "          date_debut_activite, \n",
    "          activite, \n",
    "          origine_fonds, \n",
    "          origine_fonds_info, \n",
    "          ville,\n",
    "  nom_commercial,\n",
    "  type_exploitation,\n",
    "          csv_source, \n",
    "          Coalesce(\n",
    "            try(\n",
    "              cast(file_timestamp as timestamp)\n",
    "            )\n",
    "          ) as file_timestamp \n",
    "        FROM \n",
    "          ets_partiel\n",
    "      ) as date_ \n",
    "      INNER JOIN (\n",
    "        SELECT \n",
    "          siren, \n",
    "          code_greffe, \n",
    "          nom_greffe, \n",
    "          numero_gestion, \n",
    "          id_etablissement, \n",
    "          MAX(\n",
    "            Coalesce(\n",
    "              try(\n",
    "                cast(file_timestamp as timestamp)\n",
    "              )\n",
    "            )\n",
    "          ) as file_timestamp \n",
    "        FROM \n",
    "          ets_partiel \n",
    "        GROUP BY \n",
    "          siren, \n",
    "          code_greffe, \n",
    "          nom_greffe, \n",
    "          numero_gestion, \n",
    "          id_etablissement\n",
    "      ) as max_ ON date_.siren = max_.siren \n",
    "      AND date_.code_greffe = max_.code_greffe \n",
    "      AND date_.nom_greffe = max_.nom_greffe \n",
    "      AND date_.numero_gestion = max_.numero_gestion \n",
    "      AND date_.id_etablissement = max_.id_etablissement \n",
    "      AND date_.file_timestamp = max_.file_timestamp\n",
    "  ) \n",
    "ORDER BY \n",
    "  siren, \n",
    "  code_greffe, \n",
    "  nom_greffe, \n",
    "  numero_gestion, \n",
    "  id_etablissement, \n",
    "  date_greffe\n",
    ")\n",
    "SELECT    concat_.siren, \n",
    "          concat_.code_greffe, \n",
    "          concat_.nom_greffe, \n",
    "          concat_.numero_gestion, \n",
    "          concat_.id_etablissement, \n",
    "          libelle_evt,\n",
    "          date_greffe, \n",
    "          type, \n",
    "          siege_pm, \n",
    "          rcs_registre, \n",
    "          adresse_ligne1, \n",
    "          adresse_ligne2, \n",
    "          adresse_ligne3, \n",
    "          code_postal, \n",
    "          code_commune, \n",
    "          pays, \n",
    "          domiciliataire_nom, \n",
    "          domiciliataire_siren, \n",
    "          domiciliataire_greffe, \n",
    "          domiciliataire_complement, \n",
    "          siege_domicile_representant, \n",
    "          enseigne, \n",
    "          activite_ambulante, \n",
    "          activite_saisonniere, \n",
    "          activite_non_sedentaire, \n",
    "          date_debut_activite, \n",
    "          activite, \n",
    "          origine_fonds, \n",
    "          origine_fonds_info, \n",
    "          ville,\n",
    "  nom_commercial,\n",
    "  type_exploitation,\n",
    "          csv_source,\n",
    "          origin,\n",
    "          CASE WHEN date_greffe <= date_greffe_max AND origin != 'PARTIEL' THEN 'IGNORE' ELSE NULL END as status\n",
    "\n",
    "FROM concat_\n",
    "LEFT JOIN (\n",
    "  SELECT siren, code_greffe, nom_greffe, numero_gestion, id_etablissement, date_greffe\n",
    " as date_greffe_max\n",
    "  FROM concat_ \n",
    "  WHERE origin = 'PARTIEL'\n",
    "  ) as partiel\n",
    "  ON \n",
    "  concat_.siren = partiel.siren AND\n",
    "  concat_.code_greffe = partiel.code_greffe AND\n",
    "  concat_.nom_greffe = partiel.nom_greffe AND\n",
    "  concat_.numero_gestion = partiel.numero_gestion AND\n",
    "  concat_.id_etablissement = partiel.id_etablissement\"\"\",\n",
    "               'enrichissement':{\n",
    "               'top': {},\n",
    "               'middle': {}, \n",
    "               'bottom' : {}\n",
    "           }\n",
    "           },\n",
    "    \"output_id\":[]\n",
    "    }\n",
    "},\n",
    "   \"schema\":{\n",
    "      \"name\":['code_greffe',\n",
    " 'nom_greffe',\n",
    " 'numero_gestion',\n",
    " 'siren',\n",
    " 'type',\n",
    " 'siege_pm',\n",
    " 'rcs_registre',\n",
    " 'adresse_ligne1',\n",
    " 'adresse_ligne2',\n",
    " 'adresse_ligne3',\n",
    " 'code_postal',\n",
    " 'ville',\n",
    " 'code_commune',\n",
    " 'pays',\n",
    " 'domiciliataire_nom',\n",
    " 'domiciliataire_siren',\n",
    " 'domiciliataire_greffe',\n",
    " 'domiciliataire_complement',\n",
    " 'siege_domicile_representant',\n",
    " 'nom_commercial',\n",
    " 'enseigne',\n",
    " 'activite_ambulante',\n",
    " 'activite_saisonniere',\n",
    " 'activite_non_sedentaire',\n",
    " 'date_debut_activite',\n",
    " 'activite',\n",
    " 'origine_fonds',\n",
    " 'origine_fonds_info',\n",
    " 'type_exploitation',\n",
    " 'id_etablissement',\n",
    " 'date_greffe',\n",
    " 'libelle_evt',\n",
    " 'csv_source',\n",
    " 'nature',\n",
    " 'type_data',\n",
    " 'origin',\n",
    " 'file_timestamp'],\n",
    "      \"format\":[\n",
    "         \"string\",\n",
    "         \"string\",\n",
    "         \"string\",\n",
    "         \"string\",\n",
    "         \"string\",\n",
    "         \"string\",\n",
    "         \"string\",\n",
    "         \"string\",\n",
    "         \"string\",\n",
    "         \"string\",\n",
    "         \"string\",\n",
    "         \"string\",\n",
    "         \"string\",\n",
    "         \"string\",\n",
    "         \"string\",\n",
    "         \"string\",\n",
    "         \"string\",\n",
    "         \"string\",\n",
    "         \"string\",\n",
    "         \"string\",\n",
    "         \"string\",\n",
    "         \"string\",\n",
    "         \"string\",\n",
    "         \"string\",\n",
    "         \"string\",\n",
    "         \"string\",\n",
    "         \"string\",\n",
    "         \"string\",\n",
    "         \"string\",\n",
    "         \"string\",\n",
    "         \"string\",\n",
    "         \"string\",\n",
    "         \"string\",\n",
    "         \"string\",\n",
    "         \"string\",\n",
    "         \"string\",\n",
    "         \"string\"\n",
    "      ]\n",
    "   },\n",
    "   \"Tables\":{\n",
    "      \"Stock\":{\n",
    "         \"INITIAL\":{\n",
    "               \"path\": [\"s3://calfdata/INPI/TC_1/01_donnee_source/Stock/Stock_Initial/2017/ETS\"],\n",
    "               \"tables\": [\"ets_initial\"],\n",
    "               \"origin\":\"INITIAL\",\n",
    "               \"separator\":\";\",\n",
    "               \"output_id\":[\n",
    "                  \n",
    "               ]\n",
    "         },\n",
    "         \"PARTIEL\":{\n",
    "               \"path\":[\n",
    "                  \"s3://calfdata/INPI/TC_1/01_donnee_source/Stock/Stock_Partiel/2018/ETS\",\n",
    "                  \"s3://calfdata/INPI/TC_1/01_donnee_source/Stock/Stock_Partiel/2019/ETS\"\n",
    "               ],\n",
    "               \"tables\":[\n",
    "                  \"ets_partiel_2018\",\n",
    "                  \"ets_partiel_2019\"\n",
    "               ],\n",
    "               \"origin\":\"PARTIEL\",\n",
    "               \"separator\":\";\",\n",
    "               \"output_id\":[\n",
    "                  \n",
    "               ]\n",
    "         }\n",
    "      },\n",
    "      \"Flux\":{\n",
    "         \"NEW\":{\n",
    "               \"path\":[\n",
    "                  \"s3://calfdata/INPI/TC_1/01_donnee_source/Flux/2017/ETS/NEW\",\n",
    "                  \"s3://calfdata/INPI/TC_1/01_donnee_source/Flux/2018/ETS/NEW\",\n",
    "                  \"s3://calfdata/INPI/TC_1/01_donnee_source/Flux/2019/ETS/NEW\"\n",
    "               ],\n",
    "               \"tables\":[\n",
    "                  \"ets_new_2017\",\n",
    "                  \"ets_new_2018\",\n",
    "                  \"ets_new_2019\"\n",
    "               ],\n",
    "               \"origin\":\"NEW\",\n",
    "               \"separator\":\";\",\n",
    "               \"output_id\":[\n",
    "                  \n",
    "               ]\n",
    "         },\n",
    "         \"EVT\":{\n",
    "               \"path\":[\n",
    "                  \"s3://calfdata/INPI/TC_1/01_donnee_source/Flux/2017/ETS/EVT\",\n",
    "                  \"s3://calfdata/INPI/TC_1/01_donnee_source/Flux/2018/ETS/EVT\",\n",
    "                  \"s3://calfdata/INPI/TC_1/01_donnee_source/Flux/2019/ETS/EVT\"\n",
    "               ],\n",
    "               \"tables\":[\n",
    "                  \"ets_evt_2017\",\n",
    "                  \"ets_evt_2018\",\n",
    "                  \"ets_evt_2019\"\n",
    "               ],\n",
    "               \"origin\":\"EVT\",\n",
    "               \"separator\":\";\",\n",
    "               \"output_id\":[\n",
    "                  \n",
    "               ]\n",
    "         }\n",
    "      }\n",
    "   }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Creation tables\n",
    "\n",
    "Afin de ne pas mélanger l'ensemble des tables, nous allons créer 3 tables distinctes:\n",
    "\n",
    "- 1 table pour les stocks initiaux: `ets_stock`\n",
    "- 1 table pour les événements: `ets_flux`\n",
    "- 1 table pour les partiels: `ets_partiel`\n",
    "\n",
    "Etant donné que nous avons compartimenté les données par origine et année, nous devons créer une étape intermédiaire qui contient les tables par année"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"CREATE DATABASE ets_inpi\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On drop les tables si elles existent déjà."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = parameters['global']['database']\n",
    "s3_output = parameters['global']['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for origin in parameters['Tables'].items():\n",
    "    for key0, type_origin in origin[1].items():\n",
    "        for i, path in  enumerate(type_origin['path']):\n",
    "            query = \"DROP TABLE {}\".format(type_origin['tables'][i])\n",
    "            s3.run_query(query,\n",
    "                                  database = db,\n",
    "                                  s3_output = s3_output,\n",
    "                                  filename = None,\n",
    "                                  destination_key = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On créé les tables intermédiaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for origin in  tqdm(parameters['Tables'].items()):\n",
    "    for key0, type_origin in origin[1].items():\n",
    "        \n",
    "        for i, path in  enumerate(type_origin['path']):\n",
    "            table_top = \"\"\n",
    "            table_bottom = \"\"\n",
    "            middle = \"\"\n",
    "            table_top += parameters['steps']['step_0']['query']['top'].format(db, type_origin['tables'][i])#top.format(db, type_origin['tables'][i])\n",
    "            table_bottom += parameters['steps']['step_0']['query']['bottom'].format(type_origin['separator'], path)#bottom.format(type_origin['separator'], path)\n",
    "            ### ADD VARIABLES\n",
    "            for index, name in enumerate(parameters['schema']['name']):\n",
    "                if index == len(parameters['schema']['name'])-1:\n",
    "                    middle += \"`{0}` {1}\".format(\n",
    "                        name,\n",
    "                        parameters['schema']['format'][index])\n",
    "                else:\n",
    "                    middle += \"`{0}` {1},\".format(\n",
    "                    name,\n",
    "                    parameters['schema']['format'][index])\n",
    "            query = table_top + middle + table_bottom        \n",
    "            output = s3.run_query(query,\n",
    "                                  database = db,\n",
    "                                  s3_output = s3_output,\n",
    "                                  filename = None,\n",
    "                                  destination_key = None)\n",
    "            type_origin['output_id'].append(output['QueryID'])\n",
    "            print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme indiqué précédemment, il faut concatener les tables avant de faire le filtrage et enrichissement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for origin in parameters['Tables'].items():\n",
    "    for key0, type_origin in origin[1].items():\n",
    "        if type_origin['origin'] != 'INITIAL':\n",
    "            if type_origin['origin'] == 'PARTIEL':\n",
    "                table_name = parameters['steps']['step_1']['tables'][0]\n",
    "                query = \"DROP TABLE {}\".format(table_name)\n",
    "            else:\n",
    "                table_name = parameters['steps']['step_1']['tables'][1]\n",
    "                query = \"DROP TABLE {}\".format(table_name)\n",
    "        s3.run_query(query,\n",
    "                                  database = db,\n",
    "                                  s3_output = s3_output,\n",
    "                                  filename = None,\n",
    "                                  destination_key = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for origin in parameters['Tables'].items():\n",
    "    \n",
    "    table_middle = \"\"\n",
    "    if origin[0] == 'Stock':\n",
    "        \n",
    "        for key0, type_origin in origin[1].items():\n",
    "            for i, table in enumerate(type_origin['tables']):\n",
    "                table_name = parameters['steps']['step_1']['tables'][0]\n",
    "                if table != 'ets_initial':\n",
    "                    if i == 0:\n",
    "                        table_top = parameters['steps']['step_1']['query']['top'].format(db,table_name, table)\n",
    "                    else:\n",
    "                        table_middle = parameters['steps']['step_1']['query']['middle'].format(table)\n",
    "            table_bottom = parameters['steps']['step_1']['query']['bottom'].format(','.join([str(elem) for elem in parameters['steps']['step_1']['partionned']]))\n",
    "            query = table_top + table_middle + table_bottom\n",
    "    else:\n",
    "        for key0, type_origin in origin[1].items():\n",
    "            for i, table in enumerate(type_origin['tables']):\n",
    "                table_name = parameters['steps']['step_1']['tables'][1]\n",
    "                if key0 == 'NEW' and i == 0:\n",
    "                    table_top = parameters['steps']['step_1']['query']['top'].format(db,table_name, table)\n",
    "                else:\n",
    "                    table_middle += parameters['steps']['step_1']['query']['middle'].format(table)\n",
    "        table_bottom = parameters['steps']['step_1']['query']['bottom'].format(','.join([str(elem) for elem in parameters['steps']['step_1']['partionned']]))\n",
    "        query = table_top + table_middle + table_bottom \n",
    "    output = s3.run_query(query,\n",
    "                                  database = db,\n",
    "                                  s3_output = s3_output,\n",
    "                                  filename = None,\n",
    "                                  destination_key = None)\n",
    "    parameters['steps']['step_1']['output_id'].append(output['QueryID'])\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: filtrage intra day et intra date de greffe\n",
    "\n",
    "Dans cette étape, nous devons enrichir les lignes selon la partition suivante:\n",
    "\n",
    "- siren, \n",
    "- code_greffe,\n",
    "- nom_greffe,\n",
    "- numero_gestion, \n",
    "- id_etablissement, \n",
    "    - file_timestamp\n",
    "    - date_greffe\n",
    "\n",
    "puis il faut récupérer la dernière ligne du `file_timestamp` pour une date de greffe (`date_greffe`) donnée.\n",
    "\n",
    "Nous allons procéder en deux étapes totalement identiques. La première consiste a filtrer et enrichir la data en utilisant le time_stamp (date de transmission) et dans un second temps en filtrant et enrichissant via la date de greffe (événement). Au final, nous devons avoir qu'une seule ligne enrichie pour une entreprise et un événement donnée.\n",
    "\n",
    "Etant donnée la taille de la data, nous allons préparer les flux selon les greffes. Les fichiers sont stockés dans le S3, [calfdata/INPI/TC_1/02_preparation_donnee/TEMP_ETS_FLUX](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/TC_1/02_preparation_donnee/TEMP_ETS_FLUX/?region=eu-west-3&tab=overview) pour le timestamp et vont etre récupéré dans la query suivante pour créer une table reconstruite. Chacun des csv portera le nom du greffe.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for greffe in parameters['steps']['step_2']['code_greffe']:\n",
    "        filtre_top = parameters['steps']['step_2']['query']['top']['first'].format(\n",
    "        ','.join([str(elem) for elem in parameters['steps']['step_2']['partionned']['time_stamp']]),\n",
    "            db,\n",
    "            'ets_flux',\n",
    "            greffe,\n",
    "            'code_greffe'\n",
    "        )\n",
    "        filtre_bottom =parameters['steps']['step_2']['query']['bottom'].format(\n",
    "            ','.join([str(elem).lower() for elem in parameters['steps']['step_2']['to_fill']['time_stamp']]),\n",
    "            ','.join([str(elem) for elem in parameters['steps']['step_2']['partionned']['time_stamp']])\n",
    "        )\n",
    "        query_fillin = filtre_top\n",
    "        for x, val in enumerate( parameters['steps']['step_2']['to_fill']['time_stamp']):\n",
    "\n",
    "            if x != len( parameters['steps']['step_2']['to_fill']['time_stamp']) -1:\n",
    "                query_fillin+=parameters['steps']['step_2']['query']['top']['second'].format(val.lower() )+ \",\"\n",
    "            else:\n",
    "                query_fillin+=parameters['steps']['step_2']['query']['top']['second'].format(val.lower())\n",
    "                query_fillin+= parameters['steps']['step_2']['query']['middle']['first']\n",
    "\n",
    "        for x, val in enumerate(parameters['steps']['step_2']['to_fill']['time_stamp']):\n",
    "            if x != len( parameters['steps']['step_2']['to_fill']['time_stamp']) -1:\n",
    "                query_fillin+=parameters['steps']['step_2']['query']['middle']['second'].format(val.lower())+ \",\"\n",
    "            else:\n",
    "                query_fillin+=parameters['steps']['step_2']['query']['middle']['second'].format(val.lower())\n",
    "                query_fillin+=filtre_bottom\n",
    "        \n",
    "        output = s3.run_query(query_fillin,\n",
    "                      database = db,\n",
    "                      s3_output = s3_output,\n",
    "                      filename = None,\n",
    "                      destination_key = None)\n",
    "        parameters['steps']['step_2']['output_id'].append(output['QueryID'])\n",
    "        source_key = '{}/{}.csv'.format(s3_output, output['QueryID'])\n",
    "        destination_key = '{0}/{1}.csv'.format(parameters['steps']['step_2']['path'][0][14:],greffe)\n",
    "        print(output)\n",
    "        s3.move_object_s3(source_key, destination_key, remove = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table filtree et enrichie intermediaire timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous venons de filtrer les transmissions intra day, mais pas par date de greffe. L'ensemble des CSV sont dans le S3. Il nous suffit de créer une table intermédiaire, puis de réitéter l'opération non pas sur le timestamp, mais sur la date de greffe. Il est possible qu'une transmission possède plusieurs lignes pour la même transmission. C'est une erreur de notre part lors de la création de la table initiale, nous aurions du créer un numéro de ligne au sein du groupe et ne récupérer que la ligne maximum. Temporairement, nous filtrons la dernière ligne, même si elle n'est indiquée comme la dernière dans les CSV (entre date de greffe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"DROP TABLE {}\".format(parameters['steps']['step_2']['tables'][0])\n",
    "s3.run_query(query,\n",
    "                                  database = db,\n",
    "                                  s3_output = s3_output,\n",
    "                                  filename = None,\n",
    "                                  destination_key = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_top = \"\"\n",
    "table_bottom = \"\"\n",
    "middle = \"\"\n",
    "schema_filtre = list(chain.from_iterable([parameters['steps']['step_2']['partionned']['time_stamp'],\n",
    "                          parameters['steps']['step_2']['to_fill']['time_stamp']])\n",
    "    )\n",
    "table_top += parameters['steps']['step_0']['query']['top'].format(db, parameters['steps']['step_2']['tables'][0])#top.format(db, type_origin['tables'][i])\n",
    "table_bottom += parameters['steps']['step_0']['query']['bottom'].format(parameters['steps']['step_2']['separator'],\n",
    "                                                                        parameters['steps']['step_2']['path'][0])\n",
    "for index, name in enumerate(schema_filtre):\n",
    "    if index == len(schema_filtre)-1:\n",
    "        middle += \"`{0}` {1}\".format(\n",
    "                        name,\n",
    "                        'string')\n",
    "    else:\n",
    "        middle += \"`{0}` {1},\".format(\n",
    "                    name,\n",
    "                    'string')\n",
    "query = table_top + middle + table_bottom \n",
    "output = s3.run_query(query,\n",
    "                                  database = db,\n",
    "                                  s3_output = s3_output,\n",
    "                                  filename = None,\n",
    "                                  destination_key = None)\n",
    "parameters['steps']['step_2']['output_id'].append(output['QueryID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table filtree et enrichie intermediaire date greffe\n",
    "\n",
    "Etant donnée la taille de la data, nous allons préparer les flux selon les greffes. Les fichiers sont stockés dans le S3, [calfdata/INPI/TC_1/02_preparation_donnee/TEMP_ETS_FLUX_FILTRE](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/TC_1/02_preparation_donnee/TEMP_ETS_FLUX_FILTRE/?region=eu-west-3&tab=overview) et vont etre récupéré dans la query suivante pour créer une table reconstruite. Chacun des csv portera le nom du greffe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for greffe in parameters['steps']['step_2']['code_greffe']:\n",
    "        filtre_top = parameters['steps']['step_2']['query']['top']['first'].format(\n",
    "        ','.join([str(elem) for elem in parameters['steps']['step_2']['partionned']['date_greffe']]),\n",
    "            db,\n",
    "            parameters['steps']['step_2']['tables'][0],\n",
    "            greffe,\n",
    "            'code_greffe'\n",
    "            #'{}_{}_{}'.format(parameters['steps']['step_2']['tables'][0], \n",
    "            #origin, \n",
    "            #year\n",
    "        )\n",
    "        filtre_bottom =parameters['steps']['step_2']['query']['bottom'].format(\n",
    "            ','.join([str(elem).lower() for elem in parameters['steps']['step_2']['to_fill']['date_greffe']]),\n",
    "            ','.join([str(elem) for elem in parameters['steps']['step_2']['partionned']['date_greffe']])\n",
    "        )\n",
    "        query_fillin = filtre_top\n",
    "        for x, val in enumerate( parameters['steps']['step_2']['to_fill']['date_greffe']):\n",
    "\n",
    "            if x != len( parameters['steps']['step_2']['to_fill']['date_greffe']) -1:\n",
    "                query_fillin+=parameters['steps']['step_2']['query']['top']['second'].format(val.lower() )+ \",\"\n",
    "            else:\n",
    "                query_fillin+=parameters['steps']['step_2']['query']['top']['second'].format(val.lower())\n",
    "                query_fillin+= parameters['steps']['step_2']['query']['middle']['first']\n",
    "\n",
    "        for x, val in enumerate(parameters['steps']['step_2']['to_fill']['date_greffe']):\n",
    "            if x != len( parameters['steps']['step_2']['to_fill']['date_greffe']) -1:\n",
    "                query_fillin+=parameters['steps']['step_2']['query']['middle']['second'].format(val.lower())+ \",\"\n",
    "            else:\n",
    "                query_fillin+=parameters['steps']['step_2']['query']['middle']['second'].format(val.lower())\n",
    "                query_fillin+=filtre_bottom\n",
    "        \n",
    "        output = s3.run_query(query_fillin,\n",
    "                      database = db,\n",
    "                      s3_output = s3_output,\n",
    "                      filename = None,\n",
    "                      destination_key = None)\n",
    "        source_key = '{}/{}.csv'.format(s3_output, output['QueryID'])\n",
    "        destination_key = '{0}/{1}.csv'.format(parameters['steps']['step_2']['path'][1][14:],greffe)\n",
    "        parameters['steps']['step_2']['output_id'].append(output['QueryID'])\n",
    "        print(output)\n",
    "        s3.move_object_s3(source_key, destination_key, remove = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"DROP TABLE {}\".format(parameters['steps']['step_2']['tables'][1])\n",
    "s3.run_query(query,\n",
    "                                  database = db,\n",
    "                                  s3_output = s3_output,\n",
    "                                  filename = None,\n",
    "                                  destination_key = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut créer la table filtrée et enrichie avec une seule ligne par date de greffe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_top = \"\"\n",
    "table_bottom = \"\"\n",
    "middle = \"\"\n",
    "schema_filtre = list(chain.from_iterable([parameters['steps']['step_2']['partionned']['date_greffe'],\n",
    "                          parameters['steps']['step_2']['to_fill']['date_greffe']])\n",
    "    )\n",
    "table_top += parameters['steps']['step_0']['query']['top'].format(db, parameters['steps']['step_2']['tables'][1])#top.format(db, type_origin['tables'][i])\n",
    "table_bottom += parameters['steps']['step_0']['query']['bottom'].format(parameters['steps']['step_2']['separator'],\n",
    "                                                                        parameters['steps']['step_2']['path'][1])\n",
    "for index, name in enumerate(schema_filtre):\n",
    "    if index == len(schema_filtre)-1:\n",
    "        middle += \"`{0}` {1}\".format(\n",
    "                        name,\n",
    "                        'string')\n",
    "    else:\n",
    "        middle += \"`{0}` {1},\".format(\n",
    "                    name,\n",
    "                    'string')\n",
    "query = table_top + middle + table_bottom \n",
    "output = s3.run_query(query,\n",
    "                                  database = db,\n",
    "                                  s3_output = s3_output,\n",
    "                                  filename = None,\n",
    "                                  destination_key = None)\n",
    "parameters['steps']['step_2']['output_id'].append(output['QueryID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Enrichissements des lignes d'un événement a l'autre et filtrage des événements partiels\n",
    "\n",
    "Nous avons dès à présent 3 tables contenant l'ensemble des événements d'un établissement. La table initial, la table des flux filtrés et enrichis et la table des partiels. Il faut reconstituer la table finale en concatenant ses trois tables puis en enrichissant les lignes selon l'événement précédents et en indiquant les lignes a ignorer en cas de partiel. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.run_query(\n",
    "    \"drop table {}\".format(parameters['steps']['step_3']['tables'][0]),\n",
    "    database = db,\n",
    "    s3_output = s3_output,\n",
    "    filename = None,\n",
    "    destination_key = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout d'abord, nous allons créer une table intermédiaire dans lequel la concaténation et la création du status 'IGNORE' va ête réalisé. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.run_query(\n",
    "   parameters['steps']['step_3']['query']['preparation'],\n",
    "    database = db,\n",
    "    s3_output = s3_output,\n",
    "    filename = None,\n",
    "    destination_key = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La seconde partie de l'étape va procéder a l'enrichissement des valeurs sur les flux à partir du moment ou la ligne n'est pas à ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = \"\"\"\n",
    "SELECT siren,\n",
    "                 code_greffe,\n",
    "                 nom_greffe,\n",
    "                 numero_gestion,\n",
    "                 id_etablissement,\n",
    "                 origin, \n",
    "                 status,\n",
    "                 date_greffe,\n",
    "                 libelle_evt,\n",
    "\n",
    "\"\"\"\n",
    "middle = \"\"\"\n",
    "CASE WHEN origin = 'FLUX' AND status != 'IGNORE' AND \"{0}\" = '' THEN \n",
    "LAG (\"{0}\", 1) OVER (  PARTITION BY siren,\"code_greffe\", numero_gestion, id_etablissement \n",
    " ORDER BY siren,'code_greffe', numero_gestion, id_etablissement,date_greffe ) ELSE \"{0}\" END AS \"{0}\" \n",
    "\n",
    "\"\"\"\n",
    "bottom = \"\"\"\n",
    "FROM {}\n",
    "ORDER BY siren,code_greffe, numero_gestion, id_etablissement,date_greffe\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters['steps']['step_3']['tables'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_middle = \"\"\n",
    "table_bottom = bottom.format(parameters['steps']['step_3']['tables'][0])\n",
    "for x, value in enumerate(parameters['steps']['step_3']['to_fill']):\n",
    "    if  x != len(parameters['steps']['step_3']['to_fill'])-1:\n",
    "        table_middle +=middle.format(value) +\",\"\n",
    "    else:\n",
    "        table_middle +=middle.format(value)\n",
    "query = top + table_middle + table_bottom\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, shutil, urllib, ipykernel, json\n",
    "from pathlib import Path\n",
    "from notebook import notebookapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_report(extension = \"html\", keep_code = False):\n",
    "    \"\"\"\n",
    "    Create a report from the current notebook and save it in the \n",
    "    Report folder (Parent-> child directory)\n",
    "    \n",
    "    1. Exctract the current notbook name\n",
    "    2. Convert the Notebook \n",
    "    3. Move the newly created report\n",
    "    \n",
    "    Args:\n",
    "    extension: string. Can be \"html\", \"pdf\", \"md\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ### Get notebook name\n",
    "    connection_file = os.path.basename(ipykernel.get_connection_file())\n",
    "    kernel_id = connection_file.split('-', 1)[0].split('.')[0]\n",
    "\n",
    "    for srv in notebookapp.list_running_servers():\n",
    "        try:\n",
    "            if srv['token']=='' and not srv['password']:  \n",
    "                req = urllib.request.urlopen(srv['url']+'api/sessions')\n",
    "            else:\n",
    "                req = urllib.request.urlopen(srv['url']+ \\\n",
    "                                             'api/sessions?token=' + \\\n",
    "                                             srv['token'])\n",
    "            sessions = json.load(req)\n",
    "            notebookname = sessions[0]['name']\n",
    "        except:\n",
    "            pass  \n",
    "    \n",
    "    sep = '.'\n",
    "    path = os.getcwd()\n",
    "    #parent_path = str(Path(path).parent)\n",
    "    \n",
    "    ### Path report\n",
    "    #path_report = \"{}/Reports\".format(parent_path)\n",
    "    #path_report = \"{}/Reports\".format(path)\n",
    "    \n",
    "    ### Path destination\n",
    "    name_no_extension = notebookname.split(sep, 1)[0]\n",
    "    source_to_move = name_no_extension +'.{}'.format(extension)\n",
    "    dest = os.path.join(path,'Reports', source_to_move)\n",
    "    \n",
    "    ### Generate notebook\n",
    "    if keep_code:\n",
    "        os.system('jupyter nbconvert --to {} {}'.format(\n",
    "    extension,notebookname))\n",
    "    else:\n",
    "        os.system('jupyter nbconvert --no-input --to {} {}'.format(\n",
    "    extension,notebookname))\n",
    "    \n",
    "    ### Move notebook to report folder\n",
    "    #time.sleep(5)\n",
    "    shutil.move(source_to_move, dest)\n",
    "    print(\"Report Available at this adress:\\n {}\".format(dest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_report(extension = \"html\", keep_code = True)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nteract": {
   "version": "0.24.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
