{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modification de la donnée brute\n",
    "\n",
    "L\\'objectif est de garder trace des informations d\\'origine du fichier si les données sont manipulées par la suite (agrégées, modifiées, déplacées ...).\n",
    "\n",
    "Ce notebook utilise les fichiers présents dans le s3 dans [00_RawData](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/TC_1/00_RawData/?region=eu-west-3).\n",
    "\n",
    "Pour chaque fichier présent dans le dossier source:\n",
    "\n",
    "- Il déduit à partir du nom du fichier les différentes variables de classification grâce à la fonction `̀ get_file_infos`̀ .\n",
    "        - ``origin_type`` : Flux ou Stock.\n",
    "        - ``origin`` : Initial ou Partiel pour Stock, NEW ou EVT pour Flux.\n",
    "        - ``nature`` : ACTES, ETS, OBS, COMPTES_ANNUELS, PM, PP, REP.\n",
    "\n",
    "- Il calcule le dossier destination du fichier csv et crée une liste de fichiers à envoyer.\n",
    "\n",
    "Grâce à la fonction `̀ prepare_object`̀ :\n",
    "- Il lit un fichier les colonnes suivantes aux csv:\n",
    "- Il ajoute les colonnes suivantes au csv:\n",
    "\n",
    "    - ``csv_source`` : nom du fichier source. Exemple format: 3801_189_20180130_065752_9_ets_nouveau_modifie_EVT.csv\n",
    "    - ``file_timestamp`` = la date du fichier ou une date du fichier source (par ex date_greffe) pour les stock initial qui ont tous par défaut une date de fichier à 2017-05-24.\n",
    "    - ``origin_type`` : Flux ou Stock.\n",
    "    - ``origin`` : Initial ou Partiel pour Stock, NEW ou EVT pour Flux.\n",
    "    - ``nature`` : ACTES, ETS, OBS, COMPTES_ANNUELS, PM, PP, REP.\n",
    "        \n",
    "- il écrit directement dans S3 le fichier préparé.\n",
    "\n",
    "La donnée est stockée dans le dossier [01_donnee_source](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/TC_1/01_donnee_source/?region=eu-west-3) et l'arborescence simplifiée.\n",
    "\n",
    "La fonction `̀ log_init`̀  s'occupe de gérer les logs pour faciliter la reprise sur erreur.\n",
    "\n",
    "Options:\n",
    "- Il est possible de définir des filtres pour ne prendre qu'une certaine nature de fichiers à l'intérieur d'un dossier.\n",
    "- Il est possible de faire retourner le script sur la liste des erreurs ou sur une liste libre de clés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_infos(filename,dl_path):\n",
    "    \n",
    "    \"\"\"\n",
    "    Computes information that are in the filename to classify its content.\n",
    "    cf INPI files documentation here : https://coda.io/d/CreditAgricole_dCtnoqIftTn/INSEE-INPI_suTDp\n",
    "\n",
    "    args:\n",
    "         - filename: Any filename\n",
    "         ex : '0101_S1_20170504_11_obs.csv'\n",
    "         - dl_path : local path of file\n",
    "\n",
    "    returns:\n",
    "        A list of informations : (nature_,origin_,suborigin_,year_,timestamp_).\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    f_s=dl_path.split('/')\n",
    "    suborigin_=f_s[6]\n",
    "    origin_=f_s[2]\n",
    "    nature_=f_s[5]\n",
    "    year_=f_s[3]\n",
    "    \n",
    "    import datetime\n",
    "    from datetime import datetime\n",
    "\n",
    "    f_s = filename.split('_')\n",
    "    date_=f_s[2]\n",
    "    year_=date_[0:4]\n",
    "    month_=date_[4:6]\n",
    "    day_=date_[6:9]\n",
    "    time_= f_s[3]\n",
    "\n",
    "    datetime_str=date_ + '_' + time_\n",
    "    timestamp_=datetime.strptime(datetime_str,\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    return (nature_,origin_,suborigin_,year_,timestamp_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import S3 connectors librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+git://github.com/thomaspernet/aws-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade git+git://github.com/thomaspernet/aws-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsPy.aws_authorization import aws_connector\n",
    "from awsPy.aws_s3 import service_s3\n",
    "from awsPy.aws_athena import service_athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Connect to S3\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "bucket = 'calfdata'\n",
    "path = os.getcwd()\n",
    "parent_path = str(Path(path).parent)\n",
    "path_cred = \"{}/programme_matching/credential_AWS.json\".format(parent_path)\n",
    "\n",
    "con = aws_connector.aws_instantiate(credential = path_cred, region = 'eu-west-3')\n",
    "client= con.client_boto()\n",
    "s3 = service_s3.connect_S3(client = client, bucket = 'calfdata')\n",
    "\n",
    "bucket_name='calfdata'\n",
    "bucket = client['resource'].Bucket(bucket_name)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Raw Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "from io import StringIO\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate log files\n",
    "def log_init():\n",
    "    \"\"\"\n",
    "    Initiate log files for viewed, treated and files in error.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    logv_name=\"{}/{}_{}-{}\".format('data',timestr,folder_str,'viewed.csv')\n",
    "    logv = open(logv_name, 'w')\n",
    "    print('key', file=logv)\n",
    "    logt_name=\"{}/{}_{}-{}\".format('data',timestr,folder_str,'treated.csv')\n",
    "    logt = open(logt_name, 'w')\n",
    "    print('key', file=logt)\n",
    "    loge_name=\"{}/{}_{}-{}\".format('data',timestr,folder_str,'errors.csv')\n",
    "    loge = open(loge_name, 'w')\n",
    "    print('key;error', file=loge)    \n",
    "    \n",
    "    return (logtd,logv,logt,loge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_object(obj):\n",
    "    \"\"\"\n",
    "    Prepares a Raw csv file from INPI :\n",
    "        - Read file\n",
    "        - Add information in columns\n",
    "        - Write result directly in S3\n",
    "\n",
    "    Arg:\n",
    "        - obj : an S3 object\n",
    "        \n",
    "    Return:\n",
    "        - the destination path in S3 of the prepared file generated\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    key = obj.key\n",
    "    body = obj.get()['Body'].read()\n",
    "    s=key.split('/')\n",
    "    filename=s[len(s)-1]\n",
    "\n",
    "    (nature_,origin_,suborigin_,year_,timestamp_) = u.get_file_infos(filename)\n",
    "    dest_path=\"{}/{}/{}/{}/{}\".format('INPI/TC_1/01_donnee_source',origin_,year_,nature_,suborigin_)\n",
    "    dest_full_path=\"{}/{}\".format(dest_path,filename)\n",
    "            \n",
    "    df = pd.read_csv(io.BytesIO(body), header=0, dtype=str, sep = ';',error_bad_lines=False)\n",
    "    if nature_ == 'REP':\n",
    "        # Rename col 'date_greffe' to 'Date_Greffe'\n",
    "        df=df.rename(columns={\"date_greffe\": \"Date_Greffe\"})\n",
    "\n",
    "    df['csv_source']=filename\n",
    "    df['nature']=nature_\n",
    "    df['type']=origin_\n",
    "    df['origin']=suborigin_\n",
    "\n",
    "    if origin_=='Stock' and year_ == '2017' and nature_ == 'ACTES': \n",
    "        # Tout le stock étant initialisé à la même date en 2017, on utilise plutôt la Date_Dépôt\n",
    "        df['file_timestamp']= df['Date_Dépôt'].apply(lambda x : datetime.strptime(x,\"%Y-%m-%d\"))\n",
    "    elif origin_=='Stock' and year_ == '2017' and nature_ == 'COMPTES_ANNUELS': \n",
    "        # Tout le stock étant initialisé à la même date en 2017, on utilise plutôt la Date_Dépôt\n",
    "        df['file_timestamp']= df['Date_Dépôt'].apply(lambda x : datetime.strptime(x,\"%Y-%m-%d\"))\n",
    "    elif origin_=='Stock' and year_ == '2017': \n",
    "        # Tout le stock étant initialisé à la même date en 2017, on utilise plutôt la Date_Greffe\n",
    "        df['file_timestamp']= df['Date_Greffe'].apply(lambda x : datetime.strptime(x,\"%Y-%m-%d\"))\n",
    "    else:\n",
    "        df['file_timestamp']=timestamp_    \n",
    "\n",
    "    # Save file to destination in S3\n",
    "    # Create buffer\n",
    "    csv_buffer = StringIO()\n",
    "    # Write dataframe to buffer\n",
    "    df.to_csv(csv_buffer, sep=\";\", index=False)\n",
    "    # Create S3 object\n",
    "    s3_resource = client['resource']\n",
    "    # Write buffer to S3 object\n",
    "    s3_resource.Object(bucket_name, dest_full_path).put(Body=csv_buffer.getvalue())\n",
    "\n",
    "    \n",
    "    return dest_full_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare all items in a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select folder\n",
    "folder='INPI/TC_1/00_RawData/public/IMR_Donnees_Saisies/tc/flux/2018/01/01'\n",
    "folder_str='rawFlux20180101'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all objects in S3 folder\n",
    "list_bucket = bucket.objects.filter(Prefix=folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional filter on certain filenames\n",
    "\n",
    "filter_ = (\n",
    "#'1_PM.csv',\n",
    "#'2_PM_EVT.csv',\n",
    "#'3_PP.csv',\n",
    "#'4_PP_EVT.csv',\n",
    "#'5_rep.csv',\n",
    "#'6_rep_nouveau_modifie_EVT.csv',\n",
    "#'7_rep_partant_EVT.csv',\n",
    "#'8_ets.csv',\n",
    "#'9_ets_nouveau_modifie_EVT.csv',\n",
    "'10_ets_supprime_EVT.csv',\n",
    "#'11_obs.csv',\n",
    "'12_actes.csv',\n",
    "'13_comptes_annuels.csv'\n",
    "    )\n",
    "\n",
    "list_bucket_filter=[]\n",
    "for obj in tqdm(list_bucket):\n",
    "    if obj.key.endswith(filter_):\n",
    "        list_bucket_filter.append(obj)\n",
    "        \n",
    "list_bucket=list_bucket_filter if filter_ else list_bucket_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save todo list to a file\n",
    "import csv\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "logtd_name=\"{}/{}_{}-{}\".format('data',timestr,folder_str,'todo.csv')\n",
    "logtd = open(logtd_name, 'w')\n",
    "with logtd as myfile:\n",
    "    wr = csv.writer(myfile,delimiter=';',quoting=csv.QUOTE_ALL)\n",
    "    wr.writerow(['key'])\n",
    "    for obj in tqdm(list_bucket):\n",
    "        wr.writerow([obj.key])\n",
    "logtd.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(logtd,logv,logt,loge)=log_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in tqdm(list_bucket):\n",
    "\n",
    "    try:\n",
    "        print(obj.key, file=logv)\n",
    "        prepare_object(obj)\n",
    "        print(obj.key, file=logt)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print((obj.key,e), file=loge)\n",
    "\n",
    "logv.close()\n",
    "logt.close()\n",
    "loge.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats and error recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats \n",
    "dftd=pd.read_csv(logtd.name,header=0, sep=';')# Todo\n",
    "td=len(dftd)\n",
    "dft=pd.read_csv(logt.name,header=0, sep=';')\n",
    "t=len(dft)\n",
    "dfe=pd.read_csv(loge.name,header=0, sep=';',usecols = ['key'])\n",
    "e=len(dfe)\n",
    "# Find untreated objects\n",
    "dfnt=pd.DataFrame()\n",
    "dfnt=dftd.merge(dft,indicator = True, how='left').loc[lambda x : x['_merge']!='both']\n",
    "dfnt=dfnt.drop(columns=['_merge'])\n",
    "nt=len(dfnt)\n",
    "# Add errors\n",
    "dfnt=dfnt.append(dfe,sort=True)\n",
    "nt=len(dfnt)\n",
    "\n",
    "\n",
    "\n",
    "# Print stats\n",
    "print('Folder :' + folder)\n",
    "if filter:\n",
    "    print(\"Filter : %s\" % (filter_,))\n",
    "print('To do : ' + str(td))\n",
    "print('Treated : ' + str(t))\n",
    "print('Errors : ' + str(e))\n",
    "print('Retake : ' + str(nt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare from a file list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save errors to a new todo\n",
    "list_bucket_keys = dfnt['key'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or Create the new todo from a list of Missing files\n",
    "folder_str='prepFluxMissing2018'\n",
    "df=pd.read_csv('data/count/missingPrepFlux2018.csv',header=None, sep=';')# Todo\n",
    "df.columns = ['key']\n",
    "list_bucket_keys = df['key'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save list to a new todo\n",
    "import csv\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "logtd_name=\"{}/{}_{}-{}\".format('data',timestr,folder_str,'todo.csv')\n",
    "logtd = open(logtd_name, 'w')\n",
    "with logtd as myfile:\n",
    "    wr = csv.writer(myfile,delimiter=';',quoting=csv.QUOTE_ALL)\n",
    "    wr.writerow(['key'])\n",
    "    for key in tqdm(list_bucket_keys):\n",
    "        wr.writerow([key])\n",
    "logtd.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(logtd,logv,logt,loge)=log_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart on errors\n",
    "for n,key in tqdm(enumerate(list_bucket_keys)):\n",
    "    try:\n",
    "        print(key, file=logv)\n",
    "        \n",
    "        # get object in S3 by its key\n",
    "        obj=client['resource'].Object(bucket_name=bucket_name, key=key)\n",
    "        f=prepare_object(obj)\n",
    "        print(key, file=logt)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print((key,e), file=loge)\n",
    "\n",
    "logv.close()\n",
    "logt.close()\n",
    "loge.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
