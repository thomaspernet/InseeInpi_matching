{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sauvegarde des fichiers sur le S3\n",
    "\n",
    "Le notebook stocke les données du FTP brutes dans le dossier [00_RawData](https://s3.console.aws.amazon.com/s3/buckets/calfdata/INPI/TC_1/00_RawData/?region=eu-west-3).\n",
    "\n",
    "Ni les fichiers ni l\\'arborescence de stockage ne sont modifiés.\n",
    "\n",
    "- Connexion au S3\n",
    "- Liste les fichiers à envoyer et calcule leur destination\n",
    "- Uploade le fichier dans S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as u\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not 'workbookDir' in globals():\n",
    "    workbookDir = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import S3 connectors librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_separator = \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+git://github.com/thomaspernet/aws-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade git+git://github.com/thomaspernet/aws-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsPy.aws_authorization import aws_connector\n",
    "from awsPy.aws_s3 import service_s3\n",
    "from awsPy.aws_athena import service_athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to S3\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "bucket = 'calfdata'\n",
    "path = os.getcwd()\n",
    "parent_path = str(Path(path).parent)\n",
    "path_cred = \"{}/programme_matching/credential_AWS.json\".format(parent_path)\n",
    "\n",
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                        region = 'eu-west-3')\n",
    "client= con.client_boto()\n",
    "s3 = service_s3.connect_S3(client = client,\n",
    "                      bucket = 'calfdata')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List files to send and compute each destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('flux-2018-to-send\\\\2018\\\\12\\\\26\\\\0101\\\\482\\\\0101_482_20181226_085507_10_ets_supprime_EVT.csv',\n",
       "  'INPI/TC_1/00_RawData/public/IMR_Donnees_Saisies/tc/flux-2018-to-send/2018/12/26/0101/482'),\n",
       " ('flux-2018-to-send\\\\2018\\\\12\\\\26\\\\0101\\\\482\\\\0101_482_20181226_085507_11_obs.csv',\n",
       "  'INPI/TC_1/00_RawData/public/IMR_Donnees_Saisies/tc/flux-2018-to-send/2018/12/26/0101/482'),\n",
       " ('flux-2018-to-send\\\\2018\\\\12\\\\26\\\\0101\\\\482\\\\0101_482_20181226_085507_12_actes.csv',\n",
       "  'INPI/TC_1/00_RawData/public/IMR_Donnees_Saisies/tc/flux-2018-to-send/2018/12/26/0101/482'),\n",
       " ('flux-2018-to-send\\\\2018\\\\12\\\\26\\\\0101\\\\482\\\\0101_482_20181226_085507_13_comptes_annuels.csv',\n",
       "  'INPI/TC_1/00_RawData/public/IMR_Donnees_Saisies/tc/flux-2018-to-send/2018/12/26/0101/482'),\n",
       " ('flux-2018-to-send\\\\2018\\\\12\\\\26\\\\0101\\\\482\\\\0101_482_20181226_085507_1_PM.csv',\n",
       "  'INPI/TC_1/00_RawData/public/IMR_Donnees_Saisies/tc/flux-2018-to-send/2018/12/26/0101/482')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List files to send and compute each destination\n",
    "import os\n",
    "source_path= 'flux-2018-to-send'\n",
    "root_dest_path=aws_separator.join(['INPI', 'TC_1' , '00_RawData', 'public' ,'IMR_Donnees_Saisies' ,'tc'])\n",
    "\n",
    "list_files=[]\n",
    "for r,d,files in os.walk(source_path):\n",
    "    for i, file in enumerate(files):\n",
    "          if file.endswith(('.csv')):\n",
    "                \n",
    "                source_full_path=os.path.join(r,file)\n",
    "                \n",
    "                dest_path = aws_separator.join([root_dest_path,r.replace(\"\\\\\",\"/\")])\n",
    "\n",
    "                list_files.append((source_full_path,dest_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Send files to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send files to s3\n",
    "\n",
    "for (full_path,destination_in_s3) in list_files:\n",
    "    filename=full_path.split('\\\\')[-1]\n",
    "    key = aws_separator.join([destination_in_s3,filename])\n",
    "    if not s3.key_exist(key):\n",
    "        s3.upload_file(full_path, destination_in_s3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
