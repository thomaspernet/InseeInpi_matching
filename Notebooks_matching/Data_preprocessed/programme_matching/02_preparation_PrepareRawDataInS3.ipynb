{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_infos(filename,dl_path):\n",
    "    f_s=dl_path.split('/')\n",
    "    suborigin_=f_s[6]\n",
    "    origin_=f_s[2]\n",
    "    nature_=f_s[5]\n",
    "    year_=f_s[3]\n",
    "    \n",
    "    import datetime\n",
    "    from datetime import datetime\n",
    "\n",
    "    f_s = filename.split('_')\n",
    "    date_=f_s[2]\n",
    "    year_=date_[0:4]\n",
    "    month_=date_[4:6]\n",
    "    day_=date_[6:9]\n",
    "    time_= f_s[3]\n",
    "\n",
    "    datetime_str=date_ + '_' + time_\n",
    "    timestamp_=datetime.strptime(datetime_str,\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    return (nature_,origin_,suborigin_,year_,timestamp_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    # Import S3 connectors librairies\n",
    "    #from awsPy.aws_authorization import aws_connector\n",
    "    import importlib.util\n",
    "    spec = importlib.util.spec_from_file_location(\"aws_connector.py\", \"../../../../aws-python/awsPy/aws_authorization/aws_connector.py\")\n",
    "    aws_connector = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(aws_connector)\n",
    "\n",
    "    #from awsPy.aws_s3 import service_s3\n",
    "    import importlib.util\n",
    "    spec = importlib.util.spec_from_file_location(\"service_s3.py\", \"../../../../aws-python/awsPy/aws_s3/service_s3.py\")\n",
    "    service_s3 = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(service_s3)\n",
    "\n",
    "    # Connect to S3\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "\n",
    "    bucket = 'calfdata'\n",
    "    path = os.getcwd()\n",
    "    parent_path = str(Path(path).parent)\n",
    "    path_cred = \"{}/programme_matching/credential_AWS.json\".format(parent_path)\n",
    "\n",
    "    con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                            region = 'eu-west-3')\n",
    "    client= con.client_boto()\n",
    "    s3 = service_s3.connect_S3(client = client,\n",
    "                          bucket = 'calfdata')\n",
    "\n",
    "    bucket_name='calfdata'\n",
    "    bucket = client['resource'].Bucket(bucket_name)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From INPI/Flux folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dossier source à parcourir\n",
    "prefix_root='INPI/TC_1/Flux'# Attention pas plus loin dans l'arborescence que 'INPI/TC_1/Flux/'\n",
    "# Calcul de la liste des dossiers qui devront être traités\n",
    "\n",
    "list_folders=[]\n",
    "for y in range(2017, 2018):\n",
    "    for m in range(1, 13):\n",
    "        m_ = '0'+ str(m) if m<10 else str(m)\n",
    "        for n in ['ETS']:#['ACTES','ETS','OBS','PM','PP','REP','COMPTES_ANNUELS']\n",
    "            for s in ['EVT','NEW']:\n",
    "                prefix_=\"{}/{}/{}/{}/{}\".format(prefix_root,y,m_,n,s)\n",
    "                list_folders.append(prefix_)\n",
    "log = open('data/log-flux.txt', 'w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recréation pour correction du Flux/2018/Actes/New\n",
    "'''list_folders=['INPI/TC_1/Flux/2018/01/OBS/EVT',\n",
    "             'INPI/TC_1/Flux/2018/02/OBS/EVT',\n",
    "             'INPI/TC_1/Flux/2018/03/OBS/EVT',\n",
    "              'INPI/TC_1/Flux/2018/04/OBS/EVT',\n",
    "              'INPI/TC_1/Flux/2018/05/OBS/EVT',\n",
    "              'INPI/TC_1/Flux/2018/06/OBS/EVT',\n",
    "              'INPI/TC_1/Flux/2018/07/OBS/EVT',\n",
    "              'INPI/TC_1/Flux/2018/08/OBS/EVT',\n",
    "              'INPI/TC_1/Flux/2018/09/OBS/EVT',\n",
    "              'INPI/TC_1/Flux/2018/10/OBS/EVT',\n",
    "              'INPI/TC_1/Flux/2018/11/OBS/EVT',\n",
    "              'INPI/TC_1/Flux/2018/12/OBS/EVT'\n",
    "             ]\n",
    "             \n",
    "log = open('data/log-flux-2018-obsevt.txt', 'w+')             \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "from io import StringIO\n",
    "\n",
    "for prefix_ in list_folders:\n",
    "    print('treat ' + prefix_)\n",
    "    list_bucket = list(bucket.objects.filter(Prefix=prefix_))\n",
    "    \n",
    "    for obj in tqdm(list_bucket):\n",
    "        \n",
    "        try:\n",
    "            key = obj.key\n",
    "            body = obj.get()['Body'].read()\n",
    "            print(key, file=log)\n",
    "\n",
    "            s=key.split('/')\n",
    "            filename=s[len(s)-1]\n",
    "\n",
    "            (nature_,origin_,suborigin_,year_,timestamp_) = u.get_file_infos(filename)\n",
    "            dest_path=\"{}/{}/{}/{}/{}\".format('INPI/TC_1/01_donnee_source',origin_,year_,nature_,suborigin_)\n",
    "            dest_full_path=\"{}/{}\".format(dest_path,filename)\n",
    "            \n",
    "            \n",
    "            print(filename)\n",
    "            df = pd.read_csv(io.BytesIO(body), header=0, dtype=str, sep = ';',error_bad_lines=False)\n",
    "            if nature_ == 'REP':\n",
    "                # Rename col 'date_greffe' to 'Date_Greffe'\n",
    "                df=df.rename(columns={\"date_greffe\": \"Date_Greffe\"})\n",
    "\n",
    "            df['csv_source']=filename\n",
    "            df['nature']=nature_\n",
    "            df['type']=origin_\n",
    "            df['origin']=suborigin_\n",
    "\n",
    "            if origin_=='Stock' and year_ == '2017' and nature_ == 'ACTES': \n",
    "                # Tout le stock étant initialisé à la même date en 2017, on utilise plutôt la Date_Dépôt\n",
    "                df['file_timestamp']= df['Date_Dépôt'].apply(lambda x : datetime.strptime(x,\"%Y-%m-%d\"))\n",
    "            elif origin_=='Stock' and year_ == '2017' and nature_ == 'COMPTES_ANNUELS': \n",
    "                # Tout le stock étant initialisé à la même date en 2017, on utilise plutôt la Date_Dépôt\n",
    "                df['file_timestamp']= df['Date_Dépôt'].apply(lambda x : datetime.strptime(x,\"%Y-%m-%d\"))\n",
    "            elif origin_=='Stock' and year_ == '2017': \n",
    "                # Tout le stock étant initialisé à la même date en 2017, on utilise plutôt la Date_Greffe\n",
    "                df['file_timestamp']= df['Date_Greffe'].apply(lambda x : datetime.strptime(x,\"%Y-%m-%d\"))\n",
    "            else:\n",
    "                df['file_timestamp']=timestamp_    \n",
    "\n",
    "            # Save file to destination in S3\n",
    "            # Create buffer\n",
    "            csv_buffer = StringIO()\n",
    "            # Write dataframe to buffer\n",
    "            df.to_csv(csv_buffer, sep=\";\", index=False)\n",
    "            # Create S3 object\n",
    "            s3_resource = client['resource']\n",
    "            # Write buffer to S3 object\n",
    "            s3_resource.Object(bucket_name, dest_full_path).put(Body=csv_buffer.getvalue())\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "log.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Raw Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "from io import StringIO\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate log files\n",
    "def log_init():\n",
    "\n",
    "    logv_name=\"{}/{}_{}-{}\".format('data',timestr,folder_str,'viewed.csv')\n",
    "    logv = open(logv_name, 'w')\n",
    "    print('key', file=logv)\n",
    "    logt_name=\"{}/{}_{}-{}\".format('data',timestr,folder_str,'treated.csv')\n",
    "    logt = open(logt_name, 'w')\n",
    "    print('key', file=logt)\n",
    "    loge_name=\"{}/{}_{}-{}\".format('data',timestr,folder_str,'errors.csv')\n",
    "    loge = open(loge_name, 'w')\n",
    "    print('key;error', file=loge)    \n",
    "    \n",
    "    return (logtd,logv,logt,loge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_object(obj):\n",
    "    key = obj.key\n",
    "    body = obj.get()['Body'].read()\n",
    "    s=key.split('/')\n",
    "    filename=s[len(s)-1]\n",
    "\n",
    "    (nature_,origin_,suborigin_,year_,timestamp_) = u.get_file_infos(filename)\n",
    "    dest_path=\"{}/{}/{}/{}/{}\".format('INPI/TC_1/01_donnee_source',origin_,year_,nature_,suborigin_)\n",
    "    dest_full_path=\"{}/{}\".format(dest_path,filename)\n",
    "            \n",
    "    df = pd.read_csv(io.BytesIO(body), header=0, dtype=str, sep = ';',error_bad_lines=False)\n",
    "    if nature_ == 'REP':\n",
    "        # Rename col 'date_greffe' to 'Date_Greffe'\n",
    "        df=df.rename(columns={\"date_greffe\": \"Date_Greffe\"})\n",
    "\n",
    "    df['csv_source']=filename\n",
    "    df['nature']=nature_\n",
    "    df['type']=origin_\n",
    "    df['origin']=suborigin_\n",
    "\n",
    "    if origin_=='Stock' and year_ == '2017' and nature_ == 'ACTES': \n",
    "        # Tout le stock étant initialisé à la même date en 2017, on utilise plutôt la Date_Dépôt\n",
    "        df['file_timestamp']= df['Date_Dépôt'].apply(lambda x : datetime.strptime(x,\"%Y-%m-%d\"))\n",
    "    elif origin_=='Stock' and year_ == '2017' and nature_ == 'COMPTES_ANNUELS': \n",
    "        # Tout le stock étant initialisé à la même date en 2017, on utilise plutôt la Date_Dépôt\n",
    "        df['file_timestamp']= df['Date_Dépôt'].apply(lambda x : datetime.strptime(x,\"%Y-%m-%d\"))\n",
    "    elif origin_=='Stock' and year_ == '2017': \n",
    "        # Tout le stock étant initialisé à la même date en 2017, on utilise plutôt la Date_Greffe\n",
    "        df['file_timestamp']= df['Date_Greffe'].apply(lambda x : datetime.strptime(x,\"%Y-%m-%d\"))\n",
    "    else:\n",
    "        df['file_timestamp']=timestamp_    \n",
    "\n",
    "    # Save file to destination in S3\n",
    "    # Create buffer\n",
    "    csv_buffer = StringIO()\n",
    "    # Write dataframe to buffer\n",
    "    df.to_csv(csv_buffer, sep=\";\", index=False)\n",
    "    # Create S3 object\n",
    "    s3_resource = client['resource']\n",
    "    # Write buffer to S3 object\n",
    "    s3_resource.Object(bucket_name, dest_full_path).put(Body=csv_buffer.getvalue())\n",
    "\n",
    "    \n",
    "    return dest_full_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare all items in a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select folder\n",
    "folder='INPI/TC_1/00_RawData/public/IMR_Donnees_Saisies/tc/flux/2018/01/01'\n",
    "folder_str='rawFlux20180101'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_bucket = bucket.objects.filter(Prefix=folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional filter on certain filenames\n",
    "\n",
    "filter_ = (\n",
    "#'1_PM.csv',\n",
    "#'2_PM_EVT.csv',\n",
    "#'3_PP.csv',\n",
    "#'4_PP_EVT.csv',\n",
    "#'5_rep.csv',\n",
    "#'6_rep_nouveau_modifie_EVT.csv',\n",
    "#'7_rep_partant_EVT.csv',\n",
    "#'8_ets.csv',\n",
    "#'9_ets_nouveau_modifie_EVT.csv',\n",
    "'10_ets_supprime_EVT.csv',\n",
    "#'11_obs.csv',\n",
    "'12_actes.csv',\n",
    "'13_comptes_annuels.csv'\n",
    "    )\n",
    "\n",
    "list_bucket_filter=[]\n",
    "for obj in tqdm(list_bucket):\n",
    "    if obj.key.endswith(filter_):\n",
    "        list_bucket_filter.append(obj)\n",
    "        \n",
    "list_bucket=list_bucket_filter if filter_ else list_bucket_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save todo list to a file\n",
    "import csv\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "logtd_name=\"{}/{}_{}-{}\".format('data',timestr,folder_str,'todo.csv')\n",
    "logtd = open(logtd_name, 'w')\n",
    "with logtd as myfile:\n",
    "    wr = csv.writer(myfile,delimiter=';',quoting=csv.QUOTE_ALL)\n",
    "    wr.writerow(['key'])\n",
    "    for obj in tqdm(list_bucket):\n",
    "        wr.writerow([obj.key])\n",
    "logtd.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(logtd,logv,logt,loge)=log_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in tqdm(list_bucket):\n",
    "\n",
    "    try:\n",
    "        print(obj.key, file=logv)\n",
    "        prepare_object(obj)\n",
    "        print(obj.key, file=logt)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print((obj.key,e), file=loge)\n",
    "\n",
    "logv.close()\n",
    "logt.close()\n",
    "loge.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats and error recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats \n",
    "dftd=pd.read_csv(logtd.name,header=0, sep=';')# Todo\n",
    "td=len(dftd)\n",
    "dft=pd.read_csv(logt.name,header=0, sep=';')\n",
    "t=len(dft)\n",
    "dfe=pd.read_csv(loge.name,header=0, sep=';',usecols = ['key'])\n",
    "e=len(dfe)\n",
    "# Find untreated objects\n",
    "dfnt=pd.DataFrame()\n",
    "dfnt=dftd.merge(dft,indicator = True, how='left').loc[lambda x : x['_merge']!='both']\n",
    "dfnt=dfnt.drop(columns=['_merge'])\n",
    "nt=len(dfnt)\n",
    "# Add errors\n",
    "dfnt=dfnt.append(dfe,sort=True)\n",
    "nt=len(dfnt)\n",
    "\n",
    "\n",
    "\n",
    "# Print stats\n",
    "print('Folder :' + folder)\n",
    "if filter:\n",
    "    print(\"Filter : %s\" % (filter_,))\n",
    "print('To do : ' + str(td))\n",
    "print('Treated : ' + str(t))\n",
    "print('Errors : ' + str(e))\n",
    "print('Retake : ' + str(nt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare from a file list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save errors to a new todo\n",
    "list_bucket_keys = dfnt['key'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or Create the new todo from a list of Missing files\n",
    "folder_str='prepFluxMissing2018'\n",
    "df=pd.read_csv('data/count/missingPrepFlux2018.csv',header=None, sep=';')# Todo\n",
    "df.columns = ['key']\n",
    "list_bucket_keys = df['key'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save list to a new todo\n",
    "import csv\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "logtd_name=\"{}/{}_{}-{}\".format('data',timestr,folder_str,'todo.csv')\n",
    "logtd = open(logtd_name, 'w')\n",
    "with logtd as myfile:\n",
    "    wr = csv.writer(myfile,delimiter=';',quoting=csv.QUOTE_ALL)\n",
    "    wr.writerow(['key'])\n",
    "    for key in tqdm(list_bucket_keys):\n",
    "        wr.writerow([key])\n",
    "logtd.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(logtd,logv,logt,loge)=log_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart on errors\n",
    "for n,key in tqdm(enumerate(list_bucket_keys)):\n",
    "    try:\n",
    "        print(key, file=logv)\n",
    "        \n",
    "        # get object in S3 by its key\n",
    "        obj=client['resource'].Object(bucket_name=bucket_name, key=key)\n",
    "        f=prepare_object(obj)\n",
    "        print(key, file=logt)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print((key,e), file=loge)\n",
    "\n",
    "logv.close()\n",
    "logt.close()\n",
    "loge.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2017 : 31 520 fichiers ~ 1h - 4 erreurs à traiter 20200412-225237_prepFluxMissing2017-errors.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2018 : 137 752 fichiers ~ 4h40?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
