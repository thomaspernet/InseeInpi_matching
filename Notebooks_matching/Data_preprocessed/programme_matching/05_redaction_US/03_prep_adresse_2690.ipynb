{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation regex adresse \n",
    "\n",
    "```\n",
    "Entant que {X} je souhaite {normaliser la variable pays} afin de {pouvoir la faire correspondre à l'INSEE}\n",
    "```\n",
    "\n",
    "**Metadatab**\n",
    "\n",
    "- Taiga:\n",
    "    - Numero US: [2690](https://tree.taiga.io/project/olivierlubet-air/us/2690)\n",
    "- Gitlab\n",
    "    - Notebook: [03_prep_adresse_2690](https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/05_redaction_US/03_prep_adresse_2690.ipynb)\n",
    "    - Markdown: [03_prep_adresse_2690](https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/05_redaction_US/03_prep_adresse_2690.md)\n",
    "    - Data:\n",
    "        - [inpi_ets_exemple_1](https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/data/RawData/INPI/Stock/inpi_ets_exemple_1.csv)\n",
    "        - [upper_stop](https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/data/input/Parameters/upper_stop.csv)\n",
    "\n",
    "# Contexte\n",
    "\n",
    "En plus des variables de matching, l'étape de siretisation va utiliser le regex pattern pour être sur de fiabilité, mais aussi aider au dédoublonnage. Pour cela, nous avons mis en place une règle de gestion pour s'assurer de la fiabilité du matching et du dédoublonnage via une variable contenant un pattern regex. Des lors, nous pouvons comparer le pattern regex recréé via les variables de l'adresses et le comparer à la variable adresse de l'INSEE qui elle affiche plus de normalisme.\n",
    "\n",
    "\n",
    "## Règles de gestion\n",
    "\n",
    "*   Définition partiel\n",
    "\n",
    "    *   si csv dans le dossier Stock, année > 2017, alors partiel, c'est a dire, modification complète du dossier due a une anomalie.\n",
    "    *   la date d’ingestion est indiquée dans le path, ie comme les flux\n",
    "*   Une séquence est un classement chronologique pour le quadruplet suivant:\n",
    "\n",
    "    *   _siren_ + _code greffe_ + _numero gestion_ + _ID établissement_\n",
    "*  Une création d'une séquence peut avoir plusieurs transmission a des intervalles plus ou moins long\n",
    "    *   Si plusieurs transmissions avec le libellé “création établissement” ou “création\" , alors il faut prendre la dernière date de transmission\n",
    "    *   Il y a certains cas ou les lignes de créations doublons sont de faux événements (mauvais envoie de la part du greffier)\n",
    "        *   Si le timestamp entre la première ligne et dernière ligne est supérieures a 31 jour (exclut), il faut:\n",
    "            *   Récupération de la dernière ligne, et créer une variable flag, comme pour le statut\n",
    "*   Evénement 1\n",
    "    *   Les événements doivent impérativement suivre l'ordre d'apparition dans le csv du FTP\n",
    "        *   Pour les événements, il est possible d'avoir plusieurs informations renseignées pour une même date de transmission pour une même séquence\n",
    "    *   Le remplissage doit se faire de la manière suivante pour la donnée brute\n",
    "        *   Pour une date de transmission donnée, c'est la dernière ligne de la séquence qui doit être utilisée, remplie des valeurs manquantes extraites des lignes précédentes. Si la dernière ligne de la séquence contient un champs non vide, il ne faut pas la remplacer par la ligne précédente.\n",
    "- Partiel\n",
    "  - En cas de corrections majeures, la séquence annule et remplace la création et événements antérieurs. Dans ce cas, toutes les données qui ont pu être transmises antérieurement via le stock initial ou le flux doivent donc être ignorées (prendre en compte la date de transmission indiquée dans le nom des sous-répertoires du stock et des fichiers\n",
    "-  Siren sans Siège ou Principal\n",
    "  - Il est possible qu'un SIREN n'ai pas de siege/principal. Normalement, cela doit être corrigé par un partiel\n",
    "-  Etablissement sans création\n",
    "  - Il arrive que des établissements soient supprimés (EVT) mais n'ont pas de ligne \"création d'entreprise\". Si cela, arrive, Infogreffe doit envoyer un partiel pour corriger. Il arrive que le greffe envoie seulement une ligne pour SEP, lorsque le Principal est fermé, le siège est toujours ouvert. Mais pas de nouvelle ligne dans la base. Le partiel devrait corriger cela.\n",
    "- La variable `ville` de l'INPI n'est pas normalisée. C'est une variable libre de la créativité du greffier, qui doit être formalisée du mieux possible afin de permettre la jointure avec l'INSEE. Plusieurs règles regex ont été recensé comme la soustraction des numéros, caractères spéciaux, parenthèses, etc. Il est possible d'améliorer les règles si nécessaire\n",
    "- Le code postal doit être formalisé correctement, a savoir deux longueurs possibles: zero (Null) ou cinq. Dans certains cas, le code postal se trouve dans la variable de la ville.\n",
    "- La variable pays doit être formalisée, a savoir correspondre au code pays de l'INSEE. Bien que la majeure partie des valeurs soit FRANCE ou France, il convient de normaliser la variable pour récuperer les informations des pays hors France.\n",
    "- [NEW] Les variables de l'adresse de l'INPI ne sont pas normalisées, et ne peuvent être utilisées en l'état. Il est donc indispensable de retravailler les variables adresse pour pouvoir les comparer avec l'INSEE. Nous utilisons une règle (pattern) regex pour vérifier si les mots contenus dans l'adresse de l'INPI sont aussi contenus à l'INSEE.\n",
    "\n",
    "Workflow US (via stock)\n",
    "\n",
    "![workflow](https://www.lucidchart.com/publicSegments/view/d9e4494d-bfaf-4d0e-9e0f-53011cda7eb9/image.png)\n",
    "\n",
    "# US / ISSUES liées\n",
    "\n",
    "[PO & DEV : s'il existe des références, les inscrire]\n",
    "\n",
    "# Besoin\n",
    "\n",
    "Dans cette US, le besoin est le suivant:\n",
    "\n",
    "- Création d'une variable combinant les 3 variables adresse de l'INPI, néttoyée des accents, espaces et mise en majuscule.\n",
    "- Création d'une variable combinant les 3 variables adresse de l'INPI contenant un pattern regex, qui va servir à la comparaison avec la variable adresse de l'INSEE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spécifications\n",
    "\n",
    "### Origine information (si applicable) \n",
    "\n",
    "- Metadata:\n",
    "    - Type: [CSV]\n",
    "    - Source: [Gitlab](https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/data/RawData/INPI/Stock/inpi_ets_exemple_1.csv)\n",
    "    - Summary: Echantillon aléatoire de 3000 observations récupérées de notre table ETS\n",
    "    - Type: [CSV]\n",
    "    - Source: [Gitlab](https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/data/input/Parameters/upper_stop.csv)\n",
    "    - Summary: Liste de stop word a retirer dans l'adresse\n",
    "        \n",
    "## Input\n",
    "\n",
    "[PO : dans le cas de transformation de données, préciser ,les sources :\n",
    "\n",
    "*   Applications\n",
    "*   Schémas\n",
    "*   Tables: `inpi_etablissement_historique`\n",
    "*   CSV: [upper_stop](https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/data/input/Parameters/upper_stop.csv)\n",
    "*   Champs: `adresse_ligne1`, `adresse_ligne2` et `adresse_ligne3`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple Input 1\n",
    "\n",
    "L'exemple ci-dessous indique uniquement la donnée brute de la table `inpi_etablissement_historique` avec les 3 champs dont nous avons besoin: `adresse_ligne1`, `adresse_ligne2` et `adresse_ligne3`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etb_ex = 'https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/raw'\\\n",
    "'/master/Notebooks_matching/Data_preprocessed/programme_matching/data/RawData' \\\n",
    "'/INPI/Stock/inpi_ets_exemple_1.csv'\n",
    "\n",
    "df_ets = pd.read_csv(etb_ex)\n",
    "print(df_ets[['siren', 'adresse_ligne1', 'adresse_ligne2',\n",
    "              'adresse_ligne3']].head().to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple Input 2\n",
    "\n",
    "L'exemple ci-dessous indique une liste de candidat au stop word. Tous les mots contenus dans cette liste vont être enlever de l'adresse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word = 'https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/raw/' \\\n",
    "'master/Notebooks_matching/Data_preprocessed/programme_matching/data/input/' \\\n",
    "'Parameters/upper_stop.csv'\n",
    "df_stop = pd.read_csv(stop_word, names=[\"stop\"])\n",
    "print(df_stop.head().to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output\n",
    "\n",
    "[PO : dans le cas de transformation de données, préciser les sorties :\n",
    "\n",
    "*   BDD cibles\n",
    "*   Tables: `inpi_etablissement_historique`\n",
    "*   Champs: `adress_nettoyee`, `adresse_regex`\n",
    "\n",
    "]\n",
    "\n",
    "Le tableau ci dessous explicite les deux variables attendues, a savoir `adress_nettoyee` et `adresse_regex`. Les deux variables vont utiliser les variables de l'adresse pour reconstituer une adresse nettoyée puis en faire un pattern regex.\n",
    "\n",
    "- La variable `adress_nettoyee` est la recombinaision des trois variables de l'adresse, nettoyée des accents, espace en debut de texte et mise en majuscule.\n",
    "- La variable `adresse_regex` est la création du pattern regex, ayant le signe `$` en fin de mot et séparer avec `|`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_split_adress(x):\n",
    "        \"\"\"\n",
    "        Découpe l'adresse en une liste de mots. L'input de la fonction est\n",
    "        généralement une adresse au préalable normalisée. Exemple:\n",
    "        Adresse : \"JONQUILLES JAUNES BORD MER\" -> [JONQUILLES,JAUNES,BORD,MER]\n",
    "\n",
    "        Args:\n",
    "        - x: Une serie contenant l'adresse. De préférence, une serie avec une\n",
    "        adresse normalisée\n",
    "\n",
    "        Return:\n",
    "        Une liste\n",
    "        \"\"\"\n",
    "        split_ = x.str.split().to_list()\n",
    "        return  split_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_regex_adress(x):\n",
    "        \"\"\"\n",
    "        Regroupe les mots de l'adresse ensemble avec comme séparateur \"|\" et\n",
    "        le signe $ en fin de mot pour indiquer qu'il ne faut parser que le mot\n",
    "        en question et pas ce qu'il y a après.\n",
    "\n",
    "        Args:\n",
    "        - x: column conntenant l'adresse dans un dataFrame pandas\n",
    "\n",
    "        Returns:\n",
    "        un String concatenés des mots de la colonne\n",
    "        \"\"\"\n",
    "        try:\n",
    "            split_ = [i + \"$\" for i in x]\n",
    "            reg = '|'.join(split_)\n",
    "        except:\n",
    "            reg = np.nan\n",
    "        return  reg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_adress(df):\n",
    "        \"\"\"\n",
    "        Créer deux colonnes nétoyées de l'adresse a partir d'un dataframe INPI.\n",
    "        La première variable va nétoyer l'adresse en enlevant les valeurs comme\n",
    "        route, avenue qui ne sont pas indiquées dans l'INSEE (variables de\n",
    "        matching)n netoie les accents, digits, etc. La deuxième variable va\n",
    "        concatener l'adresse en vue d'un parsing regex\n",
    "\n",
    "        Args:\n",
    "        - df: Pandas DataFrame\n",
    "\n",
    "        Returns:\n",
    "        DataFrame Pandas nétoyé avec les variables adresses nétoyées\n",
    "        \"\"\"\n",
    "        temp_adresse = (df\n",
    "        .assign(\n",
    "\n",
    "        adress_nettoyee = lambda x:\n",
    "            x['adresse_ligne1'].fillna('') + ' '+\\\n",
    "            x['adresse_ligne2'].fillna('') + ' '+\\\n",
    "            x['adresse_ligne3'].fillna(''),\n",
    "        adresse_new_clean=lambda x: x['adress_nettoyee'].str.normalize(\n",
    "                'NFKD')\n",
    "            .str.encode('ascii', errors='ignore')\n",
    "            .str.decode('utf-8')\n",
    "            .str.replace('[^\\w\\s]|\\d+', ' ')\n",
    "            .str.upper(),\n",
    "        )\n",
    "        .assign(\n",
    "        adresse_new_clean = lambda x: x['adresse_new_clean'].apply(\n",
    "        lambda x:' '.join([word for word in str(x).split() if word not in\n",
    "        (df_stop['stop'].to_list())])),\n",
    "        adress_nettoyee = lambda x: x['adress_nettoyee'].str.normalize(\n",
    "            'NFKD')\n",
    "        .str.encode('ascii', errors='ignore')\n",
    "        .str.decode('utf-8')\n",
    "        .str.replace('[^\\w\\s]', ' ')\n",
    "        .str.upper(),\n",
    "        adresse_new_clean_split=lambda x:\n",
    "        create_split_adress(x['adresse_new_clean']\n",
    "        ),\n",
    "        adresse_regex = lambda x:\n",
    "        x['adresse_new_clean_split'].apply(lambda x:\n",
    "        create_regex_adress(x))\n",
    "        )\n",
    "                        .drop(columns = ['adresse_new_clean','adresse_new_clean_split'])\n",
    "                       )\n",
    "\n",
    "        return temp_adresse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prepare_adress(df =df_ets)[['siren',\n",
    "                           'adresse_ligne1',\n",
    "                           'adresse_ligne2',\n",
    "                           'adresse_ligne3',\n",
    "                           'adress_nettoyee',\n",
    "                           'adresse_regex'\n",
    "                          ]\n",
    "                         ].head().to_markdown())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare_adress(df =df_ets)[['siren',\n",
    "#                           'adresse_ligne1',\n",
    "#                           'adresse_ligne2',\n",
    "#                           'adresse_ligne3',\n",
    "#                           'adress_nettoyee',\n",
    "#                           'adresse_regex'\n",
    "#                          ]\n",
    "#                         ].to_csv('inpi_ets_exemple_1_2697.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Règles de gestion applicables\n",
    "\n",
    "[PO : Formules applicables]\n",
    "\n",
    "- [NEW] Les variables de l'adresse de l'INPI ne sont pas normalisées, et ne peuvent être utilisées en l'état. Il est donc indispensable de retravailler les variables adresse pour pouvoir les comparer avec l'INSEE. Nous utilisons une règle (pattern) regex pour vérifier si les mots contenus dans l'adresse de l'INPI sont aussi contenus à l'INSEE.\n",
    "\n",
    "# Charges de l'équipe\n",
    "\n",
    "[\n",
    "\n",
    "PO : Si des étapes particulières / des points d'attention sont attendus, être aussi explicite que possible\n",
    "\n",
    "Spécifiquement pour l'intégration de nouvelles données dans DATUM :\n",
    "\n",
    "*   Nombre de lignes chargées pour chaque nouvelle table\n",
    "*   Poids de chaque nouvelle table\n",
    "*   Durée du traitement ajouté (+ durée avant et après)\n",
    "\n",
    "]\n",
    "\n",
    "Pour construire la variable `adresse_regex`, nous avons créé 3 fonctions Python, que nous avons utilisé de manière séquentielle:\n",
    "\n",
    "- `create_split_adress`: Découpe l'adresse en une liste de mots. [Snipet](https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/snippets/60)\n",
    "- `create_regex_adress`: Regroupe les mots de l'adresse ensemble avec comme séparateur \"|\" et le signe `$` [Snipet](https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/snippets/61)\n",
    "- `prepare_adress`: Créer deux colonnes nétoyées de l'adresse a partir d'un dataframe INPI [Snipet](https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/snippets/62)\n",
    "\n",
    "Les étapes sont les suivantes:\n",
    "\n",
    "1. Combiner les variables `adresse_ligne1`, `adresse_ligne2` et `adresse_ligne3` ensemble. Si la variable ne contient pas de valeur, simplement remplir avec un espace vide. Il ne faut pas que l'adresse contienne `nan` ou `NULL` entre deux textes. Exemple \"13 rue de NAN la liberté\". Cette variable est appelée `adress_nettoyee`\n",
    "2. Extraction des accents, des espaces en début de texte, les digits, et remplacer avec ` `.\n",
    "3. Mettre en majuscule \n",
    "4. Extraction des stops word. Bien penser a extraire les stops words en prenant en compte les étapes précédentes, a savoir reconstruction de l'adresse, premier nettoyage et mise en majuscule\n",
    "5. Creation d'une liste avec les mots de l'adrese.  Exemple: Adresse : \"JONQUILLES JAUNES BORD MER\" -> [JONQUILLES,JAUNES,BORD,MER]. \n",
    "6. Creation du pattern regex avec le `$` en fin de mot, séparé par `|`. Exemple `[PROFESSEUR, BERGONIE]` devient `PROFESSEUR$|BERGONIE$`. Cette variable est appelée `adresse_regex`\n",
    "7. Nettoyer `adress_nettoyee` des accents, espaces en début de texte et mise en majuscule\n",
    "\n",
    "# Tests d'acceptance\n",
    "\n",
    "[PO : comment contrôler que la réalisation est conforme]\n",
    "\n",
    "**Code reproduction**\n",
    "\n",
    "\n",
    "- Prendre le csv suivant [inpi_ets_exemple_1](https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/data/RawData/INPI/Stock/inpi_ets_exemple_1.csv) et vérifier d'avoir les memes valeurs pour la variable `adresse_regex`. Si les valeurs diffèrent pour certaines lignes, les indiquer dans un fichier Excel\n",
    "\n",
    "# CONCEPTION\n",
    "\n",
    "Conception réalisée par ............. et ..................\n",
    "\n",
    "[DEV :\n",
    "\n",
    "Important :\n",
    "\n",
    "*   Ce chapitre doit impérativement être complété **avant de basculer l'US à 'développement en cours'**\n",
    "*   La conception doit systématiquement être **faite à deux**\n",
    "*   Il ne doit **pas y avoir de code dans ce chapitre**\n",
    "*   Tout au long du développement, ce chapitre doit être enrichi\n",
    "*   Le nom du binôme ayant participé à la conception doit être précisé dans l'US\n",
    "\n",
    "Contenu :\n",
    "\n",
    "*   Décrire les traitements nouveaux / modifiés : emplacement des fichiers (liens vers GIT), mise en avant des évolutions fortes, impacts dans la chaîne d'exploitation\n",
    "*   Points d'attention spécifiques : notamment sur les règles de gestion et leur mise en oeuvre technique\n",
    "\n",
    "]\n",
    "\n",
    "# Evolution de la documentation\n",
    "\n",
    "[DEV :\n",
    "\n",
    "*   Identifier les champs enrichis dans le dictionnaire de données\n",
    "*   Identifier les impacts dans les documents pérennes DTA, DEXP, Consignes de supervision\n",
    "*   Identifier les impacts dans les documents de MEP (FI)\n",
    "\n",
    "]\n",
    "\n",
    "# Tests réalisés\n",
    "\n",
    "[DEV : préciser les tests réalisés pour contrôler le bon fonctionnement, et les résultats obtenus]\n",
    "\n",
    "# Tests automatiques mis en oeuvre\n",
    "\n",
    "[DEV : préciser les TA et expliciter leur fonctionnement]\n",
    "\n",
    "# Démonstration\n",
    "\n",
    "[DEV : suivant le cas, publier sur le sharepoint et mettre un lien ici soit :\n",
    "\n",
    "*   Capture d'écran\n",
    "*   Vidéo publiée\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, shutil, urllib, ipykernel, json\n",
    "from pathlib import Path\n",
    "from notebook import notebookapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_report(extension = \"html\"):\n",
    "    \"\"\"\n",
    "    Create a report from the current notebook and save it in the \n",
    "    Report folder (Parent-> child directory)\n",
    "    \n",
    "    1. Exctract the current notbook name\n",
    "    2. Convert the Notebook \n",
    "    3. Move the newly created report\n",
    "    \n",
    "    Args:\n",
    "    extension: string. Can be \"html\", \"pdf\", \"markdown\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ### Get notebook name\n",
    "    connection_file = os.path.basename(ipykernel.get_connection_file())\n",
    "    kernel_id = connection_file.split('-', 1)[1].split('.')[0]\n",
    "\n",
    "    for srv in notebookapp.list_running_servers():\n",
    "        try:\n",
    "            if srv['token']=='' and not srv['password']:  \n",
    "                req = urllib.request.urlopen(srv['url']+'api/sessions')\n",
    "            else:\n",
    "                req = urllib.request.urlopen(srv['url']+ \\\n",
    "                                             'api/sessions?token=' + \\\n",
    "                                             srv['token'])\n",
    "            sessions = json.load(req)\n",
    "            notebookname = sessions[0]['name']\n",
    "        except:\n",
    "            pass  \n",
    "    \n",
    "    sep = '.'\n",
    "    path = os.getcwd()\n",
    "    parent_path = str(Path(path).parent)\n",
    "    \n",
    "    ### Path report\n",
    "    #path_report = \"{}/Reports\".format(parent_path)\n",
    "    #path_report = \"{}/Reports\".format(path)\n",
    "    \n",
    "    ### Path destination\n",
    "    name_no_extension = notebookname.split(sep, 1)[0]\n",
    "    if extension == 'markdown':\n",
    "        #extension = 'md'\n",
    "        os.remove(name_no_extension +'.{}'.format('md'))\n",
    "        source_to_move = name_no_extension +'.{}'.format('md')\n",
    "    else:\n",
    "        source_to_move = name_no_extension +'.{}'.format(extension)\n",
    "    dest = os.path.join(path,'US_md', source_to_move)\n",
    "    \n",
    "    print('jupyter nbconvert --no-input --to {} {}'.format(\n",
    "    extension,notebookname))\n",
    "    \n",
    "    ### Generate notebook\n",
    "    os.system('jupyter nbconvert --no-input --to {} {}'.format(\n",
    "    extension,notebookname))\n",
    "    \n",
    "    ### Move notebook to report folder\n",
    "    #time.sleep(5)\n",
    "    shutil.move(source_to_move, dest)\n",
    "    print(\"Report Available at this adress:\\n {}\".format(dest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_report(extension = \"markdown\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
