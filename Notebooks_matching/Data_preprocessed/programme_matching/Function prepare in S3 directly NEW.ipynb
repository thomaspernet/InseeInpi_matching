{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_infos(filename,dl_path):\n",
    "    f_s=dl_path.split('/')\n",
    "    suborigin_=f_s[6]\n",
    "    origin_=f_s[2]\n",
    "    nature_=f_s[5]\n",
    "    year_=f_s[3]\n",
    "    \n",
    "    import datetime\n",
    "    from datetime import datetime\n",
    "\n",
    "    f_s = filename.split('_')\n",
    "    date_=f_s[2]\n",
    "    year_=date_[0:4]\n",
    "    month_=date_[4:6]\n",
    "    day_=date_[6:9]\n",
    "    time_= f_s[3]\n",
    "\n",
    "    datetime_str=date_ + '_' + time_\n",
    "    timestamp_=datetime.strptime(datetime_str,\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    return (nature_,origin_,suborigin_,year_,timestamp_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    # Import S3 connectors librairies\n",
    "    #from awsPy.aws_authorization import aws_connector\n",
    "    import importlib.util\n",
    "    spec = importlib.util.spec_from_file_location(\"aws_connector.py\", \"../../../../aws-python/awsPy/aws_authorization/aws_connector.py\")\n",
    "    aws_connector = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(aws_connector)\n",
    "\n",
    "    #from awsPy.aws_s3 import service_s3\n",
    "    import importlib.util\n",
    "    spec = importlib.util.spec_from_file_location(\"service_s3.py\", \"../../../../aws-python/awsPy/aws_s3/service_s3.py\")\n",
    "    service_s3 = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(service_s3)\n",
    "\n",
    "    # Connect to S3\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "\n",
    "    bucket = 'calfdata'\n",
    "    path = os.getcwd()\n",
    "    parent_path = str(Path(path).parent)\n",
    "    path_cred = \"{}/programme_matching/credential_AWS.json\".format(parent_path)\n",
    "\n",
    "    con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                            region = 'eu-west-3')\n",
    "    client= con.client_boto()\n",
    "    s3 = service_s3.connect_S3(client = client,\n",
    "                          bucket = 'calfdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s3 = boto3.resource('s3')\n",
    "bucket_name='calfdata'\n",
    "bucket = client['resource'].Bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dossier source à parcourir\n",
    "'''prefix_root='INPI/TC_1/Flux'# Attention pas plus loin dans l'arborescence que 'INPI/TC_1/Flux/'\n",
    "# Calcul de la liste des dossiers qui devront être traités\n",
    "\n",
    "list_folders=[]\n",
    "for y in range(2017, 2018):\n",
    "    for m in range(1, 13):\n",
    "        m_ = '0'+ str(m) if m<10 else str(m)\n",
    "        for n in ['ETS']:#['ACTES','ETS','OBS','PM','PP','REP','COMPTES_ANNUELS']\n",
    "            for s in ['EVT','NEW']:\n",
    "                prefix_=\"{}/{}/{}/{}/{}\".format(prefix_root,y,m_,n,s)\n",
    "                list_folders.append(prefix_)\n",
    "list_folders'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recréation pour correction du Flux/2018/Actes/New\n",
    "'''list_folders=['INPI/TC_1/Flux/2018/01/OBS/EVT',\n",
    "             'INPI/TC_1/Flux/2018/02/OBS/EVT',\n",
    "             'INPI/TC_1/Flux/2018/03/OBS/EVT',\n",
    "              'INPI/TC_1/Flux/2018/04/OBS/EVT',\n",
    "              'INPI/TC_1/Flux/2018/05/OBS/EVT',\n",
    "              'INPI/TC_1/Flux/2018/06/OBS/EVT',\n",
    "              'INPI/TC_1/Flux/2018/07/OBS/EVT',\n",
    "              'INPI/TC_1/Flux/2018/08/OBS/EVT',\n",
    "              'INPI/TC_1/Flux/2018/09/OBS/EVT',\n",
    "              'INPI/TC_1/Flux/2018/10/OBS/EVT',\n",
    "              'INPI/TC_1/Flux/2018/11/OBS/EVT',\n",
    "              'INPI/TC_1/Flux/2018/12/OBS/EVT'\n",
    "             ]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_folders=['INPI/TC_1/00_RawData/public/IMR_Donnees_Saisies/tc/flux/2017']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209074/209074 [3:18:49<00:00, 15.68it/s]  \n"
     ]
    }
   ],
   "source": [
    "%%capture cap_out --no-stderr\n",
    "import pandas as pd\n",
    "import io\n",
    "from io import StringIO\n",
    "\n",
    "log = open('data/log-flux-2017-ets.txt', 'w+')\n",
    "\n",
    "for prefix_ in list_folders:\n",
    "    print('treat ' + prefix_)\n",
    "    list_bucket = list(bucket.objects.filter(Prefix=prefix_))\n",
    "    list_bucket=list_bucket[99000:]\n",
    "    print(str(len(list_bucket)))\n",
    "    \n",
    "    \n",
    "    for obj in tqdm(list_bucket):\n",
    "        \n",
    "        try:\n",
    "            key = obj.key\n",
    "            body = obj.get()['Body'].read()\n",
    "            print(key, file=log)\n",
    "\n",
    "            s=key.split('/')\n",
    "            filename=s[len(s)-1]\n",
    "\n",
    "            (nature_,origin_,suborigin_,year_,timestamp_) = u.get_file_infos(filename)\n",
    "            dest_path=\"{}/{}/{}/{}/{}\".format('INPI/TC_1/01_donnee_source',origin_,year_,nature_,suborigin_)\n",
    "            dest_full_path=\"{}/{}\".format(dest_path,filename)\n",
    "            \n",
    "            \n",
    "            \n",
    "            if nature_ == 'ETS':\n",
    "            \n",
    "                df = pd.read_csv(io.BytesIO(body), header=0, dtype=str, sep = ';',error_bad_lines=False)\n",
    "                if nature_ == 'REP':\n",
    "                    # Rename col 'date_greffe' to 'Date_Greffe'\n",
    "                    df=df.rename(columns={\"date_greffe\": \"Date_Greffe\"})\n",
    "\n",
    "                df['csv_source']=filename\n",
    "                df['nature']=nature_\n",
    "                df['type']=origin_\n",
    "                df['origin']=suborigin_\n",
    "\n",
    "                if origin_=='Stock' and year_ == '2017' and nature_ == 'ACTES': \n",
    "                    # Tout le stock étant initialisé à la même date en 2017, on utilise plutôt la Date_Dépôt\n",
    "                    print('toto')\n",
    "                    df['file_timestamp']= df['Date_Dépôt'].apply(lambda x : datetime.strptime(x,\"%Y-%m-%d\"))\n",
    "                elif origin_=='Stock' and year_ == '2017' and nature_ == 'COMPTES_ANNUELS': \n",
    "                    # Tout le stock étant initialisé à la même date en 2017, on utilise plutôt la Date_Dépôt\n",
    "                    df['file_timestamp']= df['Date_Dépôt'].apply(lambda x : datetime.strptime(x,\"%Y-%m-%d\"))\n",
    "                elif origin_=='Stock' and year_ == '2017': \n",
    "                    # Tout le stock étant initialisé à la même date en 2017, on utilise plutôt la Date_Greffe\n",
    "                    df['file_timestamp']= df['Date_Greffe'].apply(lambda x : datetime.strptime(x,\"%Y-%m-%d\"))\n",
    "                else:\n",
    "                    df['file_timestamp']=timestamp_ \n",
    "\n",
    "                \n",
    "                # Save file to destination in S3\n",
    "                # Create buffer\n",
    "                csv_buffer = StringIO()\n",
    "                # Write dataframe to buffer\n",
    "                df.to_csv(csv_buffer, sep=\";\", index=False)\n",
    "                # Create S3 object\n",
    "                s3_resource = client['resource']\n",
    "                # Write buffer to S3 object\n",
    "                s3_resource.Object(bucket_name, dest_full_path).put(Body=csv_buffer.getvalue())\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "log.close()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'treat INPI/TC_1/00_RawData/public/IMR_Donnees_Saisies/tc/flux/2017\\n308074\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var = cap_out.stdout\n",
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'INPI/TC_1/00_RawData/public/IMR_Donnees_Saisies/tc/flux/2017/07/14/9401/94/9401_94_20170714_065935_2_PM_EVT.csv'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_bucket[99000].key"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
