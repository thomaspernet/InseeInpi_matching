{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def prepare(dl_path,prep_path):\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import utils as u\n",
    "\n",
    "    # Add information about the source inside csv file\n",
    "    # r=root, d=directories, f = files\n",
    "    for r, d, f in os.walk(dl_path):\n",
    "        for filename in f:\n",
    "            print(filename)\n",
    "            if filename.endswith('.csv'):\n",
    "                u.add_source_info(filename,r,prep_path)\n",
    "                print('added')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STOCK 2017\n",
    "'''\n",
    "dl_path = 'stock-2017-unzipped'\n",
    "prep_path='data/prep2'\n",
    "#unzip(dl_path)\n",
    "prepare(dl_path,prep_path)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_infos(filename,dl_path):\n",
    "    f_s=dl_path.split('/')\n",
    "    suborigin_=f_s[6]\n",
    "    origin_=f_s[2]\n",
    "    nature_=f_s[5]\n",
    "    year_=f_s[3]\n",
    "    \n",
    "    import datetime\n",
    "    from datetime import datetime\n",
    "\n",
    "    f_s = filename.split('_')\n",
    "    date_=f_s[2]\n",
    "    year_=date_[0:4]\n",
    "    month_=date_[4:6]\n",
    "    day_=date_[6:9]\n",
    "    time_= f_s[3]\n",
    "\n",
    "    datetime_str=date_ + '_' + time_\n",
    "    timestamp_=datetime.strptime(datetime_str,\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    return (nature_,origin_,suborigin_,year_,timestamp_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Import S3 connectors librairies\n",
    "    #from awsPy.aws_authorization import aws_connector\n",
    "    import importlib.util\n",
    "    spec = importlib.util.spec_from_file_location(\"aws_connector.py\", \"C:/Users/Hp/Documents/GitHub/aws-python/awsPy/aws_authorization/aws_connector.py\")\n",
    "    aws_connector = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(aws_connector)\n",
    "\n",
    "    #from awsPy.aws_s3 import service_s3\n",
    "    import importlib.util\n",
    "    spec = importlib.util.spec_from_file_location(\"service_s3.py\", \"C:/Users/Hp/Documents/GitHub/aws-python/awsPy/aws_s3/service_s3.py\")\n",
    "    service_s3 = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(service_s3)\n",
    "\n",
    "    # Connect to S3\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "\n",
    "    bucket = 'calfdata'\n",
    "    path = os.getcwd()\n",
    "    parent_path = str(Path(path).parent)\n",
    "    path_cred = \"{}/programme_matching/credential_AWS.json\".format(parent_path)\n",
    "\n",
    "    con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                            region = 'eu-west-3')\n",
    "    client= con.client_boto()\n",
    "    s3 = service_s3.connect_S3(client = client,\n",
    "                          bucket = 'calfdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s3 = boto3.resource('s3')\n",
    "bucket_name='calfdata'\n",
    "bucket = client['resource'].Bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dossier source à parcourir\n",
    "prefix_root='INPI/TC_1/Flux'# Attention pas plus loin dans l'arborescence que 'INPI/TC_1/Flux/'\n",
    "# Calcul de la liste des dossiers qui devront être traités\n",
    "\n",
    "list_folders=[]\n",
    "for y in range(2018, 2020):\n",
    "    for m in range(1, 13):\n",
    "        m_ = '0'+ str(m) if m<10 else str(m)\n",
    "        for n in ['ACTES','ETS','OBS','PM','PP','REP','COMPTES_ANNUELS']:\n",
    "            for s in ['EVT','NEW']:\n",
    "                prefix_=\"{}/{}/{}/{}/{}\".format(prefix_root,y,m_,n,s)\n",
    "                list_folders.append(prefix_)\n",
    "list_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Il n'y a pas de fichier Stock dans prefix_='INPI/TC_1/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour avoir un exemple de fichier source\n",
    "prefix_='INPI/TC_1/Flux/2018/01/PP/EVT'\n",
    "list_files=s3.list_files_s3(prefix_)\n",
    "list_files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour avoir un exemple de fichier source Flux NEW\n",
    "filename='0101_163_20180103_084810_12_actes.csv'\n",
    "key='INPI/TC_1/Flux/2018/01/ACTES/NEW/0101_163_20180103_084810_12_actes.csv'\n",
    "dl_path='INPI/TC_1/Flux/2018/01/ACTES/NEW/'\n",
    "dest_path='INPI/TC_1/01_donnee_source/Flux/2018/ACTES/NEW/'# Doit être calculé\n",
    "\n",
    "(nature_,origin_,suborigin_,year_,timestamp_)=get_file_infos(filename,dl_path)\n",
    "(nature_,origin_,suborigin_,year_,timestamp_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour avoir un exemple de fichier source Flux EVT\n",
    "filename='0101_163_20180103_084810_4_PP_EVT.csv'\n",
    "key='INPI/TC_1/Flux/2018/01/PP/EVT/0101_163_20180103_084810_4_PP_EVT.csv'\n",
    "dl_path='INPI/TC_1/Flux/2018/01/PP/EVT'\n",
    "dest_path='INPI/TC_1/01_donnee_source/Flux/2018/PP/EVT/'# Doit être calculé\n",
    "\n",
    "(nature_,origin_,suborigin_,year_,timestamp_)=get_file_infos(filename,dl_path)\n",
    "(nature_,origin_,suborigin_,year_,timestamp_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recréation pour correction du Flux/2018/Actes/New\n",
    "list_folders=['INPI/TC_1/Flux/2018/01/ACTES/NEW',\n",
    "             'INPI/TC_1/Flux/2018/02/ACTES/NEW',\n",
    "             'INPI/TC_1/Flux/2018/03/ACTES/NEW',\n",
    "              'INPI/TC_1/Flux/2018/04/ACTES/NEW',\n",
    "              'INPI/TC_1/Flux/2018/05/ACTES/NEW',\n",
    "              'INPI/TC_1/Flux/2018/06/ACTES/NEW',\n",
    "              'INPI/TC_1/Flux/2018/07/ACTES/NEW',\n",
    "              'INPI/TC_1/Flux/2018/08/ACTES/NEW',\n",
    "              'INPI/TC_1/Flux/2018/09/ACTES/NEW',\n",
    "              'INPI/TC_1/Flux/2018/10/ACTES/NEW',\n",
    "              'INPI/TC_1/Flux/2018/11/ACTES/NEW',\n",
    "              'INPI/TC_1/Flux/2018/12/ACTES/NEW'\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "treat INPI/TC_1/Flux/2018/01/ACTES/NEW\n",
      "treat INPI/TC_1/Flux/2018/02/ACTES/NEW\n",
      "treat INPI/TC_1/Flux/2018/03/ACTES/NEW\n",
      "treat INPI/TC_1/Flux/2018/04/ACTES/NEW\n",
      "treat INPI/TC_1/Flux/2018/05/ACTES/NEW\n",
      "treat INPI/TC_1/Flux/2018/06/ACTES/NEW\n",
      "treat INPI/TC_1/Flux/2018/07/ACTES/NEW\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "from io import StringIO\n",
    "\n",
    "for prefix_ in list_folders:\n",
    "    print('treat ' + prefix_)\n",
    "    for obj in list(bucket.objects.filter(Prefix=prefix_)):\n",
    "        try:\n",
    "            key = obj.key\n",
    "            body = obj.get()['Body'].read()\n",
    "            df = pd.read_csv(io.BytesIO(body), header=0, dtype=str, sep = ';',error_bad_lines=False)\n",
    "\n",
    "            s=key.split('/')\n",
    "            filename=s[len(s)-1]\n",
    "            #print(filename)\n",
    "\n",
    "            (nature_,origin_,suborigin_,year_,timestamp_) = get_file_infos(filename,prefix_)\n",
    "            dest_path=\"{}/{}/{}/{}/{}\".format('INPI/TC_1/01_donnee_source',origin_,year_,nature_,suborigin_)\n",
    "            dest_full_path=\"{}/{}\".format(dest_path,filename)\n",
    "\n",
    "            df['csv_source']=filename\n",
    "            df['nature']=nature_\n",
    "            df['type']=origin_\n",
    "            df['origin']=suborigin_\n",
    "\n",
    "            if nature_ == 'REP':\n",
    "                # Rename col 'date_greffe' to 'Date_Greffe'\n",
    "                df=df.rename(columns={\"date_greffe\": \"Date_Greffe\"})\n",
    "\n",
    "            if origin_=='Stock' and year_ == '2017' and nature_ == 'ACTES': \n",
    "                # Tout le stock étant initialisé à la même date en 2017, on utilise plutôt la Date_Dépôt\n",
    "                print('toto')\n",
    "                df['file_timestamp']= df['Date_Dépôt'].apply(lambda x : datetime.strptime(x,\"%Y-%m-%d\"))\n",
    "            elif origin_=='Stock' and year_ == '2017' and nature_ == 'COMPTES_ANNUELS': \n",
    "                # Tout le stock étant initialisé à la même date en 2017, on utilise plutôt la Date_Dépôt\n",
    "                df['file_timestamp']= df['Date_Dépôt'].apply(lambda x : datetime.strptime(x,\"%Y-%m-%d\"))\n",
    "            elif origin_=='Stock' and year_ == '2017': \n",
    "                # Tout le stock étant initialisé à la même date en 2017, on utilise plutôt la Date_Greffe\n",
    "                df['file_timestamp']= df['Date_Greffe'].apply(lambda x : datetime.strptime(x,\"%Y-%m-%d\"))\n",
    "            else:\n",
    "                df['file_timestamp']=timestamp_    \n",
    "\n",
    "            #print(filename)    \n",
    "            #print((nature_,origin_,suborigin_,year_,timestamp_))\n",
    "            #print(dest_full_path)\n",
    "            \n",
    "            \n",
    "            # Save file to destination in S3\n",
    "            # Create buffer\n",
    "            csv_buffer = StringIO()\n",
    "            # Write dataframe to buffer\n",
    "            df.to_csv(csv_buffer, sep=\";\", index=False)\n",
    "            # Create S3 object\n",
    "            s3_resource = client['resource']\n",
    "            # Write buffer to S3 object\n",
    "            s3_resource.Object(bucket_name, dest_full_path).put(Body=csv_buffer.getvalue())\n",
    "        except Exception as e:\n",
    "            print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_folders[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
