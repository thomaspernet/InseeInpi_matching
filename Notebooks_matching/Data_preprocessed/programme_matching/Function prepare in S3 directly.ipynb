{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def prepare(dl_path,prep_path):\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import utils as u\n",
    "\n",
    "    # Add information about the source inside csv file\n",
    "    # r=root, d=directories, f = files\n",
    "    for r, d, f in os.walk(dl_path):\n",
    "        for filename in f:\n",
    "            print(filename)\n",
    "            if filename.endswith('.csv'):\n",
    "                u.add_source_info(filename,r,prep_path)\n",
    "                print('added')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STOCK 2017\n",
    "'''\n",
    "dl_path = 'stock-2017-unzipped'\n",
    "prep_path='data/prep2'\n",
    "#unzip(dl_path)\n",
    "prepare(dl_path,prep_path)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_infos(filename,dl_path):\n",
    "    f_s=dl_path.split('/')\n",
    "    suborigin_=f_s[6]\n",
    "    origin_=f_s[2]\n",
    "    nature_=f_s[5]\n",
    "    year_=f_s[3]\n",
    "    \n",
    "    import datetime\n",
    "    from datetime import datetime\n",
    "\n",
    "    f_s = filename.split('_')\n",
    "    date_=f_s[2]\n",
    "    year_=date_[0:4]\n",
    "    month_=date_[4:6]\n",
    "    day_=date_[6:9]\n",
    "    time_= f_s[3]\n",
    "\n",
    "    datetime_str=date_ + '_' + time_\n",
    "    timestamp_=datetime.strptime(datetime_str,\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    return (nature_,origin_,suborigin_,year_,timestamp_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Import S3 connectors librairies\n",
    "    #from awsPy.aws_authorization import aws_connector\n",
    "    import importlib.util\n",
    "    spec = importlib.util.spec_from_file_location(\"aws_connector.py\", \"C:/Users/Hp/Documents/GitHub/aws-python/awsPy/aws_authorization/aws_connector.py\")\n",
    "    aws_connector = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(aws_connector)\n",
    "\n",
    "    #from awsPy.aws_s3 import service_s3\n",
    "    import importlib.util\n",
    "    spec = importlib.util.spec_from_file_location(\"service_s3.py\", \"C:/Users/Hp/Documents/GitHub/aws-python/awsPy/aws_s3/service_s3.py\")\n",
    "    service_s3 = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(service_s3)\n",
    "\n",
    "    # Connect to S3\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "\n",
    "    bucket = 'calfdata'\n",
    "    path = os.getcwd()\n",
    "    parent_path = str(Path(path).parent)\n",
    "    path_cred = \"{}/programme_matching/credential_AWS.json\".format(parent_path)\n",
    "\n",
    "    con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                            region = 'eu-west-3')\n",
    "    client= con.client_boto()\n",
    "    s3 = service_s3.connect_S3(client = client,\n",
    "                          bucket = 'calfdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s3 = boto3.resource('s3')\n",
    "bucket_name='calfdata'\n",
    "bucket = client['resource'].Bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['INPI/TC_1/Flux/2019/05/ETS/EVT',\n",
       " 'INPI/TC_1/Flux/2019/05/ETS/NEW',\n",
       " 'INPI/TC_1/Flux/2019/06/ETS/EVT',\n",
       " 'INPI/TC_1/Flux/2019/06/ETS/NEW',\n",
       " 'INPI/TC_1/Flux/2019/07/ETS/EVT',\n",
       " 'INPI/TC_1/Flux/2019/07/ETS/NEW',\n",
       " 'INPI/TC_1/Flux/2019/08/ETS/EVT',\n",
       " 'INPI/TC_1/Flux/2019/08/ETS/NEW',\n",
       " 'INPI/TC_1/Flux/2019/09/ETS/EVT',\n",
       " 'INPI/TC_1/Flux/2019/09/ETS/NEW',\n",
       " 'INPI/TC_1/Flux/2019/10/ETS/EVT',\n",
       " 'INPI/TC_1/Flux/2019/10/ETS/NEW',\n",
       " 'INPI/TC_1/Flux/2019/11/ETS/EVT',\n",
       " 'INPI/TC_1/Flux/2019/11/ETS/NEW',\n",
       " 'INPI/TC_1/Flux/2019/12/ETS/EVT',\n",
       " 'INPI/TC_1/Flux/2019/12/ETS/NEW']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dossier source à parcourir\n",
    "prefix_root='INPI/TC_1/Flux'# Attention pas plus loin dans l'arborescence que 'INPI/TC_1/Flux/'\n",
    "# Calcul de la liste des dossiers qui devront être traités\n",
    "\n",
    "list_folders=[]\n",
    "for y in range(2019, 2020):\n",
    "    for m in range(5, 13):\n",
    "        m_ = '0'+ str(m) if m<10 else str(m)\n",
    "        for n in ['ETS']:#['ACTES','ETS','OBS','PM','PP','REP','COMPTES_ANNUELS']\n",
    "            for s in ['EVT','NEW']:\n",
    "                prefix_=\"{}/{}/{}/{}/{}\".format(prefix_root,y,m_,n,s)\n",
    "                list_folders.append(prefix_)\n",
    "list_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Il n'y a pas de fichier Stock dans prefix_='INPI/TC_1/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour avoir un exemple de fichier source\n",
    "'''prefix_='INPI/TC_1/Flux/2018/01/PP/EVT'\n",
    "list_files=s3.list_files_s3(prefix_)\n",
    "list_files[:5]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour avoir un exemple de fichier source Flux NEW\n",
    "'''filename='0101_163_20180103_084810_12_actes.csv'\n",
    "key='INPI/TC_1/Flux/2018/01/ACTES/NEW/0101_163_20180103_084810_12_actes.csv'\n",
    "dl_path='INPI/TC_1/Flux/2018/01/ACTES/NEW/'\n",
    "dest_path='INPI/TC_1/01_donnee_source/Flux/2018/ACTES/NEW/'# Doit être calculé\n",
    "\n",
    "(nature_,origin_,suborigin_,year_,timestamp_)=get_file_infos(filename,dl_path)\n",
    "(nature_,origin_,suborigin_,year_,timestamp_)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Pour avoir un exemple de fichier source Flux EVT\n",
    "filename='0101_163_20180103_084810_4_PP_EVT.csv'\n",
    "key='INPI/TC_1/Flux/2018/01/PP/EVT/0101_163_20180103_084810_4_PP_EVT.csv'\n",
    "dl_path='INPI/TC_1/Flux/2018/01/PP/EVT'\n",
    "dest_path='INPI/TC_1/01_donnee_source/Flux/2018/PP/EVT/'# Doit être calculé\n",
    "\n",
    "(nature_,origin_,suborigin_,year_,timestamp_)=get_file_infos(filename,dl_path)\n",
    "(nature_,origin_,suborigin_,year_,timestamp_)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recréation pour correction du Flux/2018/Actes/New\n",
    "'''list_folders=['INPI/TC_1/Flux/2018/01/OBS/EVT',\n",
    "             'INPI/TC_1/Flux/2018/02/OBS/EVT',\n",
    "             'INPI/TC_1/Flux/2018/03/OBS/EVT',\n",
    "              'INPI/TC_1/Flux/2018/04/OBS/EVT',\n",
    "              'INPI/TC_1/Flux/2018/05/OBS/EVT',\n",
    "              'INPI/TC_1/Flux/2018/06/OBS/EVT',\n",
    "              'INPI/TC_1/Flux/2018/07/OBS/EVT',\n",
    "              'INPI/TC_1/Flux/2018/08/OBS/EVT',\n",
    "              'INPI/TC_1/Flux/2018/09/OBS/EVT',\n",
    "              'INPI/TC_1/Flux/2018/10/OBS/EVT',\n",
    "              'INPI/TC_1/Flux/2018/11/OBS/EVT',\n",
    "              'INPI/TC_1/Flux/2018/12/OBS/EVT'\n",
    "             ]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "treat INPI/TC_1/Flux/2019/05/ETS/EVT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7078/7078 [15:15<00:00,  7.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "treat INPI/TC_1/Flux/2019/05/ETS/NEW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3539/3539 [08:05<00:00,  7.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "treat INPI/TC_1/Flux/2019/06/ETS/EVT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 6918/6918 [14:58<00:00,  7.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "treat INPI/TC_1/Flux/2019/06/ETS/NEW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3459/3459 [08:03<00:00,  7.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "treat INPI/TC_1/Flux/2019/07/ETS/EVT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 7240/7240 [1:08:32<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "treat INPI/TC_1/Flux/2019/07/ETS/NEW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|██████████████▋                                                                | 674/3620 [01:27<06:14,  7.88it/s]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "from io import StringIO\n",
    "\n",
    "for prefix_ in list_folders:\n",
    "    print('treat ' + prefix_)\n",
    "    for obj in tqdm(list(bucket.objects.filter(Prefix=prefix_))):\n",
    "        \n",
    "        try:\n",
    "            key = obj.key\n",
    "            body = obj.get()['Body'].read()\n",
    "            df = pd.read_csv(io.BytesIO(body), header=0, dtype=str, sep = ';',error_bad_lines=False)\n",
    "\n",
    "            s=key.split('/')\n",
    "            filename=s[len(s)-1]\n",
    "            #print(filename)\n",
    "\n",
    "            (nature_,origin_,suborigin_,year_,timestamp_) = get_file_infos(filename,prefix_)\n",
    "            dest_path=\"{}/{}/{}/{}/{}\".format('INPI/TC_1/01_donnee_source',origin_,year_,nature_,suborigin_)\n",
    "            dest_full_path=\"{}/{}\".format(dest_path,filename)\n",
    "\n",
    "            df['csv_source']=filename\n",
    "            df['nature']=nature_\n",
    "            df['type']=origin_\n",
    "            df['origin']=suborigin_\n",
    "\n",
    "            if nature_ == 'REP':\n",
    "                # Rename col 'date_greffe' to 'Date_Greffe'\n",
    "                df=df.rename(columns={\"date_greffe\": \"Date_Greffe\"})\n",
    "\n",
    "            if origin_=='Stock' and year_ == '2017' and nature_ == 'ACTES': \n",
    "                # Tout le stock étant initialisé à la même date en 2017, on utilise plutôt la Date_Dépôt\n",
    "                print('toto')\n",
    "                df['file_timestamp']= df['Date_Dépôt'].apply(lambda x : datetime.strptime(x,\"%Y-%m-%d\"))\n",
    "            elif origin_=='Stock' and year_ == '2017' and nature_ == 'COMPTES_ANNUELS': \n",
    "                # Tout le stock étant initialisé à la même date en 2017, on utilise plutôt la Date_Dépôt\n",
    "                df['file_timestamp']= df['Date_Dépôt'].apply(lambda x : datetime.strptime(x,\"%Y-%m-%d\"))\n",
    "            elif origin_=='Stock' and year_ == '2017': \n",
    "                # Tout le stock étant initialisé à la même date en 2017, on utilise plutôt la Date_Greffe\n",
    "                df['file_timestamp']= df['Date_Greffe'].apply(lambda x : datetime.strptime(x,\"%Y-%m-%d\"))\n",
    "            else:\n",
    "                df['file_timestamp']=timestamp_    \n",
    "\n",
    "            #print(filename)    \n",
    "            #print((nature_,origin_,suborigin_,year_,timestamp_))\n",
    "            #print(dest_full_path)\n",
    "            \n",
    "            \n",
    "            # Save file to destination in S3\n",
    "            # Create buffer\n",
    "            csv_buffer = StringIO()\n",
    "            # Write dataframe to buffer\n",
    "            df.to_csv(csv_buffer, sep=\";\", index=False)\n",
    "            # Create S3 object\n",
    "            s3_resource = client['resource']\n",
    "            # Write buffer to S3 object\n",
    "            s3_resource.Object(bucket_name, dest_full_path).put(Body=csv_buffer.getvalue())\n",
    "        except Exception as e:\n",
    "            print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
