{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import boto3, json, io, os\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import Match_inpi_insee.aws_connectors as aws\n",
    "from datetime import datetime\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instanciate AWS connection\n",
    "instance_aws = 'https://calfdata.s3.eu-west-3.amazonaws.com'\n",
    "bucket_name = 'calfdata'\n",
    "AWS_connection = aws.aws_instantiate(instance_aws, bucket_name)\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket(bucket_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterate on all ETS folders\n",
    "columns_=['Siren','Type','Adresse_Ligne1','Adresse_Ligne2','Adresse_Ligne3',\n",
    "          'Code_Postal','Ville','Code_Commune','ID_Etablissement','Date_Greffe','Libelle_Evt']\n",
    "columns_add = columns_ + [ 'file_timestamp', 'Libelle_Evt2']\n",
    "df = pd.DataFrame(columns=columns_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = []\n",
    "error_log = []\n",
    "\n",
    "for y in range(2018, 2020):\n",
    "    for m in range(1, 13):\n",
    "        m_ = '0'+ str(m) if m<10 else str(m)\n",
    "        prefix_='{}/{}/{}/{}'.format('INPI/TC_1/Flux',y,m_,'ETS/EVT')\n",
    "        df = pd.DataFrame(columns=columns_add)\n",
    "        print('starting ' + prefix_)\n",
    "        c=0\n",
    "        e=0\n",
    "        for obj in list(bucket.objects.filter(Prefix=prefix_)):\n",
    "            key = obj.key\n",
    "            body = obj.get()['Body'].read()\n",
    "            try:\n",
    "\n",
    "                df_ = pd.read_csv(io.BytesIO(body), header=0, dtype=str, sep = ';',usecols=columns_,error_bad_lines=False)\n",
    "                \n",
    "                #Adding information from filename\n",
    "                s=key.split('/')\n",
    "                filename=s[len(s)-1]\n",
    "                #print(filename)\n",
    "                f_s=filename.split('_')\n",
    "\n",
    "                # Add timestamp\n",
    "                datetime_str = '{}_{}'.format(f_s[2],f_s[3])\n",
    "                datetime_object = datetime.strptime(datetime_str, '%Y%m%d_%H%M%S')\n",
    "                #print(datetime_object)\n",
    "                df_['file_timestamp']=datetime_object\n",
    "\n",
    "                # Add detailed Event Label\n",
    "                df_['Libelle_Evt2']='_'.join(f_s[6:-1])\n",
    "                \n",
    "                df = df.append(df_,sort=False)\n",
    "                c+=1\n",
    "            except:\n",
    "                print('Error on file:' + key)\n",
    "                error_log.append({'key': key})\n",
    "                e+=1\n",
    "            pass\n",
    "\n",
    "        print('End of ' + prefix_ + ' : ' + str(len(df)) + ' rows now loaded.')\n",
    "        log.append({\n",
    "            'path': prefix_,\n",
    "            'files': c,\n",
    "            'rows': len(df),\n",
    "            'errors': e\n",
    "        })\n",
    "        \n",
    "        df.to_csv(r'{}/{}{}.{}'.format('ets_enriched',y,m_, 'csv'))#'gz'),compression='gzip')\n",
    "        json_name = '{}/{}{}-{}'.format('logs',y,m_, 'log.json')\n",
    "        #print('json_name ' + json_name)\n",
    "        json_errorname = '{}/{}{}-{}'.format('logs',y,m_, 'errorlog.json')\n",
    "        with open(json_name, 'w') as outfile:\n",
    "            json.dump(log, outfile)\n",
    "        with open(json_errorname, 'w') as outfile:\n",
    "            json.dump(error_log, outfile)\n",
    "        #df = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='0101_10_20170519_063335_10_ets_supprime_EVT.csv'\n",
    "f_s=filename.split('_')\n",
    "\n",
    "# Add timestamp\n",
    "datetime_str = '{}_{}'.format(f_s[2],f_s[3])\n",
    "datetime_object = datetime.strptime(datetime_str, '%Y%m%d_%H%M%S')\n",
    "datetime_object.second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'INPI/TC_1/Flux/2019/05/ETS/EVT/3302_34_20170530_082023_9_ets_nouveau_modifie_EVT.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAF : \n",
    "# save and remove logs\n",
    "#os.remove(json_name)\n",
    "#os.remove(json_errorname)\n",
    "# Read : https://dluo.me/s3databoto3\n",
    "# Read : https://medium.com/@victor.perez.berruezo/download-a-csv-file-from-s3-and-create-a-pandas-dataframe-in-python-ffdb08c2967c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Errors logged\n",
    "\"INPI/TC_1/Flux/2017/05/ETS/EVT/5002_10_20170519_071930_9_ets_nouveau_modifie_EVT.csv\"\n",
    "\"INPI/TC_1/Flux/2017/08/ETS/EVT/0702_70_20170819_080206_9_ets_nouveau_modifie_EVT.csv\"\n",
    "\"INPI/TC_1/Flux/2018/07/ETS/EVT/0101_310_20180705_065824_9_ets_nouveau_modifie_EVT.csv\"\n",
    "\"INPI/TC_1/Flux/2018/10/ETS/EVT/7608_418_20181006_083003_10_ets_supprime_EVT.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.read_csv('ets_enriched/201705.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_['file_timestamp']=pd.to_datetime(df_['file_timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_['year'] = df_['file_timestamp'].dt.year\n",
    "df_['month'] = df_['file_timestamp'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df__2017_05 = df_.loc[lambda x: ( x['year'].isin(['2017'])) & \n",
    "                                               (x['month'].isin(['5']))]\n",
    "df__2017_05.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df__2017_05.drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
