{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcul de la distance de Cosine et Levhenstein\n",
    "\n",
    "# Objective(s)\n",
    "\n",
    "*  Lors de cette US, nous allons créer 6 variables qui vont permettre a la réalisation des tests test_similarite_exception_words et test_distance_levhenstein_exception_words. Les six variables sont les suivantes:\n",
    "  * unzip_inpi: Mot comparé coté inpi\n",
    "  * unzip_insee: Mot comparé coté insee\n",
    "  * max_cosine_distance: Score de similarité entre le mot compaté coté inpi et coté insee\n",
    "  * levenshtein_distance: Nombre d'édition qu'il faut réaliser pour arriver à reproduire les deux mots\n",
    "  * key_except_to_test: Champs clé-valeur pour toutes les possibiltés des mots qui ne sont pas en communs entre l'insee et l'inpi\n",
    "\n",
    "# Metadata\n",
    "\n",
    "* Epic: Epic 6\n",
    "* US: US 5\n",
    "* Date Begin: 9/29/2020\n",
    "* Duration Task: 0\n",
    "* Description: Creation des variables permettant de calculer la distance de levhenstein et le cosine\n",
    "* Step type: Transform table\n",
    "* Status: Active\n",
    "  * Change Status task: Active\n",
    "  * Update table: Modify rows\n",
    "* Source URL:  \n",
    "* Task type: Jupyter Notebook\n",
    "* Users: Thomas Pernet\n",
    "* Watchers: Thomas Pernet\n",
    "* User Account: https://937882855452.signin.aws.amazon.com/console\n",
    "* Estimated Log points: 5\n",
    "* Task tag: #computation,#athena\n",
    "* Toggl Tag: #documentation\n",
    "\n",
    "# Input Cloud Storage [AWS/GCP]\n",
    "\n",
    "## Table/file\n",
    "\n",
    "* Origin: \n",
    "* Athena\n",
    "* Name: \n",
    "* ets_insee_inpi\n",
    "* Github: \n",
    "  * https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/11_sumup_siretisation/00_merge_ets_insee_inpi.md\n",
    "\n",
    "# Destination Output/Delivery\n",
    "\n",
    "## Table/file\n",
    "\n",
    "* Origin: \n",
    "* Athena\n",
    "* Name:\n",
    "* ets_inpi_similarite_max_word2vec\n",
    "* GitHub:\n",
    "* https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/11_sumup_siretisation/06_calcul_cosine_levhenstein.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connexion serveur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsPy.aws_authorization import aws_connector\n",
    "from awsPy.aws_s3 import service_s3\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os, shutil\n",
    "\n",
    "path = os.getcwd()\n",
    "parent_path = str(Path(path).parent)\n",
    "path_cred = r\"{}/credential_AWS.json\".format(parent_path)\n",
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                       region = 'eu-west-3')\n",
    "\n",
    "region = 'eu-west-3'\n",
    "bucket = 'calfdata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                       region = region)\n",
    "client= con.client_boto()\n",
    "s3 = service_s3.connect_S3(client = client,\n",
    "                      bucket = bucket, verbose = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_setting = True\n",
    "if pandas_setting:\n",
    "    cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Lors de cette US, nous allons créer 6 variables qui vont permettre a la réalisation des tests `test_similarite_exception_words` et `test_distance_levhenstein_exception_words`. Les six variables sont les suivantes:\n",
    "\n",
    "- `unzip_inpi`: Mot comparé coté inpi\n",
    "- `unzip_insee`: Mot comparé coté insee\n",
    "- `max_cosine_distance`: Score de similarité entre le mot compaté coté inpi et coté insee\n",
    "- `levenshtein_distance`: Nombre d'édition qu'il faut réaliser pour arriver à reproduire les deux mots\n",
    "- `key_except_to_test`: Champs clé-valeur pour toutes les possibiltés des mots qui ne sont pas en communs entre l'insee et l'inpi\n",
    "* Il faut penser a garder la variable `row_id` \n",
    "- La similarité doit etre calculée sur l'ensemble des éléments non communs, puis il faut récupérer la distance la plus élevée.\n",
    "\n",
    "Le tableau ci dessous récapitule les sous-ensembles 1 à 6, qui vont de la création des types de ressemble entre l'adresse de l'INSEE et de l'INPI jusqu'aux tests sur la date, l'état administratif et de l'ensemble\n",
    "\n",
    " | Rang | Nom_variable                              | Dependence                                    | Notebook                           | Difficulte | Table_generee                                                                                                                                                            | Variables_crees_US                                                                 | Possibilites                  |\n",
    "|------|-------------------------------------------|-----------------------------------------------|------------------------------------|------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------|-------------------------------|\n",
    "| 1    | status_cas                                |                                               | 01_cas_de_figure                   | Moyen      | ets_insee_inpi_status_cas                                                                                                                                              | status_cas,intersection,pct_intersection,union_,inpi_except,insee_except           | CAS_1,CAS_2,CAS_3,CAS_4,CAS_5 |\n",
    "| 2    | test_list_num_voie                        | intersection_numero_voie,union_numero_voie    | 03_test_list_num_voie              | Moyen      | ets_insee_inpi_list_num_voie                                                                                                                                           | intersection_numero_voie,union_numero_voie                                         | FALSE,NULL,TRUE,PARTIAL       |\n",
    "| 3    | test_enseigne                             | list_enseigne,enseigne                        | 04_test_enseigne                   | Moyen      | ets_insee_inpi_list_enseigne                                                                                                                                           | list_enseigne_contain                                                              | FALSE,NULL,TRUE               |\n",
    "| 4    | test_pct_intersection                     | pct_intersection,index_id_max_intersection    | 06_creation_nb_siret_siren_max_pct | Facile     | ets_insee_inpi_var_group_max                                                                                                                                           | count_inpi_index_id_siret,count_inpi_siren_siret,index_id_max_intersection         | FALSE,TRUE                    |\n",
    "| 4    | test_index_id_duplicate                   | count_inpi_index_id_siret                     | 06_creation_nb_siret_siren_max_pct | Facile     | ets_insee_inpi_var_group_max                                                                                                                                           | count_inpi_index_id_siret,count_inpi_siren_siret,index_id_max_intersection         | FALSE,TRUE                    |\n",
    "| 4    | test_siren_insee_siren_inpi               | count_initial_insee,count_inpi_siren_siret    | 06_creation_nb_siret_siren_max_pct | Facile     | ets_insee_inpi_var_group_max                                                                                                                                           | count_inpi_index_id_siret,count_inpi_siren_siret,index_id_max_intersection         | FALSE,TRUE                    |\n",
    "| 5    | test_similarite_exception_words           | max_cosine_distance                           | 08_calcul_cosine_levhenstein       | Difficile  | ets_insee_inpi_similarite_max_word2vec                                                                                                                                 | unzip_inpi,unzip_insee,max_cosine_distance,levenshtein_distance,key_except_to_test | FALSE,NULL,TRUE               |\n",
    "| 5    | test_distance_levhenstein_exception_words | levenshtein_distance                          | 08_calcul_cosine_levhenstein       | Difficile  | ets_insee_inpi_similarite_max_word2vec                                                                                                                                 | unzip_inpi,unzip_insee,max_cosine_distance,levenshtein_distance,key_except_to_test | FALSE,NULL,TRUE               |\n",
    "| 6    | test_date                                 | datecreationetablissement,date_debut_activite | 10_match_et_creation_regles.md     | Facile     | ets_insee_inpi_list_num_voie,ets_insee_inpi_list_enseigne,ets_insee_inpi_similarite_max_word2vec,ets_insee_inpi_status_cas,ets_insee_inpi_var_group_max,ets_insee_inpi |                                                                                    | FALSE,TRUE                    |\n",
    "| 6    | test_siege                                | status_ets,etablissementsiege                 | 10_match_et_creation_regles.md     | Facile     | ets_insee_inpi_list_num_voie,ets_insee_inpi_list_enseigne,ets_insee_inpi_similarite_max_word2vec,ets_insee_inpi_status_cas,ets_insee_inpi_var_group_max,ets_insee_inpi |                                                                                    | FALSE,TRUE,NULL               |\n",
    "| 6    | test_status_admin                         | etatadministratifetablissement,status_admin   | 10_match_et_creation_regles.md     | Facile     | ets_insee_inpi_list_num_voie,ets_insee_inpi_list_enseigne,ets_insee_inpi_similarite_max_word2vec,ets_insee_inpi_status_cas,ets_insee_inpi_var_group_max,ets_insee_inpi |                                                                                    | FALSE,NULL,TRUE               |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output = 'SQL_OUTPUT_ATHENA'\n",
    "database = 'ets_siretisation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "DROP TABLE ets_siretisation.ets_inpi_similarite_max_word2vec;\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database=database,\n",
    "            s3_output=s3_output,\n",
    "  filename = None, ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "CREATE TABLE ets_siretisation.ets_inpi_similarite_max_word2vec\n",
    "WITH (\n",
    "  format='PARQUET'\n",
    ") AS\n",
    "WITH dataset AS (\n",
    "  SELECT \n",
    "    ets_siretisation.ets_insee_inpi.row_id, \n",
    "    index_id, \n",
    "    status_cas, \n",
    "    inpi_except, \n",
    "    insee_except, \n",
    "    transform(\n",
    "      sequence(\n",
    "        1, \n",
    "        CARDINALITY(insee_except)\n",
    "      ), \n",
    "      x -> insee_except\n",
    "    ), \n",
    "    ZIP(\n",
    "      inpi_except, \n",
    "      transform(\n",
    "        sequence(\n",
    "          1, \n",
    "          CARDINALITY(inpi_except)\n",
    "        ), \n",
    "        x -> insee_except\n",
    "      )\n",
    "    ) as test \n",
    "  FROM \n",
    "    ets_siretisation.ets_insee_inpi  \n",
    "    \n",
    "  LEFT JOIN ets_siretisation.ets_insee_inpi_statut_cas \n",
    "  ON ets_siretisation.ets_insee_inpi.row_id = ets_siretisation.ets_insee_inpi_statut_cas.row_id\n",
    "  where \n",
    "    (status_cas != 'CAS_2' AND CARDINALITY(inpi_except)  > 0 AND CARDINALITY(insee_except) > 0)\n",
    "  )\n",
    " SELECT \n",
    "  * \n",
    "FROM \n",
    "  (\n",
    "    WITH distance AS (\n",
    "      SELECT \n",
    "        * \n",
    "      FROM \n",
    "        (\n",
    "          WITH list_weights_insee_inpi AS (\n",
    "            SELECT \n",
    "              row_id, \n",
    "              index_id, \n",
    "              status_cas, \n",
    "              inpi_except, \n",
    "              insee_except, \n",
    "              unzip_inpi, \n",
    "              unzip_insee, \n",
    "              list_weights_inpi, \n",
    "              list_weights_insee \n",
    "            FROM \n",
    "              (\n",
    "                SELECT \n",
    "                  row_id, \n",
    "                  index_id, \n",
    "                  status_cas, \n",
    "                  inpi_except, \n",
    "                  insee_except, \n",
    "                  unzip.field0 as unzip_inpi, \n",
    "                  unzip.field1 as insee, \n",
    "                  test \n",
    "                FROM \n",
    "                  dataset CROSS \n",
    "                  JOIN UNNEST(test) AS new (unzip)\n",
    "              ) CROSS \n",
    "              JOIN UNNEST(insee) as test (unzip_insee) \n",
    "              LEFT JOIN (\n",
    "                SELECT \n",
    "                  words, \n",
    "                  list_weights as list_weights_inpi \n",
    "                FROM \n",
    "                  ets_siretisation.list_weight_mots_insee_inpi_word2vec \n",
    "              ) tb_weight_inpi ON unzip_inpi = tb_weight_inpi.words \n",
    "              LEFT JOIN (\n",
    "                SELECT \n",
    "                  words, \n",
    "                  list_weights as list_weights_insee \n",
    "                FROM \n",
    "                  ets_siretisation.list_weight_mots_insee_inpi_word2vec \n",
    "              ) tb_weight_insee ON unzip_insee = tb_weight_insee.words \n",
    "          ) \n",
    "          SELECT \n",
    "            row_id, \n",
    "            index_id, \n",
    "            status_cas, \n",
    "            inpi_except, \n",
    "            insee_except, \n",
    "            unzip_inpi, \n",
    "            unzip_insee, \n",
    "            REDUCE(\n",
    "              zip_with(\n",
    "                list_weights_inpi, \n",
    "                list_weights_insee, \n",
    "                (x, y) -> x * y\n",
    "              ), \n",
    "              CAST(\n",
    "                ROW(0.0) AS ROW(sum DOUBLE)\n",
    "              ), \n",
    "              (s, x) -> CAST(\n",
    "                ROW(x + s.sum) AS ROW(sum DOUBLE)\n",
    "              ), \n",
    "              s -> s.sum\n",
    "            ) / (\n",
    "              SQRT(\n",
    "                REDUCE(\n",
    "                  transform(\n",
    "                    list_weights_inpi, \n",
    "                    (x) -> POW(x, 2)\n",
    "                  ), \n",
    "                  CAST(\n",
    "                    ROW(0.0) AS ROW(sum DOUBLE)\n",
    "                  ), \n",
    "                  (s, x) -> CAST(\n",
    "                    ROW(x + s.sum) AS ROW(sum DOUBLE)\n",
    "                  ), \n",
    "                  s -> s.sum\n",
    "                )\n",
    "              ) * SQRT(\n",
    "                REDUCE(\n",
    "                  transform(\n",
    "                    list_weights_insee, \n",
    "                    (x) -> POW(x, 2)\n",
    "                  ), \n",
    "                  CAST(\n",
    "                    ROW(0.0) AS ROW(sum DOUBLE)\n",
    "                  ), \n",
    "                  (s, x) -> CAST(\n",
    "                    ROW(x + s.sum) AS ROW(sum DOUBLE)\n",
    "                  ), \n",
    "                  s -> s.sum\n",
    "                )\n",
    "              )\n",
    "            ) AS cosine_distance \n",
    "          FROM \n",
    "            list_weights_insee_inpi\n",
    "        )\n",
    "    ) \n",
    "    SELECT \n",
    "      row_id, \n",
    "      dataset.index_id, \n",
    "      inpi_except, \n",
    "      insee_except, \n",
    "      unzip_inpi, \n",
    "      unzip_insee, \n",
    "      max_cosine_distance,\n",
    "      -- CASE WHEN max_cosine_distance >= .6 THEN 'TRUE' ELSE 'FALSE' END AS test_distance_cosine,\n",
    "      test as key_except_to_test,\n",
    "      levenshtein_distance(unzip_inpi, unzip_insee) AS levenshtein_distance\n",
    "      -- CASE WHEN levenshtein_distance(unzip_inpi, unzip_insee) <=1  THEN 'TRUE' ELSE 'FALSE' END AS test_distance_levhenstein\n",
    "    \n",
    "    FROM \n",
    "      dataset \n",
    "      LEFT JOIN (\n",
    "        SELECT \n",
    "          distance.index_id, \n",
    "          unzip_inpi, \n",
    "          unzip_insee, \n",
    "          max_cosine_distance \n",
    "        FROM \n",
    "          distance \n",
    "          RIGHT JOIN (\n",
    "            SELECT \n",
    "              index_id, \n",
    "              MAX(cosine_distance) as max_cosine_distance \n",
    "            FROM \n",
    "              distance \n",
    "            GROUP BY \n",
    "              index_id\n",
    "          ) as tb_max_distance ON distance.index_id = tb_max_distance.index_id \n",
    "          AND distance.cosine_distance = tb_max_distance.max_cosine_distance\n",
    "      ) as tb_max_distance_lookup ON dataset.index_id = tb_max_distance_lookup.index_id\n",
    "  )\n",
    "\"\"\"\n",
    "\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database=database,\n",
    "            s3_output=s3_output,\n",
    "  filename = None, ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pas a pas\n",
    "\n",
    "Pour récupérer la similarité la plus élevée entre les mots qui ne sont pas communs entre l'adresse de l'INSEE et de l'INPI, il faut suivre plusieurs étapes. Les étapes sont les suivantes:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. filtre et creation ensemble des similarités a calculer\n",
    "\n",
    "- Filtrer les lignes qui ne correspondent pas au cas 2 et qui ont une cardinalité des mots `except` supérieure à 0. Effectivement, il n'est pas nécéssaire de calculer une similarité si l'une des listes, insee ou inpi, est vide.\n",
    "- Création d'un champ clé valeur qui indique l'ensemble des similarités a calculer\n",
    "\n",
    "On utilise les fonctions:\n",
    "\n",
    "- `transform`\n",
    "- `ZIP`\n",
    "- `sequence`\n",
    "\n",
    "La difficulté dans cette étape était de trouver un moyen de dupliquer la liste de l'INSEE pour chacune des clés de la liste de l'INPI. Le trick est d'utiliser `sequence` afin de répeter autant de fois la liste de l'INSEE qu'il y a de clé à l'INPI. Plus précisément, si la liste de l'INPI a deux valeurs, ie deux clés, et que la liste de l'INSEE a trois éléments. La taille de l'INSEE n'a pas d'impact, ce qui est important c'est de connaitre la taille de l'INPI. Dans notre exemple, le code va répéter la liste de l'INSEE 2 fois, car il y a deux clés à l'INPI.\n",
    "\n",
    "Exemple concret:\n",
    "\n",
    "- INPI -> [FRERES, AMADEO]\n",
    "- INSEE -> [MARTYRS, RESISTANCE]\n",
    "- Il faut comparer: \n",
    "    - FRERES -> [MARTYRS, RESISTANCE] \n",
    "    - AMADEO -> [MARTYRS, RESISTANCE]\n",
    "- Clé valeur finale -> [\n",
    "{field0=FRERES, field1=[MARTYRS, RESISTANCE]},\n",
    "{field0=AMADEO, field1=[MARTYRS, RESISTANCE]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH dataset AS (\n",
    "  SELECT \n",
    "    ets_siretisation.ets_insee_inpi.row_id, \n",
    "    index_id, \n",
    "    status_cas, \n",
    "    inpi_except, \n",
    "    insee_except, \n",
    "    transform(\n",
    "      sequence(\n",
    "        1, \n",
    "        CARDINALITY(insee_except)\n",
    "      ), \n",
    "      x -> insee_except\n",
    "    ), \n",
    "    ZIP(\n",
    "      inpi_except, \n",
    "      transform(\n",
    "        sequence(\n",
    "          1, \n",
    "          CARDINALITY(inpi_except)\n",
    "        ), \n",
    "        x -> insee_except\n",
    "      )\n",
    "    ) as test \n",
    "  FROM \n",
    "    ets_siretisation.ets_insee_inpi  \n",
    "    \n",
    "  LEFT JOIN ets_siretisation.ets_insee_inpi_statut_cas \n",
    "  ON ets_siretisation.ets_insee_inpi.row_id = ets_siretisation.ets_insee_inpi_statut_cas.row_id\n",
    "  where \n",
    "    (status_cas != 'CAS_2' AND CARDINALITY(inpi_except)  > 0 AND CARDINALITY(insee_except) > 0)\n",
    "  LIMIT 10\n",
    "  )\n",
    "  \n",
    " SELECT \n",
    "  * \n",
    "  FROM dataset\n",
    "\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database=database,\n",
    "            s3_output=s3_output,\n",
    "  filename = 'repeat', ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Produit cartesien possibilité et liste poids\n",
    "\n",
    "Dans la seconde étape, nous allons \"exploser\" la clé-valeur afin de pouvoir attribuler la liste des poids aux mots de l'INPI et de l'INSEE.\n",
    "\n",
    "Nous allons poursuivre le reste du pas à pas avec l'index `345408`, qui fait référence à l'exemple ci dessus.\n",
    "\n",
    "L'explosion du champs `test` se fait avec la fonction `CROSS JOIN`. La variable `unzip_inpi` correspond aux clés du champs `test` alors que la variable `unzip_insee` correspond aux valeurs. Le `CROSS JOIN` implique 4 lignes au total. \n",
    "\n",
    "Nous avons donc deux colonnes avec les pairs de mots qu'il faut calculer la similarité, et deux colonnes avec les poids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH dataset AS (\n",
    "  SELECT \n",
    "    ets_siretisation.ets_insee_inpi.row_id, \n",
    "    index_id, \n",
    "    status_cas, \n",
    "    inpi_except, \n",
    "    insee_except, \n",
    "    transform(\n",
    "      sequence(\n",
    "        1, \n",
    "        CARDINALITY(insee_except)\n",
    "      ), \n",
    "      x -> insee_except\n",
    "    ), \n",
    "    ZIP(\n",
    "      inpi_except, \n",
    "      transform(\n",
    "        sequence(\n",
    "          1, \n",
    "          CARDINALITY(inpi_except)\n",
    "        ), \n",
    "        x -> insee_except\n",
    "      )\n",
    "    ) as test \n",
    "  FROM \n",
    "    ets_siretisation.ets_insee_inpi  \n",
    "    \n",
    "  LEFT JOIN ets_siretisation.ets_insee_inpi_statut_cas \n",
    "  ON ets_siretisation.ets_insee_inpi.row_id = ets_siretisation.ets_insee_inpi_statut_cas.row_id\n",
    "  where \n",
    "    (status_cas != 'CAS_2' AND CARDINALITY(inpi_except)  > 0 AND CARDINALITY(insee_except) > 0 AND index_id = 728021)\n",
    "  \n",
    "  )\n",
    "SELECT \n",
    "              row_id, \n",
    "              index_id, \n",
    "              status_cas, \n",
    "              inpi_except, \n",
    "              insee_except, \n",
    "              unzip_inpi, \n",
    "              unzip_insee, \n",
    "              list_weights_inpi, \n",
    "              list_weights_insee \n",
    "            FROM \n",
    "              (\n",
    "                SELECT \n",
    "                  row_id, \n",
    "                  index_id, \n",
    "                  status_cas, \n",
    "                  inpi_except, \n",
    "                  insee_except, \n",
    "                  unzip.field0 as unzip_inpi, \n",
    "                  unzip.field1 as insee, \n",
    "                  test \n",
    "                FROM \n",
    "                  dataset CROSS \n",
    "                  JOIN UNNEST(test) AS new (unzip)\n",
    "              ) CROSS \n",
    "              JOIN UNNEST(insee) as test (unzip_insee) \n",
    "              LEFT JOIN (\n",
    "                SELECT \n",
    "                  words, \n",
    "                  list_weights as list_weights_inpi \n",
    "                FROM \n",
    "                  ets_siretisation.list_weight_mots_insee_inpi_word2vec \n",
    "              ) tb_weight_inpi ON unzip_inpi = tb_weight_inpi.words \n",
    "              LEFT JOIN (\n",
    "                SELECT \n",
    "                  words, \n",
    "                  list_weights as list_weights_insee \n",
    "                FROM \n",
    "                  ets_siretisation.list_weight_mots_insee_inpi_word2vec \n",
    "              ) tb_weight_insee ON unzip_insee = tb_weight_insee.words \n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database=database,\n",
    "            s3_output=s3_output,\n",
    "  filename = 'explosion', ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Calcul de la similarité\n",
    "\n",
    "Le calcul de la similarité s'effectue avec la distance de cosine. Pour connaitre le pas a pas, veuillez vous référer au notebook [07_creation_table_poids_Word2Vec.md#test-acceptanceanalyse-du-modele](https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/11_sumup_siretisation/05_creation_table_poids_Word2Vec.md) pour comprendre les étapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query =  \"\"\"\n",
    "WITH dataset AS (\n",
    "  SELECT \n",
    "    ets_siretisation.ets_insee_inpi.row_id, \n",
    "    index_id, \n",
    "    status_cas, \n",
    "    inpi_except, \n",
    "    insee_except, \n",
    "    transform(\n",
    "      sequence(\n",
    "        1, \n",
    "        CARDINALITY(insee_except)\n",
    "      ), \n",
    "      x -> insee_except\n",
    "    ), \n",
    "    ZIP(\n",
    "      inpi_except, \n",
    "      transform(\n",
    "        sequence(\n",
    "          1, \n",
    "          CARDINALITY(inpi_except)\n",
    "        ), \n",
    "        x -> insee_except\n",
    "      )\n",
    "    ) as test \n",
    "  FROM \n",
    "    ets_siretisation.ets_insee_inpi  \n",
    "    \n",
    "  LEFT JOIN ets_siretisation.ets_insee_inpi_statut_cas \n",
    "  ON ets_siretisation.ets_insee_inpi.row_id = ets_siretisation.ets_insee_inpi_statut_cas.row_id\n",
    "  where \n",
    "    (status_cas != 'CAS_2' AND CARDINALITY(inpi_except)  > 0 AND CARDINALITY(insee_except) > 0 AND index_id = 728021)\n",
    "  )\n",
    "  SELECT \n",
    "  * \n",
    "FROM \n",
    "  (\n",
    "    WITH distance AS (\n",
    "SELECT \n",
    "              row_id, \n",
    "              index_id, \n",
    "              status_cas, \n",
    "              inpi_except, \n",
    "              insee_except, \n",
    "              unzip_inpi, \n",
    "              unzip_insee, \n",
    "              list_weights_inpi, \n",
    "              list_weights_insee \n",
    "            FROM \n",
    "              (\n",
    "                SELECT \n",
    "                  row_id, \n",
    "                  index_id, \n",
    "                  status_cas, \n",
    "                  inpi_except, \n",
    "                  insee_except, \n",
    "                  unzip.field0 as unzip_inpi, \n",
    "                  unzip.field1 as insee, \n",
    "                  test \n",
    "                FROM \n",
    "                  dataset CROSS \n",
    "                  JOIN UNNEST(test) AS new (unzip)\n",
    "              ) CROSS \n",
    "              JOIN UNNEST(insee) as test (unzip_insee) \n",
    "              LEFT JOIN (\n",
    "                SELECT \n",
    "                  words, \n",
    "                  list_weights as list_weights_inpi \n",
    "                FROM \n",
    "                  ets_siretisation.list_weight_mots_insee_inpi_word2vec \n",
    "              ) tb_weight_inpi ON unzip_inpi = tb_weight_inpi.words \n",
    "              LEFT JOIN (\n",
    "                SELECT \n",
    "                  words, \n",
    "                  list_weights as list_weights_insee \n",
    "                FROM \n",
    "                  ets_siretisation.list_weight_mots_insee_inpi_word2vec \n",
    "              ) tb_weight_insee ON unzip_insee = tb_weight_insee.words \n",
    "      )\n",
    "    SELECT row_id, \n",
    "            index_id, \n",
    "            status_cas, \n",
    "            inpi_except, \n",
    "            insee_except, \n",
    "            unzip_inpi, \n",
    "            unzip_insee, \n",
    "            REDUCE(\n",
    "              zip_with(\n",
    "                list_weights_inpi, \n",
    "                list_weights_insee, \n",
    "                (x, y) -> x * y\n",
    "              ), \n",
    "              CAST(\n",
    "                ROW(0.0) AS ROW(sum DOUBLE)\n",
    "              ), \n",
    "              (s, x) -> CAST(\n",
    "                ROW(x + s.sum) AS ROW(sum DOUBLE)\n",
    "              ), \n",
    "              s -> s.sum\n",
    "            ) / (\n",
    "              SQRT(\n",
    "                REDUCE(\n",
    "                  transform(\n",
    "                    list_weights_inpi, \n",
    "                    (x) -> POW(x, 2)\n",
    "                  ), \n",
    "                  CAST(\n",
    "                    ROW(0.0) AS ROW(sum DOUBLE)\n",
    "                  ), \n",
    "                  (s, x) -> CAST(\n",
    "                    ROW(x + s.sum) AS ROW(sum DOUBLE)\n",
    "                  ), \n",
    "                  s -> s.sum\n",
    "                )\n",
    "              ) * SQRT(\n",
    "                REDUCE(\n",
    "                  transform(\n",
    "                    list_weights_insee, \n",
    "                    (x) -> POW(x, 2)\n",
    "                  ), \n",
    "                  CAST(\n",
    "                    ROW(0.0) AS ROW(sum DOUBLE)\n",
    "                  ), \n",
    "                  (s, x) -> CAST(\n",
    "                    ROW(x + s.sum) AS ROW(sum DOUBLE)\n",
    "                  ), \n",
    "                  s -> s.sum\n",
    "                )\n",
    "              )\n",
    "            ) AS cosine_distance  \n",
    "    FROM distance\n",
    "    )\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database=database,\n",
    "            s3_output=s3_output,\n",
    "  filename = 'cosine', ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        ).sort_values(by = 'cosine_distance', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Recupération de la similarité maximum par `row_id` et calcul Levensthein\n",
    "\n",
    "La dernière étape consiste a récupérer la similarité maximum sur les doublons provenant du `row_id` de sorte à n'avoir qu'une ligne par pair "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH dataset AS (\n",
    "  SELECT \n",
    "    ets_siretisation.ets_insee_inpi.row_id, \n",
    "    index_id, \n",
    "    status_cas, \n",
    "    inpi_except, \n",
    "    insee_except, \n",
    "    transform(\n",
    "      sequence(\n",
    "        1, \n",
    "        CARDINALITY(insee_except)\n",
    "      ), \n",
    "      x -> insee_except\n",
    "    ), \n",
    "    ZIP(\n",
    "      inpi_except, \n",
    "      transform(\n",
    "        sequence(\n",
    "          1, \n",
    "          CARDINALITY(inpi_except)\n",
    "        ), \n",
    "        x -> insee_except\n",
    "      )\n",
    "    ) as test \n",
    "  FROM \n",
    "    ets_siretisation.ets_insee_inpi  \n",
    "    \n",
    "  LEFT JOIN ets_siretisation.ets_insee_inpi_statut_cas \n",
    "  ON ets_siretisation.ets_insee_inpi.row_id = ets_siretisation.ets_insee_inpi_statut_cas.row_id\n",
    "  where \n",
    "    (status_cas != 'CAS_2' AND CARDINALITY(inpi_except)  > 0 AND CARDINALITY(insee_except) > 0 AND index_id = 728021)\n",
    "  )\n",
    " SELECT \n",
    "  * \n",
    "FROM \n",
    "  (\n",
    "    WITH distance AS (\n",
    "      SELECT \n",
    "        * \n",
    "      FROM \n",
    "        (\n",
    "          WITH list_weights_insee_inpi AS (\n",
    "            SELECT \n",
    "              row_id, \n",
    "              index_id, \n",
    "              status_cas, \n",
    "              inpi_except, \n",
    "              insee_except, \n",
    "              unzip_inpi, \n",
    "              unzip_insee, \n",
    "              list_weights_inpi, \n",
    "              list_weights_insee \n",
    "            FROM \n",
    "              (\n",
    "                SELECT \n",
    "                  row_id, \n",
    "                  index_id, \n",
    "                  status_cas, \n",
    "                  inpi_except, \n",
    "                  insee_except, \n",
    "                  unzip.field0 as unzip_inpi, \n",
    "                  unzip.field1 as insee, \n",
    "                  test \n",
    "                FROM \n",
    "                  dataset CROSS \n",
    "                  JOIN UNNEST(test) AS new (unzip)\n",
    "              ) CROSS \n",
    "              JOIN UNNEST(insee) as test (unzip_insee) \n",
    "              LEFT JOIN (\n",
    "                SELECT \n",
    "                  words, \n",
    "                  list_weights as list_weights_inpi \n",
    "                FROM \n",
    "                  ets_siretisation.list_weight_mots_insee_inpi_word2vec \n",
    "              ) tb_weight_inpi ON unzip_inpi = tb_weight_inpi.words \n",
    "              LEFT JOIN (\n",
    "                SELECT \n",
    "                  words, \n",
    "                  list_weights as list_weights_insee \n",
    "                FROM \n",
    "                  ets_siretisation.list_weight_mots_insee_inpi_word2vec \n",
    "              ) tb_weight_insee ON unzip_insee = tb_weight_insee.words \n",
    "          ) \n",
    "          SELECT \n",
    "            row_id, \n",
    "            index_id, \n",
    "            status_cas, \n",
    "            inpi_except, \n",
    "            insee_except, \n",
    "            unzip_inpi, \n",
    "            unzip_insee, \n",
    "            REDUCE(\n",
    "              zip_with(\n",
    "                list_weights_inpi, \n",
    "                list_weights_insee, \n",
    "                (x, y) -> x * y\n",
    "              ), \n",
    "              CAST(\n",
    "                ROW(0.0) AS ROW(sum DOUBLE)\n",
    "              ), \n",
    "              (s, x) -> CAST(\n",
    "                ROW(x + s.sum) AS ROW(sum DOUBLE)\n",
    "              ), \n",
    "              s -> s.sum\n",
    "            ) / (\n",
    "              SQRT(\n",
    "                REDUCE(\n",
    "                  transform(\n",
    "                    list_weights_inpi, \n",
    "                    (x) -> POW(x, 2)\n",
    "                  ), \n",
    "                  CAST(\n",
    "                    ROW(0.0) AS ROW(sum DOUBLE)\n",
    "                  ), \n",
    "                  (s, x) -> CAST(\n",
    "                    ROW(x + s.sum) AS ROW(sum DOUBLE)\n",
    "                  ), \n",
    "                  s -> s.sum\n",
    "                )\n",
    "              ) * SQRT(\n",
    "                REDUCE(\n",
    "                  transform(\n",
    "                    list_weights_insee, \n",
    "                    (x) -> POW(x, 2)\n",
    "                  ), \n",
    "                  CAST(\n",
    "                    ROW(0.0) AS ROW(sum DOUBLE)\n",
    "                  ), \n",
    "                  (s, x) -> CAST(\n",
    "                    ROW(x + s.sum) AS ROW(sum DOUBLE)\n",
    "                  ), \n",
    "                  s -> s.sum\n",
    "                )\n",
    "              )\n",
    "            ) AS cosine_distance \n",
    "          FROM \n",
    "            list_weights_insee_inpi\n",
    "        )\n",
    "    ) \n",
    "    SELECT \n",
    "      row_id, \n",
    "      dataset.index_id, \n",
    "      inpi_except, \n",
    "      insee_except, \n",
    "      unzip_inpi, \n",
    "      unzip_insee, \n",
    "      max_cosine_distance,\n",
    "      test as key_except_to_test,\n",
    "      levenshtein_distance(unzip_inpi, unzip_insee) AS levenshtein_distance\n",
    "    \n",
    "    FROM \n",
    "      dataset \n",
    "      LEFT JOIN (\n",
    "        SELECT \n",
    "          distance.index_id, \n",
    "          unzip_inpi, \n",
    "          unzip_insee, \n",
    "          max_cosine_distance \n",
    "        FROM \n",
    "          distance \n",
    "          RIGHT JOIN (\n",
    "            SELECT \n",
    "              index_id, \n",
    "              MAX(cosine_distance) as max_cosine_distance \n",
    "            FROM \n",
    "              distance \n",
    "            GROUP BY \n",
    "              index_id\n",
    "          ) as tb_max_distance ON distance.index_id = tb_max_distance.index_id \n",
    "          AND distance.cosine_distance = tb_max_distance.max_cosine_distance\n",
    "      ) as tb_max_distance_lookup ON dataset.index_id = tb_max_distance_lookup.index_id\n",
    "  )\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database=database,\n",
    "            s3_output=s3_output,\n",
    "  filename = 'max_cosine', ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, shutil, urllib, ipykernel, json\n",
    "from pathlib import Path\n",
    "from notebook import notebookapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_report(extension = \"html\", keep_code = False):\n",
    "    \"\"\"\n",
    "    Create a report from the current notebook and save it in the \n",
    "    Report folder (Parent-> child directory)\n",
    "    \n",
    "    1. Exctract the current notbook name\n",
    "    2. Convert the Notebook \n",
    "    3. Move the newly created report\n",
    "    \n",
    "    Args:\n",
    "    extension: string. Can be \"html\", \"pdf\", \"md\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ### Get notebook name\n",
    "    connection_file = os.path.basename(ipykernel.get_connection_file())\n",
    "    kernel_id = connection_file.split('-', 1)[0].split('.')[0]\n",
    "\n",
    "    for srv in notebookapp.list_running_servers():\n",
    "        try:\n",
    "            if srv['token']=='' and not srv['password']:  \n",
    "                req = urllib.request.urlopen(srv['url']+'api/sessions')\n",
    "            else:\n",
    "                req = urllib.request.urlopen(srv['url']+ \\\n",
    "                                             'api/sessions?token=' + \\\n",
    "                                             srv['token'])\n",
    "            sessions = json.load(req)\n",
    "            notebookname = sessions[0]['name']\n",
    "        except:\n",
    "            pass  \n",
    "    \n",
    "    sep = '.'\n",
    "    path = os.getcwd()\n",
    "    #parent_path = str(Path(path).parent)\n",
    "    \n",
    "    ### Path report\n",
    "    #path_report = \"{}/Reports\".format(parent_path)\n",
    "    #path_report = \"{}/Reports\".format(path)\n",
    "    \n",
    "    ### Path destination\n",
    "    name_no_extension = notebookname.split(sep, 1)[0]\n",
    "    source_to_move = name_no_extension +'.{}'.format(extension)\n",
    "    dest = os.path.join(path,'Reports', source_to_move)\n",
    "    \n",
    "    ### Generate notebook\n",
    "    if keep_code:\n",
    "        os.system('jupyter nbconvert --to {} {}'.format(\n",
    "    extension,notebookname))\n",
    "    else:\n",
    "        os.system('jupyter nbconvert --no-input --to {} {}'.format(\n",
    "    extension,notebookname))\n",
    "    \n",
    "    ### Move notebook to report folder\n",
    "    #time.sleep(5)\n",
    "    shutil.move(source_to_move, dest)\n",
    "    print(\"Report Available at this adress:\\n {}\".format(dest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_report(extension = \"html\", keep_code = True)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nteract": {
   "version": "0.24.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
