{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation table merge INSEE INPI filtree \n",
    "\n",
    "Copy paste from Coda to fill the information\n",
    "\n",
    "# Objective(s)\n",
    "\n",
    "*  Création d’une table contenant la jointure des tables de l’INSEE et de l’INPI transformées. \n",
    "* Dans cette jointure, il n’est pas nécessaire de récupérer toutes les variables. Seules les variables énumérées ci-dessous seront utilisées:\n",
    "  * `row_id`\n",
    "  * `index_id` \n",
    "  * `sequence_id`  \n",
    "  * `count_initial_insee`  \n",
    "  * `ets_inpi_sql.siren` \n",
    "  * `siret` \n",
    "  * `datecreationetablissement` \n",
    "  * `date_debut_activite` \n",
    "  * `etatadministratifetablissement` \n",
    "  * `status_admin` \n",
    "  * `etablissementsiege` \n",
    "  * `status_ets` \n",
    "  * `adresse_distance_inpi` \n",
    "  * `adresse_distance_insee` \n",
    "  * `list_numero_voie_matching_inpi` \n",
    "  * `list_numero_voie_matching_insee` \n",
    "  * `ets_inpi_sql.code_postal_matching` \n",
    "  * `ets_inpi_sql.ville_matching` \n",
    "  * `codecommuneetablissement` \n",
    "  * `code_commune` \n",
    "  * `enseigne` \n",
    "  * `list_enseigne` \n",
    "\n",
    "# Metadata\n",
    "\n",
    "* Epic: Epic 6\n",
    "* US: US 4\n",
    "* Date Begin: 9/28/2020\n",
    "* Duration Task: 0\n",
    "* Description: Merger les tables insee et inpi afin d’avoir une table prête pour la réalisation des tests pour la siretisation\n",
    "* Step type: Transform table\n",
    "* Status: Active\n",
    "  * Change Status task: Active\n",
    "  * Update table: Modify rows\n",
    "* Source URL: US 04 Merge table INSEE INPI\n",
    "* Task type: Jupyter Notebook\n",
    "* Users: Thomas Pernet\n",
    "* Watchers: Thomas Pernet\n",
    "* User Account: https://937882855452.signin.aws.amazon.com/console\n",
    "* Estimated Log points: 10\n",
    "* Task tag: #athena,#sql,#data-preparation,#inpi,#insee\n",
    "* Toggl Tag: #documentation\n",
    "\n",
    "# Input Cloud Storage [AWS/GCP]\n",
    "\n",
    "## Table/file\n",
    "\n",
    "* Origin: \n",
    "* Athena\n",
    "* Name: \n",
    "* ets_inpi_transformed\n",
    "* ets_insee_transformed\n",
    "* Github: \n",
    "  * https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/10_sumup_preparation/02_creation_variables_siretisation_inpi.md\n",
    "  * https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/10_sumup_preparation/03_creation_variables_siretisation_insee.md\n",
    "\n",
    "# Destination Output/Delivery\n",
    "\n",
    "## Table/file\n",
    "* Origin: \n",
    "* Athena\n",
    "* Name:\n",
    "* ets_insee_inpi\n",
    "* GitHub:\n",
    "* https://github.com/thomaspernet/InseeInpi_matching/blob/master/Notebooks_matching/Data_preprocessed/programme_matching/11_sumup_siretisation/00_merge_ets_insee_inpi.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "\n",
    "La seconde partie du projet consiste a utiliser la table transformée de l'INSEE afin d'attribuer un siret au établissement présent dans la table transformée de l'INPI. Pour réaliser ce processus dit de siretisation, il faut tout d'abord joindre les tables créés durant la partie précédente. La table de l'INSEE ne contient qu'un seul siret (établissement), avec les informations les plus récentes uniquement. La table de l'INPI, quant à elle, rassemble toutes les informations connues des greffes. Autrement dit, la table a une profondeur historique. Nous devons donc utiliser les informations les plus récentes pour déduire le siret d'un établissement à l'INPI. La façon dont nous réalisons la jointure (sur le siren et le code postal) va déboucher sur des doublons. Effectivement, une entreprise peut avoir plus d'un établissement pour un code postal donné. \n",
    "\n",
    "Le dédoublonnement de cette table va être faite par l'intermédiaire de règles de gestion (tests) que nous avons élaboré. Nous allons utiliser une matrice dans lequel figure un ensemble de règles de gestion. Cette matrice est un produit cartésien de toutes les variables que nous avons transformé lors de la partie une, trié de manière ordonnée. Plus précisément, les variables générées lors de la partie précédente permettent la création de règles, majoritairement booléennes. Les règles sont ensuite triées par ordre d'importance, ou la règle une est prioritaire sur la règle deux, la règles deux est prioritaire sur la règle trois et ainsi de suite. Le produit cartésien va produire une matrice carrée avec toutes les possibilités, dans lequel figure le rang. Le rang un est la meilleure combinaison possible des tests. Chaque ligne de la table insee-inpi contient une batterie de variables tests qui va être comparée avec la matrice des règles de gestion. Cette comparaison fait sens pour les lignes doublonnées car c'est elles que nous cherchons a filtrer. Nous allons comparer les tests de chacune des lignes et ne garder uniquement la ligne dont le rang est le plus faible. Cette technique va nous permettre de dédoublonner les établissements en ne gardant que le meilleur des probables sachant les informations que nous avons. Les autres lignes doublonées pour le même établissement n'ont pas des informations suffisament pertinentes. Ainsi, chacun des établissements au sens de l'INPI se voit attribuer un siret le plus probable sachant l'information contenu dans les deux tables.\n",
    "\n",
    "Le processus de siretisation est découpé en 3 ensembles (figure ci dessous). Premièrement, il faut joindre les tables de l'INSEE et de l'INPI. Ensuite, il est nécéssaire de créer les variables qui vont être utiliser lors des tests puis bien sur générer les tests. Troisièmenent, le rapprochement avec la matrice des règles de gestion va permettre de dédoublonner les lignes avec plusieurs probables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://scm.saas.cagip.group.gca/PERNETTH/inseeinpi_matching/-/raw/master/IMAGES/09_schema_siretisation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connexion serveur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsPy.aws_authorization import aws_connector\n",
    "from awsPy.aws_s3 import service_s3\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os, shutil\n",
    "\n",
    "path = os.getcwd()\n",
    "parent_path = str(Path(path).parent)\n",
    "path_cred = r\"{}/credential_AWS.json\".format(parent_path)\n",
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                       region = 'eu-west-3')\n",
    "\n",
    "region = 'eu-west-3'\n",
    "bucket = 'calfdata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                       region = region)\n",
    "client= con.client_boto()\n",
    "s3 = service_s3.connect_S3(client = client,\n",
    "                      bucket = bucket, verbose = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_setting = True\n",
    "if pandas_setting:\n",
    "    cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jointure INSEE-INPI\n",
    "\n",
    "La jointure entre la table de l'INSEE et de l'INPI se fait sur le siren, la ville et le code postal. A noter que nous devons créer une variable avec le numéro de lignes, qui va nous être util ultérieurement pour merger les tables intermédiaires.\n",
    "\n",
    "Dans cette jointure, il n’est pas nécessaire de récupérer toutes les variables. Seules les variables énumérées ci-dessous seront utilisées:\n",
    "\n",
    "  * `row_id`\n",
    "  * `index_id` \n",
    "  * `sequence_id`  \n",
    "  * `count_initial_insee`  \n",
    "  * `ets_inpi_sql.siren` \n",
    "  * `siret` \n",
    "  * `datecreationetablissement` \n",
    "  * `date_debut_activite` \n",
    "  * `etatadministratifetablissement` \n",
    "  * `status_admin` \n",
    "  * `etablissementsiege` \n",
    "  * `status_ets` \n",
    "  * `adresse_distance_inpi` \n",
    "  * `adresse_distance_insee` \n",
    "  * `list_numero_voie_matching_inpi` \n",
    "  * `list_numero_voie_matching_insee` \n",
    "  * `ets_inpi_sql.code_postal_matching` \n",
    "  * `ets_inpi_sql.ville_matching` \n",
    "  * `codecommuneetablissement` \n",
    "  * `code_commune` \n",
    "  * `enseigne` \n",
    "  * `list_enseigne` \n",
    "\n",
    "Nous ne mettons pas les informations dans le fichier JSON de configuration car cette partie n'a pas été intégrer dans la mise en production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output = 'SQL_OUTPUT_ATHENA'\n",
    "database = 'ets_siretisation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "DROP TABLE `ets_siretisation.ets_insee_inpi`;\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database=database,\n",
    "            s3_output=s3_output,\n",
    "  filename = None, ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "CREATE TABLE ets_siretisation.ets_insee_inpi \n",
    "WITH (\n",
    "  format='PARQUET'\n",
    ") AS\n",
    "  SELECT \n",
    "    ROW_NUMBER() OVER () AS row_id, \n",
    "    index_id, \n",
    "    sequence_id, \n",
    "    count_initial_insee, \n",
    "    ets_inpi_transformed.siren, \n",
    "    siret, \n",
    "    datecreationetablissement, \n",
    "    date_debut_activite, \n",
    "    etatadministratifetablissement, \n",
    "    status_admin, \n",
    "    etablissementsiege, \n",
    "    status_ets, \n",
    "    adresse_distance_inpi, \n",
    "    adresse_distance_insee, \n",
    "    list_numero_voie_matching_inpi, \n",
    "    list_numero_voie_matching_insee, \n",
    "    ets_inpi_transformed.code_postal_matching, \n",
    "    ets_inpi_transformed.ville_matching, \n",
    "    codecommuneetablissement, \n",
    "    code_commune, \n",
    "    enseigne, \n",
    "    list_enseigne\n",
    "  FROM \n",
    "    ets_inpi.ets_inpi_transformed \n",
    "    INNER JOIN (\n",
    "      SELECT \n",
    "        count_initial_insee, \n",
    "        siren, \n",
    "        siret, \n",
    "        datecreationetablissement, \n",
    "        etablissementsiege, \n",
    "        etatadministratifetablissement, \n",
    "        codepostaletablissement, \n",
    "        codecommuneetablissement, \n",
    "        ville_matching, \n",
    "        list_numero_voie_matching_insee, \n",
    "        adresse_reconstituee_insee, \n",
    "        adresse_distance_insee, \n",
    "        list_enseigne\n",
    "      FROM \n",
    "        ets_insee.ets_insee_transformed\n",
    "    ) as insee \n",
    "    ON ets_inpi_transformed.siren = insee.siren \n",
    "    AND ets_inpi_transformed.ville_matching = insee.ville_matching \n",
    "    AND ets_inpi_transformed.code_postal_matching = insee.codepostaletablissement \n",
    "    WHERE status is NULL\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "s3.run_query(\n",
    "    query=query,\n",
    "    database=database,\n",
    "    s3_output=s3_output,\n",
    "    filename = None, ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse\n",
    "\n",
    "1. Imprimer 10 lignes aléatoirement\n",
    "2. Compter le nombre d'observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM ets_siretisation.ets_insee_inpi\n",
    "limit 10\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database='ets_siretisation',\n",
    "            s3_output=s3_output,\n",
    "  filename = 'exemple_siretisation', ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT COUNT(*) AS CNT\n",
    "FROM ets_siretisation.ets_insee_inpi\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "            query=query,\n",
    "            database='ets_siretisation',\n",
    "            s3_output=s3_output,\n",
    "  filename = 'count_siretisation', ## Add filename to print dataframe\n",
    "  destination_key = None ### Add destination key if need to copy output\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, shutil, urllib, ipykernel, json\n",
    "from pathlib import Path\n",
    "from notebook import notebookapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_report(extension = \"html\", keep_code = False):\n",
    "    \"\"\"\n",
    "    Create a report from the current notebook and save it in the \n",
    "    Report folder (Parent-> child directory)\n",
    "    \n",
    "    1. Exctract the current notbook name\n",
    "    2. Convert the Notebook \n",
    "    3. Move the newly created report\n",
    "    \n",
    "    Args:\n",
    "    extension: string. Can be \"html\", \"pdf\", \"md\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ### Get notebook name\n",
    "    connection_file = os.path.basename(ipykernel.get_connection_file())\n",
    "    kernel_id = connection_file.split('-', 1)[0].split('.')[0]\n",
    "\n",
    "    for srv in notebookapp.list_running_servers():\n",
    "        try:\n",
    "            if srv['token']=='' and not srv['password']:  \n",
    "                req = urllib.request.urlopen(srv['url']+'api/sessions')\n",
    "            else:\n",
    "                req = urllib.request.urlopen(srv['url']+ \\\n",
    "                                             'api/sessions?token=' + \\\n",
    "                                             srv['token'])\n",
    "            sessions = json.load(req)\n",
    "            notebookname = sessions[0]['name']\n",
    "        except:\n",
    "            pass  \n",
    "    \n",
    "    sep = '.'\n",
    "    path = os.getcwd()\n",
    "    #parent_path = str(Path(path).parent)\n",
    "    \n",
    "    ### Path report\n",
    "    #path_report = \"{}/Reports\".format(parent_path)\n",
    "    #path_report = \"{}/Reports\".format(path)\n",
    "    \n",
    "    ### Path destination\n",
    "    name_no_extension = notebookname.split(sep, 1)[0]\n",
    "    source_to_move = name_no_extension +'.{}'.format(extension)\n",
    "    dest = os.path.join(path,'Reports', source_to_move)\n",
    "    \n",
    "    ### Generate notebook\n",
    "    if keep_code:\n",
    "        os.system('jupyter nbconvert --to {} {}'.format(\n",
    "    extension,notebookname))\n",
    "    else:\n",
    "        os.system('jupyter nbconvert --no-input --to {} {}'.format(\n",
    "    extension,notebookname))\n",
    "    \n",
    "    ### Move notebook to report folder\n",
    "    #time.sleep(5)\n",
    "    shutil.move(source_to_move, dest)\n",
    "    print(\"Report Available at this adress:\\n {}\".format(dest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_report(extension = \"html\", keep_code = True)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nteract": {
   "version": "0.24.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
